{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-5.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-5.5) Perceptron Algorithm\n",
    "- in Wx+b=0 or =1,2 etc (which is equivalent of making b:=b-1 or b-2 etc)\n",
    "    - <div dir=rtl> برأيى إن سبب إن بينتج خطوط متوازية من تغيير ال b لأن الميل ثابت ، حيث تذكر إن الميل بيأثر فيه المتجه W</div>\n",
    "\n",
    "### (3-5.7) Margin Error\n",
    "> لحد الآن أنا شايف إنه هو نفس الخط ، لكن لما ضرب ال دبليو و ال بى فى اتنين، فبالرغم إنه نفس الخط لكن الماردن اتغير\n",
    "    اللى لسة عاةز أشوفه ، أمال استفدنا احنا ايه لما هو نفس الخط ؟! <br>\n",
    "    اااه اعتقد لسة احنا مش بنقول هنستفاد إيه ، احنا بس بنعمل صيغة و بنوضح إزاى ممكن المارجن ندخله فى معادلة الايرور ، فأكيد هنشوف دا بعد شوية إزاى هنستخدمه ، أكيد انا منتظر إن الخط يميل وكدا ويتحرك و يتغير صح ؟ هشوف\n",
    "- المهم تلاحظ من الفيديو دا إزاى بيزيد أو يقل المارجن بإننا بنضرب معاملات المعادلة كلها فى ثابت\n",
    "\n",
    "### (3-5.8) (Optional) Margin Error Calculation \n",
    " عاوز أفهم آخر جملة قبل الرسمة 4 <br>\n",
    "بعد الرسمة 4 ، ليه اشترط إن مدام المقام هو قيمة مطلقة ، فنقدر نعمل اللى عمله... طب لو كان المقام قيمة متجهة ،، مكنش هينفع؟ ليه؟\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-6.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-6.9) AdaBoost in sklearn\n",
    "أنا مش فاهم دلوقت هل فى خطأ فى الشرح لما قال إن <br> \n",
    "model has been fitted to a tree ??!!\n",
    "\n",
    "فبالتالى أنا عاوز أعرف هل أنا بعمل <br> \n",
    "fitting to the model first then use tha AdaBoost ? or it is demonstrated wrongly in that section? <br><br>\n",
    "\n",
    "<div dir=rtl>\n",
    "عاوز أجرب كذا base_estimator و أشوف بيعملوا إيه ،، و كمان هل ممكن أعمل ال base_estimator خوارزميات مختلفة ؟ ولا لازم نفس الخوارزيمة هى اللى بيتم استخدامها كذا مرة ؟\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.1) Announcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.2) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.3) Classification Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.4) Classification Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.5) Linear Boundries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.6) Higher Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.7) Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.8) Perceptron as Logical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.9) Why \"Neural Networks\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.10) Perceptron Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.11) Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.11) Non-Linear Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.12) Error Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.13) Log-loss Error Function \n",
    "- Nice reasoning for why we can't use \"count of error points\" as the error metric (b/c it is discrete and discrete functions are not continuous, and we really need a continuous error function (and differentiable) so that a small change in the precision will be captured as transition (change) in the output of the funtion) (but in discrete error func, we can change the function of the line , but the error is still the same). <br>\n",
    "- Note: don't confuse the error function (loss function) with the function of the line (the hypothesis)\n",
    "- <font color=red>**Q\\**</font> what is the \"loss\" in \"log-loss error\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.14) Discrete vs Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.15) Softmax\n",
    "- i need to know: how softmax turns into sigmoid when n=2 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.16) One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.17) Maximum likelihood\n",
    "- my note: the reason that made the first model give low overall probability is that : the misclassified points have low probability of belonging to their right region. ex, the misclassified red point , has P(blue) =0.9 , and hence have P(red)0.1, but notice that we use the latter in calculating the whole probability of the model, i.e we use the P(red) fot the red point, and P(blue) if the point is actuallu blue. <br>\n",
    "- I might ask, what is the meaning of the red point having high P(blue)=0.9? and i will tell that this is the wrong prediction made by the model . the model assumes that this point has higher probability of being blue because it lies far in the blue region.\n",
    "- i still need a formal definition of P(all) and maximum liklihood , are they the samething or do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.18) Maximizing Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.19) Cross-Entropy 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.20) Cross-Entropy 2\n",
    "In the quiz, I stayed half an hour searching for the error, and it was that i  made this line as <br>\n",
    "`for i in Y` <br>\n",
    "when it should be <br>\n",
    "`for i in range(len(Y))` <br>\n",
    "because I want `i` to be an increasing index, not each element in the `Y` list <br>\n",
    "وكدا كدا الحل دايما مش بيعمل فور لوب كدا، بل دا تفكيرك عشان انت متعود على السى ، لكن هنا اللستات بتتشقلب مع بعض عادى بدون لوب <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**why the udacity solution made the np.float_ to the two lists?** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.828313737302301\n",
      "[0. 1. 0. 0.]\n",
      "[0.6 0.4 0.9 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y=[1,0,1,1] \n",
    "P=[0.4,0.6,0.1,0.5]\n",
    "\n",
    "Y_new = np.float_(Y)\n",
    "P_new = np.float_(P)\n",
    "\n",
    "CE_entropy = -np.sum(Y_new * np.log(P_new) + (1 - Y_new) * np.log(1 - P_new)) # note that np.log is the natural log (ln)\n",
    "print (CE_entropy)\n",
    "\n",
    "# 1- Y , ERROR , can't do subtraction operation between tupes : int and  list\n",
    "print (1-Y_new)\n",
    "print (1-P_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y is python list , and you cant do operation such as `1-Y` on a python list. <br>\n",
    "Y_new is a numpy ndarray (n dimensional array), which can be operated on as `1-Y_new` and the result is also ndarray. <br>\n",
    "so np.float_(Y) casts the list to an ndarray  of float elements <br>\n",
    "I might ask, why he made it `float`, not `int`? well. obviously float is the general case, and since we almost always will have fractions (like we do in the P list) so we have to cast the lists to float lists <br>\n",
    "you might look at the ndarray and its function in the numpy documentaion : https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to know why numpy use underscore after the types , here is a link, and biscally numpy does that so that these types do not clash with the same type define in python (without underscore) -> https://stackoverflow.com/questions/6205020/numpy-types-with-underscore-int-float-etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.21) Multi-class Cross Entropy\n",
    "- i need to answer the question at the end of the video : how do we say that the cross entropy formula is the same for the multi class m>2 and for m=2 while we saw that it was different form at m=2.\n",
    "- first i want to point out that the previous case it think it was m=2 class, but in the example of the three animals, it is 3*animals i.e it is 6 classes because each animal defines two classes (exists or does not exist)\n",
    "- but the problem is that the summation is from j=0 to m, which (from looking at the doors) implies that m=3 not 6 !! \n",
    "- so here comes a second question, if we have 3 animals and two doors, how will we separate the number of doors and the number of animals in the formula, i think there is a lot of ambiguity in the example given in this topic of multi-class cross Entropy\n",
    "- third question, whay is the meaning of \"Cross\" in the Cross entropy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.22) Logistic Regression\n",
    "- Note: at the end of the video, $x^i$ denotes each point in our input data samples, because remember :\n",
    "    - $Wx+b$ is the equation of the line (this defines a line to be drawn, a seperator)\n",
    "    - $Wx^i+b$ this is not an equation of a line, but rather, it is an equation where we substitute $x$ by a point: $x^i$, so making this substitution will give as a single value (a number, not a line nor a separator)\n",
    "    - also rememebr that $x^i$ is a pair of values : $(x_1,x_2)$, i.e $x^i$ is a 2x1 vecotor (or 1x2 i dont remembr) , (or nx1 for higher dimensions.  also it could be 1xn, i will revise and see)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.22) Gradient Descent\n",
    "- <font color=red>**Q\\**</font> in calculating $\\frac{\\partial }{\\partial w_j}E$ , why did he considered $y$ to be constant w.r.t $w_j$ (i.e $\\frac{\\partial }{\\partial w_j}y=y$)? I ask this because as far as i know , $y=Wx+b$, so $y$ is **not** constant w.r.t $w_j$, right ? <br>\n",
    "- <font color=blue> **No my dear friend** </font> , I knew later that $y$ is the label ,(e.g in a binary class classification, y is 0 or 1), so it is a consntat , but this raises another question, if $\\hat{y}=Wx^i+b$ , so <font color=red>**Q\\**</font> what is the variable assgined to (or used to name) the hypothesis : (var? $= Wx+b$) ? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.23) Logistic Regresion Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.24) Pre-Lab: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.25) Notebook: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.26) Perceptron vs Gradient Descent\n",
    "it is importnat to note the values that can be taken by $\\hat{y}$ I both GD and Percptron algo.. note in which it is it continuous and in which it is discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.27) Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center> Example of code writing equations here, not in inline mode, so that all letters appear well (inline makes them very small)</center>**  <br>\n",
    "\\begin{equation*}\n",
    "P(E)   = {n \\choose k} p^k (1-p)^{ n-k}\n",
    "\\end{equation*}\n",
    "\n",
    "**<center> or make a boxed equation (i can use it for final results or for hint to recall previous equations say for derivative of $ln$, etc)</center>**  <br>\n",
    "\\begin{equation*}\n",
    "    \\boxed{P(E)   = {n \\choose k} p^k (1-p)^{ n-k}}\n",
    "\\end{equation*}\n",
    "\n",
    "**<center> what i want to do is : before any mathematical derivation, i list the mathimatical rules that i am gonna use, so that i remember it easily . i may preceed these equation by the ward \"remember:\"</center>**  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : Cloud Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.1) Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.2) Create an AWS Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.3) Get Access to GPU Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.4) Launch an Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.5) Login to the Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : Deep Neural Networks\n",
    "- References :\n",
    "    - A post shared by ibrahim sobh: a recipe for training nn : \n",
    "        - http://karpathy.github.io/2019/04/25/recipe/\n",
    "    - 37 reasons why your neural network is not working (other than overfitting) , i think he talks about the implementation itself\n",
    "        - medium link https://medium.com/m/global-identity?redirectUrl=https%3A%2F%2Fblog.slavv.com%2F37-reasons-why-your-neural-network-is-not-working-4020854bd607%3Ffbclid%3DIwAR0FtIi4E6FSyn2anmsCcmnrzCHt9OLSlciXm4O9YT4WGVohiQnFO2X9M0Q\n",
    "        - other link if the above did not work https://weekly-geekly.github.io/articles/334944/index.html\n",
    "    - deep mind tweet: For neural networks to be deployed in the real world, we need guarantees that our network satisfy desired specifications. We introduce a method to verify complex non-linear specifications. :https://twitter.com/DeepMindAI/status/1125503318749581312\n",
    "        - link of the paper : https://arxiv.org/pdf/1902.09592.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.1) Non-linear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.2) Continuous Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.3) Non-linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.4) Neural Networks Architecture\n",
    "- <font color=red>**Q\\**</font> when combining two nns, why take the sigmoid of the addition of the two points? i know that he said that we want the scale to be between 0 and 1, but we could've used averaging and it would achieve the same goal!! ... i may try both and see the resutling boundries (i should know how to plot these things!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.5) Feedforward\n",
    "- at video 1 : feedforward\n",
    "    - <font color=red>**Q\\**</font> at $0:41$, how did he know that $w_1$ is bigger than $w_2$ ?! is this implied from the drwaing ? i want to know this !!\n",
    "    - <font color=red>**Q\\**</font> at $3:34$, what is that strange circle between matrices ? is this a multiplication operatot or what ?!\n",
    "    - <font color=red>**Q\\**</font> what does `Dense` mean ?\n",
    "    - <font color=red>**Q\\**</font> what if i made a mistake (or changed my mind) and wanter to change something in the middle of the path of the network (e.g an activation function)?\n",
    "    - <font color=red>**Q\\**</font> it is written \"We can see that the output has dimension 1.\" , where can i see that ???\n",
    "    - <font color=red>**Q\\**</font> in the quiz, \n",
    "        - when he starts with a random seed to the numpy package, what does this line affect afterword?\n",
    "        - when he did \"One-hot encoding the output\", what was the shape of y before and after this step? (this will tell me why did he do this in the first place! i ask this because i see that the output is not categorical, so why did he do that ! )\n",
    "        - i skipped this quiz because there are lots of things that i do not understand. (كلفتونى أوى فى الكيراس!!) i will return and do it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.6) Backpropagation\n",
    "- **Recommendation from eng.mohamed hammad** look at : \n",
    "    - https://github.com/DebPanigrahi/Machine-Learning/blob/master/back_prop.pdf\n",
    "    - https://www.facebook.com/mohamed.hamedhammad/posts/2503343279738207\n",
    "    - https://www.facebook.com/mohamed.hamedhammad/posts/2538529736219561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.7) Keras\n",
    "- <font color=red>**Q\\**</font> why he writes `dtype=np.float32` in defining X and y ??\n",
    "- <font color=red>**Q\\**</font> why `X.shape[1]` what does it mean, and why not `X.shape[0]`?? i need to get the code here and see each output of these things and i shall demonstrate each line myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.8) Pre-Lab: Student Admission in Keras</font>\n",
    "- what does dense do ?\n",
    "- why did he put 32 as the first input to the neuron ?\n",
    "- how do we choose the number of hidden layers and the number of neurons in each layer ?\n",
    "- i want to see the effect of adding/changing each neuron/layer/activation function. <br><br>\n",
    "\n",
    "- <font color=blue> **answer** </font> the last two questions are part of the DNN hyperparamater tuning, do the changes in lab \"(4-4.8) Mini project: Training an MLP on MNIST\n",
    "\" to get the intuition, and see this link to know how to tune these parameters -> https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.9) Lab: Student Admission in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.10) Training Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.11) Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.12) Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.13) Regularization 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.14) Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-315) Local Minima\n",
    "- <font color=red>**Q\\**</font> what is the solution ?!!\n",
    "    - <font color=blue> the answer is at (4-3.20) Random Restart and (4-3.21) Momentum</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.16) Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.17) Other Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.18) Batch vs Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.19) Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.20) Random Restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.21) Momentum\n",
    "- <font color=red>**Q\\**</font> I don't understand why the momentum idea behaves in such manner! i mean, what is so special about the global minimum that makes the algorithm converge to it ? what if the global minimum was not so deep (surrounded by low humps that the algorithm may also go over it !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.22) Optimizers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.23) Error Functions Around the World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.24) Neural Network Regression\n",
    "- at $1:30$, he says \"the way we turn a linear function into a ReLU is by turning off the parts of the negative values (or underneath the x-axis into zero)\"\n",
    "    - I think by underneath x-axis he means (to the left of the y-axis, i.e the negative part of the x-axis)\n",
    "- <font color=red>**Q\\**</font> why we bother and use ReLU ? why not just use a linear function ? (what is the advantgae of ReLU over the linear function)    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.25) Neural Networks Playground\n",
    "How did Jay alammar made these beautiful interaction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.26) Mini Project Intro </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-3.27) Pre-Lab: IMDB Data in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-3.28) Lab: IMDB Data in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.29) Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read this tweet from ian goodfellow about OctConv which is a simple replacement for the traditional convolution operation that gets better accuracy with fewer FLOPs <br>\n",
    "https://twitter.com/goodfellow_ian/status/1117929612200120320?s=19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.1) Introducing Alexis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.2) Applications of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.3) How Computers Interprets Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.4) MLPs for Image Classification\n",
    "Q\\ what is the input shape in `model.add(Flatten(input_shape=X_train.shape[1:]))` ? i mean whay is [1:] does ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.5) Categorical Cross-Entropy\n",
    "- Q\\ `accuracy = 100*score[1]` what is returnd into the score var, so that we take its 2nd element  ?\n",
    "    - for the line `score = model.evaluate(X_test, y_test, verbose=0)` the documentation says: evaluate *\"Returns (1)the loss value & (2)metrics values for the model\"* \n",
    "    - this makes sense, but i still want to know what is the *\"the loss value\" is it the error value ? how does it compare with the metric value ? why would i need both numbers?*\n",
    "        - answer, in an upcoming video, i knew that : Overfitting is detected by comparing the validation loss to the training loss. If the training loss is much lower than the validation loss, then the model might be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.6) Model Validation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.7) When do MLPs (not) work well?\n",
    "- unitl perior of here, all video were about MLPs (Deep nn).\n",
    "- here the video started to introduce CNN and compare it with DNN (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-4.8) Mini project: Training an MLP on MNIST </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.9) Local Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.10) Convolution Layers (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.11) Convolution Layers (Part 2) \n",
    "- Q\\ why does she use ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.12) Stride and Padding  \n",
    "(stride = |noun| a long, decisive step. |verb|walk with long, decisive steps in a specified direction.  ) <br>\n",
    "(Padding = حشوة أو حشو)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.13) Convolutional Layers in Keras\n",
    "- Q\\ in the reading it says \"You are **strongly encouraged** to add a ReLU activation function to **every** convolutional layer in your networks.\" why is that ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.14) Quiz: Dimensionality\n",
    "- I will demonstrate the equation of the width of the conv layer **in the case of `padding = 'valid'`**\n",
    "\\begin{equation*}\n",
    "width = ceil(float( \\frac{W_{in} - F + 1}{float(S)}))\n",
    "\\end{equation*}\n",
    "- for the nomenator `W_in - F + 1` , i was able to think about it like this : \n",
    "    - `W_in - (F - 1)` which says: if i have a width W_in , and a filter with with width F, how many steps are required for the filter to slide over the W_in (assuming that stride (طول الخطوة) is 1)? the answer is `W_{in} - (F - 1)` , and for the `-1` you can understand it by thinking of some examples :\n",
    "        - if W_in =6 , and F=3  , it will require the filter to do 4 steps to reach the end of the image (try to do it manually to see it)\n",
    "        - also think of W_in=6 and F=2, it will take you 5 steps\n",
    "    - now the previous assumed that the stride is 1, if the stride (طول الخطوة) was S, simply devide on it to get the number of steps using this new stride. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.15) Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.16) Max Pooling Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.17) CNNs for Image Classification\n",
    "- at $1:16$, why she said \"where the spatial dimension is a **power of 2** or else a number that is **divisible by a large power of 2**\" . why exactly is these two constrains ?:\n",
    "    - **power of 2**\n",
    "    - **divisible by a large power of 2** (also why **large** power of 2 , not any power ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.18) CNNs in Keras: Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-4.19) Mini project: CNNs in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.20) Image Augmentation in Keras\n",
    "- Q\\ when the reading says : \"where x_train.shape[0] corresponds to the number of unique samples in the training dataset x_train.\" , i think by unique means \"not including the augmentation\" right ?\n",
    "- i don't understand the reading that says \"By setting steps_per_epoch to this value, we ensure that the model sees x_train.shape[0] augmented images in each epoch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-4.21)  Mini project: Image Augmentation in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.22) Groundbreaking CNN Architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.23) Visualizing CNNs (Part 1)\n",
    "- Q\\ at $1:06$, i do not whay does \"take a filter from a cnn, then construct images that maximize the activation of that filter \" mean ? \n",
    "    - answer found in this link , under the title \"Retrieving images that maximally activate a neuron\" -> http://cs231n.github.io/understanding-cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.24) Visualizing CNNs (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.25) Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.26) Transfer Learning in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Deep Learning for Cancer Detection with Sebastian Thrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.2) Skin Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.3) Survival Probability of Skin Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.4) Medical Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.5) The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.6) Image Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.7) Quiz: Data Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.8) Solution: Data Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.9) Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.10) Quiz: Random vs Pre-initialized Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.11) Solution: Random vs Pre-initialized Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.12) Validation the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.13) Quiz: Sensitivity and Specifity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.14) Solution: Sensitivity and Specifity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.15) More on Sensitivity and Specifity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.16) Quiz: Diagnosing Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.17) Solution: Diagnosing Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.18) Refresh on ROC curves\n",
    "عجبنى جدا الأنيميشن اللى فى فى آخر فيديو فى الصفحة ، برأيى هعمل زيه و أكتب (فى الريبو بتاع الإنتويشن) الاستفادة من الأنيميشن دا إيه أو الرسمة يعنى دى ناخد منها (نبنى عليها) قرارات إزاى ،،، و هشير الفيديو دا فى لنكد إن وفى تويتر و فى جروب الفيس بتاع الذكاء الاصطناعى"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.19) Quiz: ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.20) Solution: ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.21) Comparing our Results with Doctors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.22) Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.23) What is the network looking at?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.24) Refresh on Confusing Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.25) Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.26) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.27) Useful Resoucres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.28) Mini Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-5.29) Mini Project: Dermatologist AI </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.30) Share your Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Deep Learning Assesment\n",
    "QUESTION 2 OF 5: <br>\n",
    "my asnwer is calculating : $3*2 + 3*1$\n",
    "\n",
    "QUESTION 3 OF 5: <br>\n",
    "I need to understand why!\n",
    "\n",
    "QUESTION 4 OF 5: <br>\n",
    "what is the kernel ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Unsupervised Learning \n",
    "### introduction and sub-summary\n",
    "in this unit we have 9 lessons: <br>\n",
    "- lesson 9: we learn two concepts\n",
    "    - Random Projection : \n",
    "        - same as PCA but the line is chosen randomly.\n",
    "        - it is faster than PCA\n",
    "        - we can specify epslon or n_compeonets\n",
    "    - ICA : (Independit component analysis)\n",
    "        - statistal method applied when we think that features are statistically independet\n",
    "        - common use cases \n",
    "            - in seprating voice in the cocktail party problem\n",
    "            - in seperating signals in EEG\n",
    "            - some finance application but it is rarely useful to use ICA in finance since financial systems are very complex.\n",
    "        - it has a mathematical derivation, but the most importnat thing that you have to know is the assumptions that is considered to make this algo work, they are :\n",
    "            - a\n",
    "            - a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Clustreing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-.6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.11) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.13) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.14) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.15) \n",
    "note that the K-means algorithm is a hill climbing algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.17) \n",
    "important rule of thumps : the more classes u have, the more local minimum will exists, the more you need to run K-means algorithm to reach to the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : Clustering Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.2) k-means Clustering of Movie Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.3) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : Hierarchical Density-based Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.1) K-means considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.2) Overview of other clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.3) Hierarchical clustering: single-link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.4) Examining single-link clustering\n",
    "- i was wondering how the coloring of clusters is done in this algotithm (the final result) (b/c the dendrogram goes all the way to 1 cluster!)? \n",
    "    - and i think the answer is that the algortithm stops depending on a threshold that is set to tell us that two cluster should not be merged. I think this is mapped to the dendrogram when we see a long vertical distance before two clusters are merged (as Jay said in the video about the long vertical distance in the dendrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.5) complete-link, average-link, Ward\n",
    "- I don't understand how subtracting $A_1^2, A_2^2, B_1^2, B_2^2$ reduces variance ! what is the variance anyway in the context of clustering? (i want to visualize clusters with high variance and with low variance to compare the two.)\n",
    "- Q\\ $A_1 = A_2$ and $B_1 = B_2$ right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.6) Hierarchical clustering implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.7) [Lab] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.8) [Lab Solution] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.9) HC examples and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.10) [Quiz] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.11) DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.12) DBSCAN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.13) [Lab] DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.14) [Lab Solution] DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.15) DBSCAN examples & applications\n",
    "- it is very important to me to know that DBSCAN is used as anomaly detection (detecting outliers in a dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.16) [Quiz] DBSCAN\n",
    "- Question 2 of 2 : imporntant to note that DBSCAN is not a hierarchical clustering method, but is a density-based clustering method that is robust against noisy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Gaussian Mixture models and Cluster Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.2) Gaussian Mixture Model (GMM) Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.3) Gaussian Distribution in One Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.4) GMM Clustering in One Dimension\n",
    "Q\\ how did he know that the physics test was hard by looking at its distribution of scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.5) Gaussian Disrtibution in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.6) GMM in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.7) Quiz: Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.8) Overview of The Expectation Maximization (EM) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.9) Expectation Maximization Part 1\n",
    "- I want to study the things in statistics and probability that was mentioned in this video\n",
    "- i want to define soft clustering, is it assigning probabilities to points of membership to the available clusters? i will search for the proper definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.10) Expectation Maximization Part 2\n",
    "- in the weighted average calculation, i was surprised that we did not devide by the number of elements, but we devided by the sum of the probabilities (since all probabilities are less than 1, so this sum is sure less than the number of points).\n",
    "    - 1- i want to know why\n",
    "    - 2- i want to compare this with if i had devided by the number of points, and i want to visualize both and get an intuition about the difference between the right method and my wrong assumption.\n",
    "- i want to know what is the log-likelihood, an what is mixing coefficient (note: they are in statistics)\n",
    "- Q\\ what is the \"1n\" that is in the video that precedes the equation of evaluating the log-likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.11) Visual Example of EM Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.12) Quiz: Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.13) GMM Implementation\n",
    "- Q\\ i want to something about the content of the array that contains the predicted labels, what if a point is inside two clusters (i.e when the two clusters overlap as the case in the previous video), to which cluster will it be assigned ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note:** on the difference between probability and membership\n",
    "- The term \"membership\" got me thinking if there is a difference between the two terms : membership and probability. \n",
    "    - first i assumed that i will say \"probabiliy of X\" when X is an event in the future that may have some kind of probability to happen, while i will say \"the membership of X\" when i talk/analyse something that already exists, such as saying that the movie (which is something already exists) is a member of (or belonging to) the genre action (by 80%) and romance (by 20%).\n",
    "    - i also assumed that probability and membership have the same mathematical tools, but we say probabilty of membership depending on the thing that we study\n",
    "- my assumptions were not so bad, as i found out that there are two fields that are related alot : probability and fuzzy logic (the latter has the membership function). and with little search i found that there are differences. look at the following references : <br>\n",
    "https://goodmath.scientopia.org/2011/02/02/fuzzy-logic-vs-probability/ <br>\n",
    "https://www.researchgate.net/post/Difference_between_fuzzy_logic_and_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.14) GMM Examples & Applications\n",
    "- I am wondering if i can mix up GMM with CNN and come up with useful things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.15) Cluster Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.16) Cluster Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.17) External Validation indices\n",
    "- i want to compare the `external validation idnex` to calculating the `accuracy` of of the clustering (the ratio of the points that was correctly clustered). \n",
    "    - Q\\ why we needed the `external validation idnex` when we could just use the `accuracy`?\n",
    "        - i think the answer lies in the objective we need when we asses the clustering, it is not just how many points are correctly clustered, but rather there were two objectives that was mentioned in the previous video that assess the clustering, namely: being compact and separated.\n",
    "    -Q\\ what does it mean for `ARI` to be equal to `0` ? what about `-1` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.18) Quiz: Adjusted Rand index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.19) Internal Validation Indices\n",
    "- Q\\ why the formula devides by $max(a_i,b_i)$ , i mean 1- why we devide? 2- why we use the max (what is the benefit of using the maximum?)!? \n",
    "    - is it because this normailize the output of this index to be between `-1` and `1` ? (i still need to see when it becomes `-1` and when it becomes `1` and when it becomes `0` or `-0.5` etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.20) Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.21) GMM & Cluster Validation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.22) GMM & Cluster Validation Lab Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.1) Chris's T-shirt Size (intuition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.2) A Metric for Chris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.3) Height + Weight for Cameron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.4) Sarah's Height + Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.5) Chris 's Shirt Size by Our Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.6) Comparing Features with Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.7) Feature Scaling Formula Quiz 1\n",
    "- For the formula of feature scaling, I, Ahmed, want to demonstrated this formula.\n",
    "- so the formula is as follows:\n",
    "\\begin{equation*}\n",
    "x' = \\frac{x-x_{min}}{x_{max}-x_{min}}\n",
    "\\end{equation*}\n",
    "- take for example the array [10 , 15 , 20]\n",
    "- the numerator works on shifting the array so that the minimum number becomes zero, so when we do $x-x_{min}$ the resulting array is [0 , 5 , 10]\n",
    "- and the denominator works on scaling the result array by deviding on its range, the range would be max of the resulting new array (=10) or it could be calculated from the old array as $x_{max}-x_{min}$,\n",
    "- so the final result becomes [0 ,  0.5 ,  1]\n",
    "- note : \n",
    "    - $x$ represents each element in the old array (the old one),  $x_{min}$ and $x_{max}$ are single elements in the array (the old one)\n",
    "    - $x'$ represents each new element in the final result (i.e each rescaled element)\n",
    "- when I demonstrate this concept to students, i may say that different range of values introduce a problem in our ML algorithm, since our algorithms usually use \"addition\" [i need to give example here], and we already know (in the course of numercal  analysis for example) that when we add large number with small number, the small number contribute to the result in an non-sensible way (we actuall sometimes ignore the small term in the addition when it is added to a large number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.8) Feature Scaling Formula Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.9) Feature Scaling Formula Quiz 3\n",
    "- it is important to note the advantage and disadvante of rescaling to the range of [0 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(5-5.10) Min/Max Rescaling Codeing Quiz</font>\n",
    "- a very important note was given in that quiz which is, the straight forward solution will be prone to divide-by-zero error.\n",
    "    - i should always think of this error whenever i implement anything that has devision operation in it\n",
    "- look here to see several ways to make an operation (such as subtracting a number) on every element on a python list : \n",
    "    - https://stackoverflow.com/questions/4918425/subtract-a-value-from-every-number-in-a-list-in-python\n",
    "    - https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.11) Min/Max Scaler in sklearn\n",
    "- i did not understand the part when she talked about the content of the numpy array! (she said things about each training point and each feature, and i did not understand that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.12) Quiz on Algorithms Requiring Rescaling\n",
    "- important video!! and i want analyse (maybe search online for fast answers) the other ML algorithms i took so far to see which ones are affected by features ranges (so we have to do feature scaling with them) and which ones do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.1) Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.2) Trickier Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.3) One-Dimensional, or Two?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.4) Slightly Less Perfect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.5) Trickiest Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.6) PCA for Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.7) Center of a New Coordinate System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.8) Principal Axis of New Coordinate System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.9) Second Principal Component of New System\n",
    "- Q\\ i did not understand how did he made the $-1$ !!? i do not see how should i look at the new components to conclude the $\\Delta y$ and $\\Delta x$\n",
    "    - answer, assume that the vectors are on the origin point of the old coordinates, and try to find out the values of the new vectors that represent the new coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.10) Practice Finding Centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.11) Pactice Finding New Axes\n",
    "- after finding the chagne for the new x vector, getting the new y vector may be hard at first, but i know two ways to get it :\n",
    "    - recall that to get the slope of perpendicular  - > (اقلب الميل و غير إشارته)\n",
    "    - i can also draw an imaginary triangle of the change in x, and then rotate this triangle by +90 degrees and hence i will get the change in y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.12) Which Data is Ready for PCA\n",
    "- Q\\ for the third data set :\n",
    "    - why he said it is impossible to do regression for this dataset (why he said it is impossible to build a regression that goes vertically?) ?! (note he said \"remember\" so it might be mentioned in the older version of the course, i may see it, but is suggest to search for the answer online to not waste my time searching for small piece of info in tens of vides)\n",
    "    - why the PCA is not as follows : the x' is to the bottom and y' is to the right ?\n",
    "    \n",
    "- note : he said that the circualr dataset will always give a deterministic result of PCA, i need to know why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.13) When Does an Axis Dominate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.14) Measurable vs. Latent Features Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.15) From Four Features to Two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.16) Compression While Preserving Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.17) Composote Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.18) Maximal Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.19) Advantages of Maximal Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.20) Maximal Variance and Information Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.21) Info Loss and Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.22) Neighborhood Composite Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.23) PCA for Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.24) Maximum number of PCs Quiz\n",
    "Q\\ why it is the $min(n_{samples}, n_{features})$ !! i thought it would be always the $n_{features}$ ! i need to understand that and maybe make an example where $n_{samples}<n_{features}$ so that i can see how the $n_{samples}$ will set the max num of PCs   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.25) Review/Definition of PCA\n",
    "- This is an important video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.26) Applying PCA to Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.27) PCA on the Enron Finance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.28) PCA in sklearn\n",
    "Q\\ i do not understand the writing : \"The projection step of PCA can be easiest to understand when you subtract out the mean shift of the new principal components, so the new and old dimensions have the same mean:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.29) When to Use PCA\n",
    "- Q\\ her talking about that PC made me wonder, how could it be to have a 2nd PC stronger than the 1st PC, i need to understand that b/c the demonstration so far implied that the 1st PC is at the dirction of maximum variance, and the 2nd PC is orthogonal to the 1st PC.\n",
    "    - note: in the PCA mini-Project, it is written : \"We mentioned that PCA will order the principal components, with the first PC giving the direction of maximal variance, second PC has second-largest variance, and so on. How much of the variance is explained by the first principal component? The second?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.30) PCA for Facial Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.31) Eigenfaces Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 7 : PCA Mini-Project\n",
    "- i need to answer the four questions at the end of that jupyter project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8 : Random Projection and ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.1) Random Projection\n",
    "- looking at $0.51$ then $2:11$ i conclude that a vertical \"in the table of the dataset\" column from top to bottom (which represents a single feature, but in several samples) is put in a horizontal row in the Matrix .\n",
    "    - similarly, each row in the table (represinting one point / one sample) is a column in the matrix\n",
    "    - in other words, going from table to matrix is as if we done a transpose to the table (خلى كل صف عمود و كل عمود صف)\n",
    "    \n",
    "- in the video, and also in the scikit-learn documentation, they talk about \"conservative estimation\"!! what is this ? for exapmle, at the sklearn site , the following is written \"It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.\" -> https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.2) Quiz: Random Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.3) Random Projection in sklearn\n",
    "- i've noticed that there are three types of \"fit\" functions so far in sklearn:\n",
    "    - fit\n",
    "    - fit_predict\n",
    "    - fit_transform\n",
    "- i want to summariez the difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.4) Independent Componenet Analysis (ICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.5) FastICA Algorithm\n",
    "- Q\\ what does it mean to center the dataset ? and whitening the dataset ?\n",
    "- i did not understand anything regarding the math :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.6) Quiz: ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.7) ICA in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.8) [Lab] Independent Componenet Analysis\n",
    "- it is written \"Map the values to the appropriate range for int16 audio. That range is between -32768 and +32767. A basic mapping can be done by multiplying by 32767.\"\n",
    "    - Q\\ how come that a basic mapping can be done by multiplying by 32767 ??!!!!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.9) [Solution] Independent Componenet Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.10) ICA Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 9 : Unsupervised Learning Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QUESTION 2 OF 3\n",
    "    - Note to not confuse K-nearest neighbors and K-means clustering. K-nearest neighbors is a classification algorithm, which is a subset of supervised learning. K-means is a clustering algorithm, which is a subset of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- References to check during the study\n",
    "    - post by mohamed hammad, in which he gave us three lecture that can make us sense more the way that RL learn : https://www.facebook.com/100001876777351/posts/2627523623986838\n",
    "    - a paper tweet by DeepMindAi, in which they reviews recent techniques in deep RL that narrow the gap in learning speed between humans and agents, & demonstrate an interplay between fast and slow learning w/ parallels in animal/human cognition\n",
    "        - https://twitter.com/deepmindai/status/1123979484485570566?s=19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links to tensor flow by jay alammar https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a tweet by Mat Leonard, in which someone,in a blog, talks about why we may leave matplotlib in favor of Altair for visualization, the blog starts by saying \"Sadly, in Python, we do not have a ggplot2.\" (the visualization library in R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side effects in python !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999964\n"
     ]
    }
   ],
   "source": [
    "print(8.5-8.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see this link for explanation -> https://www.quora.com/In-Python-why-does-8-5-8-4-give-0-099999999999999964"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 403,
   "position": {
    "height": "40px",
    "left": "921px",
    "right": "20px",
    "top": "79px",
    "width": "335px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
