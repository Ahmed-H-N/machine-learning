{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-5.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-5.5) Perceptron Algorithm\n",
    "- in Wx+b=0 or =1,2 etc (which is equivalent of making b:=b-1 or b-2 etc)\n",
    "    - <div dir=rtl> برأيى إن سبب إن بينتج خطوط متوازية من تغيير ال b لأن الميل ثابت ، حيث تذكر إن الميل بيأثر فيه المتجه W</div>\n",
    "\n",
    "### (3-5.7) Margin Error\n",
    "> لحد الآن أنا شايف إنه هو نفس الخط ، لكن لما ضرب ال دبليو و ال بى فى اتنين، فبالرغم إنه نفس الخط لكن الماردن اتغير\n",
    "    اللى لسة عاةز أشوفه ، أمال استفدنا احنا ايه لما هو نفس الخط ؟! <br>\n",
    "    اااه اعتقد لسة احنا مش بنقول هنستفاد إيه ، احنا بس بنعمل صيغة و بنوضح إزاى ممكن المارجن ندخله فى معادلة الايرور ، فأكيد هنشوف دا بعد شوية إزاى هنستخدمه ، أكيد انا منتظر إن الخط يميل وكدا ويتحرك و يتغير صح ؟ هشوف\n",
    "- المهم تلاحظ من الفيديو دا إزاى بيزيد أو يقل المارجن بإننا بنضرب معاملات المعادلة كلها فى ثابت\n",
    "\n",
    "### (3-5.8) (Optional) Margin Error Calculation \n",
    " عاوز أفهم آخر جملة قبل الرسمة 4 <br>\n",
    "بعد الرسمة 4 ، ليه اشترط إن مدام المقام هو قيمة مطلقة ، فنقدر نعمل اللى عمله... طب لو كان المقام قيمة متجهة ،، مكنش هينفع؟ ليه؟\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-6.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-6.9) AdaBoost in sklearn\n",
    "أنا مش فاهم دلوقت هل فى خطأ فى الشرح لما قال إن <br> \n",
    "model has been fitted to a tree ??!!\n",
    "\n",
    "فبالتالى أنا عاوز أعرف هل أنا بعمل <br> \n",
    "fitting to the model first then use tha AdaBoost ? or it is demonstrated wrongly in that section? <br><br>\n",
    "\n",
    "<div dir=rtl>\n",
    "عاوز أجرب كذا base_estimator و أشوف بيعملوا إيه ،، و كمان هل ممكن أعمل ال base_estimator خوارزميات مختلفة ؟ ولا لازم نفس الخوارزيمة هى اللى بيتم استخدامها كذا مرة ؟\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.1) Announcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.2) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.3) Classification Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.4) Classification Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.5) Linear Boundries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.6) Higher Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.7) Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.8) Perceptron as Logical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.9) Why \"Neural Networks\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.10) Perceptron Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.11) Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.11) Non-Linear Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.12) Error Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.13) Log-loss Error Function \n",
    "- Nice reasoning for why we can't use \"count of error points\" as the error metric (b/c it is discrete and discrete functions are not continuous, and we really need a continuous error function (and differentiable) so that a small change in the precision will be captured as transition (change) in the output of the funtion) (but in discrete error func, we can change the function of the line , but the error is still the same). <br>\n",
    "- Note: don't confuse the error function (loss function) with the function of the line (the hypothesis)\n",
    "- <font color=red>**Q\\**</font> what is the \"loss\" in \"log-loss error\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.14) Discrete vs Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.15) Softmax\n",
    "- i need to know: how softmax turns into sigmoid when n=2 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.16) One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.17) Maximum likelihood\n",
    "- my note: the reason that made the first model give low overall probability is that : the misclassified points have low probability of belonging to their right region. ex, the misclassified red point , has P(blue) =0.9 , and hence have P(red)0.1, but notice that we use the latter in calculating the whole probability of the model, i.e we use the P(red) fot the red point, and P(blue) if the point is actuallu blue. <br>\n",
    "- I might ask, what is the meaning of the red point having high P(blue)=0.9? and i will tell that this is the wrong prediction made by the model . the model assumes that this point has higher probability of being blue because it lies far in the blue region.\n",
    "- i still need a formal definition of P(all) and maximum liklihood , are they the samething or do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.18) Maximizing Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.19) Cross-Entropy 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.20) Cross-Entropy 2\n",
    "In the quiz, I stayed half an hour searching for the error, and it was that i  made this line as <br>\n",
    "`for i in Y` <br>\n",
    "when it should be <br>\n",
    "`for i in range(len(Y))` <br>\n",
    "because I want `i` to be an increasing index, not each element in the `Y` list <br>\n",
    "وكدا كدا الحل دايما مش بيعمل فور لوب كدا، بل دا تفكيرك عشان انت متعود على السى ، لكن هنا اللستات بتتشقلب مع بعض عادى بدون لوب <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**why the udacity solution made the np.float_ to the two lists?** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.828313737302301\n",
      "[0. 1. 0. 0.]\n",
      "[0.6 0.4 0.9 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y=[1,0,1,1] \n",
    "P=[0.4,0.6,0.1,0.5]\n",
    "\n",
    "Y_new = np.float_(Y)\n",
    "P_new = np.float_(P)\n",
    "\n",
    "CE_entropy = -np.sum(Y_new * np.log(P_new) + (1 - Y_new) * np.log(1 - P_new)) # note that np.log is the natural log (ln)\n",
    "print (CE_entropy)\n",
    "\n",
    "# 1- Y , ERROR , can't do subtraction operation between tupes : int and  list\n",
    "print (1-Y_new)\n",
    "print (1-P_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y is python list , and you cant do operation such as `1-Y` on a python list. <br>\n",
    "Y_new is a numpy ndarray (n dimensional array), which can be operated on as `1-Y_new` and the result is also ndarray. <br>\n",
    "so np.float_(Y) casts the list to an ndarray  of float elements <br>\n",
    "I might ask, why he made it `float`, not `int`? well. obviously float is the general case, and since we almost always will have fractions (like we do in the P list) so we have to cast the lists to float lists <br>\n",
    "you might look at the ndarray and its function in the numpy documentaion : https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to know why numpy use underscore after the types , here is a link, and biscally numpy does that so that these types do not clash with the same type define in python (without underscore) -> https://stackoverflow.com/questions/6205020/numpy-types-with-underscore-int-float-etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.21) Multi-class Cross Entropy\n",
    "- i need to answer the question at the end of the video : how do we say that the cross entropy formula is the same for the multi class m>2 and for m=2 while we saw that it was different form at m=2.\n",
    "- first i want to point out that the previous case it think it was m=2 class, but in the example of the three animals, it is 3*animals i.e it is 6 classes because each animal defines two classes (exists or does not exist)\n",
    "- but the problem is that the summation is from j=0 to m, which (from looking at the doors) implies that m=3 not 6 !! \n",
    "- so here comes a second question, if we have 3 animals and two doors, how will we separate the number of doors and the number of animals in the formula, i think there is a lot of ambiguity in the example given in this topic of multi-class cross Entropy\n",
    "- third question, whay is the meaning of \"Cross\" in the Cross entropy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.22) Logistic Regression\n",
    "- Note: at the end of the video, $x^i$ denotes each point in our input data samples, because remember :\n",
    "    - $Wx+b$ is the equation of the line (this defines a line to be drawn, a seperator)\n",
    "    - $Wx^i+b$ this is not an equation of a line, but rather, it is an equation where we substitute $x$ by a point: $x^i$, so making this substitution will give as a single value (a number, not a line nor a separator)\n",
    "    - also rememebr that $x^i$ is a pair of values : $(x_1,x_2)$, i.e $x^i$ is a 2x1 vecotor (or 1x2 i dont remembr) , (or nx1 for higher dimensions.  also it could be 1xn, i will revise and see)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.22) Gradient Descent\n",
    "- <font color=red>**Q\\**</font> in calculating $\\frac{\\partial }{\\partial w_j}E$ , why did he considered $y$ to be constant w.r.t $w_j$ (i.e $\\frac{\\partial }{\\partial w_j}y=y$)? I ask this because as far as i know , $y=Wx+b$, so $y$ is **not** constant w.r.t $w_j$, right ? <br>\n",
    "- <font color=blue> **No my dear friend** </font> , I knew later that $y$ is the label ,(e.g in a binary class classification, y is 0 or 1), so it is a consntat , but this raises another question, if $\\hat{y}=Wx^i+b$ , so <font color=red>**Q\\**</font> what is the variable assgined to (or used to name) the hypothesis : (var? $= Wx+b$) ? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.23) Logistic Regresion Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.24) Pre-Lab: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.25) Notebook: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.26) Perceptron vs Gradient Descent\n",
    "it is importnat to note the values that can be taken by $\\hat{y}$ I both GD and Percptron algo.. note in which it is it continuous and in which it is discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.27) Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center> Example of code writing equations here, not in inline mode, so that all letters appear well (inline makes them very small)</center>**  <br>\n",
    "\\begin{equation*}\n",
    "P(E)   = {n \\choose k} p^k (1-p)^{ n-k}\n",
    "\\end{equation*}\n",
    "\n",
    "**<center> or make a boxed equation (i can use it for final results or for hint to recall previous equations say for derivative of $ln$, etc)</center>**  <br>\n",
    "\\begin{equation*}\n",
    "    \\boxed{P(E)   = {n \\choose k} p^k (1-p)^{ n-k}}\n",
    "\\end{equation*}\n",
    "\n",
    "**<center> what i want to do is : before any mathematical derivation, i list the mathimatical rules that i am gonna use, so that i remember it easily . i may preceed these equation by the ward \"remember:\"</center>**  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : Cloud Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.1) Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.2) Create an AWS Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.3) Get Access to GPU Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.4) Launch an Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.5) Login to the Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : Deep Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.1) Non-linear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.2) Continuous Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.3) Non-linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.4) Neural Networks Architecture\n",
    "- <font color=red>**Q\\**</font> when combining two nns, why take the sigmoid of the addition of the two points? i know that he said that we want the scale to be between 0 and 1, but we could've used averaging and it would achieve the same goal!! ... i may try both and see the resutling boundries (i should know how to plot these things!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.5) Feedforward\n",
    "- at video 1 : feedforward\n",
    "    - <font color=red>**Q\\**</font> at $0:41$, how did he know that $w_1$ is bigger than $w_2$ ?! is this implied from the drwaing ? i want to know this !!\n",
    "    - <font color=red>**Q\\**</font> at $3:34$, what is that strange circle between matrices ? is this a multiplication operatot or what ?!\n",
    "    - <font color=red>**Q\\**</font> what does `Dense` mean ?\n",
    "    - <font color=red>**Q\\**</font> what if i made a mistake (or changed my mind) and wanter to change something in the middle of the path of the network (e.g an activation function)?\n",
    "    - <font color=red>**Q\\**</font> it is written \"We can see that the output has dimension 1.\" , where can i see that ???\n",
    "    - <font color=red>**Q\\**</font> in the quiz, \n",
    "        - when he starts with a random seed to the numpy package, what does this line affect afterword?\n",
    "        - when he did \"One-hot encoding the output\", what was the shape of y before and after this step? (this will tell me why did he do this in the first place! i ask this because i see that the output is not categorical, so why did he do that ! )\n",
    "        - i skipped this quiz because there are lots of things that i do not understand. (كلفتونى أوى فى الكيراس!!) i will return and do it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.6) Backpropagation\n",
    "- **Recommendation from eng.mohamed hammad** look at : \n",
    "    - https://github.com/DebPanigrahi/Machine-Learning/blob/master/back_prop.pdf\n",
    "    - https://www.facebook.com/mohamed.hamedhammad/posts/2503343279738207\n",
    "    - https://www.facebook.com/mohamed.hamedhammad/posts/2538529736219561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.7) Keras\n",
    "- <font color=red>**Q\\**</font> why he writes `dtype=np.float32` in defining X and y ??\n",
    "- <font color=red>**Q\\**</font> why `X.shape[1]` what does it mean, and why not `X.shape[0]`?? i need to get the code here and see each output of these things and i shall demonstrate each line myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.8) Pre-Lab: Student Admission in Keras</font>\n",
    "- what does dense do ?\n",
    "- why did he put 32 as the first input to the neuron ?\n",
    "- how do we choose the number of hidden layers and the number of neurons in each layer ?\n",
    "- i want to see the effect of adding/changing each neuron/layer/activation function. <br><br>\n",
    "\n",
    "- <font color=blue> **answer** </font> the last two questions are part of the DNN hyperparamater tuning, do the changes in lab \"(4-4.8) Mini project: Training an MLP on MNIST\n",
    "\" to get the intuition, and see this link to know how to tune these parameters -> https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.9) Lab: Student Admission in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.10) Training Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.11) Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.12) Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.13) Regularization 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.14) Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-315) Local Minima\n",
    "- <font color=red>**Q\\**</font> what is the solution ?!!\n",
    "    - <font color=blue> the answer is at (4-3.20) Random Restart and (4-3.21) Momentum</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.16) Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.17) Other Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.18) Batch vs Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.19) Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.20) Random Restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.21) Momentum\n",
    "- <font color=red>**Q\\**</font> I don't understand why the momentum idea behaves in such manner! i mean, what is so special about the global minimum that makes the algorithm converge to it ? what if the global minimum was not so deep (surrounded by low humps that the algorithm may also go over it !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.22) Optimizers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.23) Error Functions Around the World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.24) Neural Network Regression\n",
    "- at $1:30$, he says \"the way we turn a linear function into a ReLU is by turning off the parts of the negative values (or underneath the x-axis into zero)\"\n",
    "    - I think by underneath x-axis he means (to the left of the y-axis, i.e the negative part of the x-axis)\n",
    "- <font color=red>**Q\\**</font> why we bother and use ReLU ? why not just use a linear function ? (what is the advantgae of ReLU over the linear function)    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.25) Neural Networks Playground\n",
    "How did Jay alammar made these beautiful interaction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.26) Mini Project Intro </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-3.27) Pre-Lab: IMDB Data in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-3.28) Lab: IMDB Data in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.29) Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.1) Introducing Alexis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.2) Applications of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.3) How Computers Interprets Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.4) MLPs for Image Classification\n",
    "Q\\ what is the input shape in `model.add(Flatten(input_shape=X_train.shape[1:]))` ? i mean whay is [1:] does ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.5) Categorical Cross-Entropy\n",
    "- Q\\ `accuracy = 100*score[1]` what is returnd into the score var, so that we take its 2nd element  ?\n",
    "    - for the line `score = model.evaluate(X_test, y_test, verbose=0)` the documentation says: evaluate *\"Returns (1)the loss value & (2)metrics values for the model\"* \n",
    "    - this makes sense, but i still want to know what is the *\"the loss value\" is it the error value ? how does it compare with the metric value ? why would i need both numbers?*\n",
    "        - answer, in an upcoming video, i knew that : Overfitting is detected by comparing the validation loss to the training loss. If the training loss is much lower than the validation loss, then the model might be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.6) Model Validation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.7) When do MLPs (not) work well?\n",
    "- unitl perior of here, all video were about MLPs (Deep nn).\n",
    "- here the video started to introduce CNN and compare it with DNN (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-4.8) Mini project: Training an MLP on MNIST </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.9) Local Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.10) Convolution Layers (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.11) Convolution Layers (Part 2) \n",
    "- Q\\ why does she use ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.12) Stride and Padding  \n",
    "(stride = |noun| a long, decisive step. |verb|walk with long, decisive steps in a specified direction.  ) <br>\n",
    "(Padding = حشوة أو حشو)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.13) Convolutional Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.14) Quiz: Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.15) Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.16) Max Pooling Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.17) CNNs for Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.18) CNNs in Keras: Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.19) Mini project: CNNs in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.20) Image Augmentation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.21) Mini project: Image Augmentation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.22) Groundbreaking CNN Architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.23) Visualizing CNNs (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.24) Visualizing CNNs (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.25) Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.26) Transfer Learning in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links to tensor flow by jay alammar https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 403,
   "position": {
    "height": "40px",
    "left": "921px",
    "right": "20px",
    "top": "79px",
    "width": "335px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
