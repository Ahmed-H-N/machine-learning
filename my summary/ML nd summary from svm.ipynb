{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-5.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-5.5) Perceptron Algorithm\n",
    "- in Wx+b=0 or =1,2 etc (which is equivalent of making b:=b-1 or b-2 etc)\n",
    "    - <div dir=rtl> برأيى إن سبب إن بينتج خطوط متوازية من تغيير ال b لأن الميل ثابت ، حيث تذكر إن الميل بيأثر فيه المتجه W</div>\n",
    "\n",
    "### (3-5.7) Margin Error\n",
    "> لحد الآن أنا شايف إنه هو نفس الخط ، لكن لما ضرب ال دبليو و ال بى فى اتنين، فبالرغم إنه نفس الخط لكن الماردن اتغير\n",
    "    اللى لسة عاةز أشوفه ، أمال استفدنا احنا ايه لما هو نفس الخط ؟! <br>\n",
    "    اااه اعتقد لسة احنا مش بنقول هنستفاد إيه ، احنا بس بنعمل صيغة و بنوضح إزاى ممكن المارجن ندخله فى معادلة الايرور ، فأكيد هنشوف دا بعد شوية إزاى هنستخدمه ، أكيد انا منتظر إن الخط يميل وكدا ويتحرك و يتغير صح ؟ هشوف\n",
    "- المهم تلاحظ من الفيديو دا إزاى بيزيد أو يقل المارجن بإننا بنضرب معاملات المعادلة كلها فى ثابت\n",
    "\n",
    "### (3-5.8) (Optional) Margin Error Calculation \n",
    " عاوز أفهم آخر جملة قبل الرسمة 4 <br>\n",
    "بعد الرسمة 4 ، ليه اشترط إن مدام المقام هو قيمة مطلقة ، فنقدر نعمل اللى عمله... طب لو كان المقام قيمة متجهة ،، مكنش هينفع؟ ليه؟\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-6.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-6.9) AdaBoost in sklearn\n",
    "أنا مش فاهم دلوقت هل فى خطأ فى الشرح لما قال إن <br> \n",
    "model has been fitted to a tree ??!!\n",
    "\n",
    "فبالتالى أنا عاوز أعرف هل أنا بعمل <br> \n",
    "fitting to the model first then use tha AdaBoost ? or it is demonstrated wrongly in that section? <br><br>\n",
    "\n",
    "<div dir=rtl>\n",
    "عاوز أجرب كذا base_estimator و أشوف بيعملوا إيه ،، و كمان هل ممكن أعمل ال base_estimator خوارزميات مختلفة ؟ ولا لازم نفس الخوارزيمة هى اللى بيتم استخدامها كذا مرة ؟\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.1) Announcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.2) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.3) Classification Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.4) Classification Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.5) Linear Boundries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.6) Higher Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.7) Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.8) Perceptron as Logical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.9) Why \"Neural Networks\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.10) Perceptron Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.11) Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.11) Non-Linear Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.12) Error Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.13) Log-loss Error Function \n",
    "- Nice reasoning for why we can't use \"count of error points\" as the error metric (b/c it is discrete and discrete functions are not continuous, and we really need a continuous error function (and differentiable) so that a small change in the precision will be captured as transition (change) in the output of the funtion) (but in discrete error func, we can change the function of the line , but the error is still the same). <br>\n",
    "- Note: don't confuse the error function (loss function) with the function of the line (the hypothesis)\n",
    "- <font color=red>**Q\\**</font> what is the \"loss\" in \"log-loss error\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.14) Discrete vs Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.15) Softmax\n",
    "- i need to know: how softmax turns into sigmoid when n=2 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.16) One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.17) Maximum likelihood\n",
    "- my note: the reason that made the first model give low overall probability is that : the misclassified points have low probability of belonging to their right region. ex, the misclassified red point , has P(blue) =0.9 , and hence have P(red)0.1, but notice that we use the latter in calculating the whole probability of the model, i.e we use the P(red) fot the red point, and P(blue) if the point is actuallu blue. <br>\n",
    "- I might ask, what is the meaning of the red point having high P(blue)=0.9? and i will tell that this is the wrong prediction made by the model . the model assumes that this point has higher probability of being blue because it lies far in the blue region.\n",
    "- i still need a formal definition of P(all) and maximum liklihood , are they the samething or do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.18) Maximizing Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.19) Cross-Entropy 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.20) Cross-Entropy 2\n",
    "In the quiz, I stayed half an hour searching for the error, and it was that i  made this line as <br>\n",
    "`for i in Y` <br>\n",
    "when it should be <br>\n",
    "`for i in range(len(Y))` <br>\n",
    "because I want `i` to be an increasing index, not each element in the `Y` list <br>\n",
    "وكدا كدا الحل دايما مش بيعمل فور لوب كدا، بل دا تفكيرك عشان انت متعود على السى ، لكن هنا اللستات بتتشقلب مع بعض عادى بدون لوب <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**why the udacity solution made the np.float_ to the two lists?** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.828313737302301\n",
      "[0. 1. 0. 0.]\n",
      "[0.6 0.4 0.9 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y=[1,0,1,1] \n",
    "P=[0.4,0.6,0.1,0.5]\n",
    "\n",
    "Y_new = np.float_(Y)\n",
    "P_new = np.float_(P)\n",
    "\n",
    "CE_entropy = -np.sum(Y_new * np.log(P_new) + (1 - Y_new) * np.log(1 - P_new)) # note that np.log is the natural log (ln)\n",
    "print (CE_entropy)\n",
    "\n",
    "# 1- Y , ERROR , can't do subtraction operation between tupes : int and  list\n",
    "print (1-Y_new)\n",
    "print (1-P_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y is python list , and you cant do operation such as `1-Y` on a python list. <br>\n",
    "Y_new is a numpy ndarray (n dimensional array), which can be operated on as `1-Y_new` and the result is also ndarray. <br>\n",
    "so np.float_(Y) casts the list to an ndarray  of float elements <br>\n",
    "I might ask, why he made it `float`, not `int`? well. obviously float is the general case, and since we almost always will have fractions (like we do in the P list) so we have to cast the lists to float lists <br>\n",
    "you might look at the ndarray and its function in the numpy documentaion : https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to know why numpy use underscore after the types , here is a link, and biscally numpy does that so that these types do not clash with the same type define in python (without underscore) -> https://stackoverflow.com/questions/6205020/numpy-types-with-underscore-int-float-etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.21) Multi-class Cross Entropy\n",
    "- i need to answer the question at the end of the video : how do we say that the cross entropy formula is the same for the multi class m>2 and for m=2 while we saw that it was different form at m=2.\n",
    "- first i want to point out that the previous case it think it was m=2 class, but in the example of the three animals, it is 3*animals i.e it is 6 classes because each animal defines two classes (exists or does not exist)\n",
    "- but the problem is that the summation is from j=0 to m, which (from looking at the doors) implies that m=3 not 6 !! \n",
    "- so here comes a second question, if we have 3 animals and two doors, how will we separate the number of doors and the number of animals in the formula, i think there is a lot of ambiguity in the example given in this topic of multi-class cross Entropy\n",
    "- third question, whay is the meaning of \"Cross\" in the Cross entropy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.22) Logistic Regression\n",
    "- Note: at the end of the video, $x^i$ denotes each point in our input data samples, because remember :\n",
    "    - $Wx+b$ is the equation of the line (this defines a line to be drawn, a seperator)\n",
    "    - $Wx^i+b$ this is not an equation of a line, but rather, it is an equation where we substitute $x$ by a point: $x^i$, so making this substitution will give as a single value (a number, not a line nor a separator)\n",
    "    - also rememebr that $x^i$ is a pair of values : $(x_1,x_2)$, i.e $x^i$ is a 2x1 vecotor (or 1x2 i dont remembr) , (or nx1 for higher dimensions.  also it could be 1xn, i will revise and see)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.22) Gradient Descent\n",
    "- <font color=red>**Q\\**</font> in calculating $\\frac{\\partial }{\\partial w_j}E$ , why did he considered $y$ to be constant w.r.t $w_j$ (i.e $\\frac{\\partial }{\\partial w_j}y=y$)? I ask this because as far as i know , $y=Wx+b$, so $y$ is **not** constant w.r.t $w_j$, right ? <br>\n",
    "- <font color=blue> **No my dear friend** </font> , I knew later that $y$ is the label ,(e.g in a binary class classification, y is 0 or 1), so it is a consntat , but this raises another question, if $\\hat{y}=Wx^i+b$ , so <font color=red>**Q\\**</font> what is the variable assgined to (or used to name) the hypothesis : (var? $= Wx+b$) ? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.23) Logistic Regresion Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.24) Pre-Lab: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.25) Notebook: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.26) Perceptron vs Gradient Descent\n",
    "it is importnat to note the values that can be taken by $\\hat{y}$ I both GD and Percptron algo.. note in which it is it continuous and in which it is discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.27) Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center> Example of code writing equations here, not in inline mode, so that all letters appear well (inline makes them very small)</center>** \n",
    "\n",
    "\\begin{equation*}\n",
    "P(E)   = {n \\choose k} p^k (1-p)^{ n-k}\n",
    "\\end{equation*}\n",
    "\n",
    "**<center> or make a boxed equation (i can use it for final results or for hint to recall previous equations say for derivative of $ln$, etc)</center>** \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\boxed{P(E)   = {n \\choose k} p^k (1-p)^{ n-k}}\n",
    "\\end{equation*}\n",
    "\n",
    "**<center> what i want to do is : before any mathematical derivation, i list the mathimatical rules that i am gonna use, so that i remember it easily . i may preceed these equation by the ward \"remember:\"</center>**  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i want to say that cases are great $ \\begin{cases} one \\\\ two \\\\ three \\end{cases} $ i also can complete here  <br> and can complete down there ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : Cloud Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.1) Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.2) Create an AWS Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.3) Get Access to GPU Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.4) Launch an Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.5) Login to the Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : Deep Neural Networks\n",
    "- References :\n",
    "    - A post shared by ibrahim sobh: a recipe for training nn : \n",
    "        - http://karpathy.github.io/2019/04/25/recipe/\n",
    "    - 37 reasons why your neural network is not working (other than overfitting) , i think he talks about the implementation itself\n",
    "        - medium link https://medium.com/m/global-identity?redirectUrl=https%3A%2F%2Fblog.slavv.com%2F37-reasons-why-your-neural-network-is-not-working-4020854bd607%3Ffbclid%3DIwAR0FtIi4E6FSyn2anmsCcmnrzCHt9OLSlciXm4O9YT4WGVohiQnFO2X9M0Q\n",
    "        - other link if the above did not work https://weekly-geekly.github.io/articles/334944/index.html\n",
    "    - deep mind tweet: For neural networks to be deployed in the real world, we need guarantees that our network satisfy desired specifications. We introduce a method to verify complex non-linear specifications. :https://twitter.com/DeepMindAI/status/1125503318749581312\n",
    "        - link of the paper : https://arxiv.org/pdf/1902.09592.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.1) Non-linear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.2) Continuous Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.3) Non-linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.4) Neural Networks Architecture\n",
    "- <font color=red>**Q\\**</font> when combining two nns, why take the sigmoid of the addition of the two points? i know that he said that we want the scale to be between 0 and 1, but we could've used averaging and it would achieve the same goal!! ... i may try both and see the resutling boundries (i should know how to plot these things!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.5) Feedforward\n",
    "- at video 1 : feedforward\n",
    "    - <font color=red>**Q\\**</font> at $0:41$, how did he know that $w_1$ is bigger than $w_2$ ?! is this implied from the drwaing ? i want to know this !!\n",
    "    - <font color=red>**Q\\**</font> at $3:34$, what is that strange circle between matrices ? is this a multiplication operatot or what ?!\n",
    "    - <font color=red>**Q\\**</font> what does `Dense` mean ?\n",
    "    - <font color=red>**Q\\**</font> what if i made a mistake (or changed my mind) and wanter to change something in the middle of the path of the network (e.g an activation function)?\n",
    "    - <font color=red>**Q\\**</font> it is written \"We can see that the output has dimension 1.\" , where can i see that ???\n",
    "    - <font color=red>**Q\\**</font> in the quiz, \n",
    "        - when he starts with a random seed to the numpy package, what does this line affect afterword?\n",
    "        - when he did \"One-hot encoding the output\", what was the shape of y before and after this step? (this will tell me why did he do this in the first place! i ask this because i see that the output is not categorical, so why did he do that ! )\n",
    "        - i skipped this quiz because there are lots of things that i do not understand. (كلفتونى أوى فى الكيراس!!) i will return and do it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.6) Backpropagation\n",
    "- **Recommendation from eng.mohamed hammad** look at : \n",
    "    - https://github.com/DebPanigrahi/Machine-Learning/blob/master/back_prop.pdf\n",
    "    - https://www.facebook.com/mohamed.hamedhammad/posts/2503343279738207\n",
    "    - https://www.facebook.com/mohamed.hamedhammad/posts/2538529736219561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.7) Keras\n",
    "- <font color=red>**Q\\**</font> why he writes `dtype=np.float32` in defining X and y ??\n",
    "- <font color=red>**Q\\**</font> why `X.shape[1]` what does it mean, and why not `X.shape[0]`?? i need to get the code here and see each output of these things and i shall demonstrate each line myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.8) Pre-Lab: Student Admission in Keras</font>\n",
    "- what does dense do ?\n",
    "- why did he put 32 as the first input to the neuron ?\n",
    "- how do we choose the number of hidden layers and the number of neurons in each layer ?\n",
    "- i want to see the effect of adding/changing each neuron/layer/activation function. <br><br>\n",
    "\n",
    "- <font color=blue> **answer** </font> the last two questions are part of the DNN hyperparamater tuning, do the changes in lab \"(4-4.8) Mini project: Training an MLP on MNIST\n",
    "\" to get the intuition, and see this link to know how to tune these parameters -> https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.9) Lab: Student Admission in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.10) Training Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.11) Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.12) Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.13) Regularization 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.14) Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-315) Local Minima\n",
    "- <font color=red>**Q\\**</font> what is the solution ?!!\n",
    "    - <font color=blue> the answer is at (4-3.20) Random Restart and (4-3.21) Momentum</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.16) Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.17) Other Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.18) Batch vs Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.19) Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.20) Random Restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.21) Momentum\n",
    "- <font color=red>**Q\\**</font> I don't understand why the momentum idea behaves in such manner! i mean, what is so special about the global minimum that makes the algorithm converge to it ? what if the global minimum was not so deep (surrounded by low humps that the algorithm may also go over it !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.22) Optimizers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.23) Error Functions Around the World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.24) Neural Network Regression\n",
    "- at $1:30$, he says \"the way we turn a linear function into a ReLU is by turning off the parts of the negative values (or underneath the x-axis into zero)\"\n",
    "    - I think by underneath x-axis he means (to the left of the y-axis, i.e the negative part of the x-axis)\n",
    "- <font color=red>**Q\\**</font> why we bother and use ReLU ? why not just use a linear function ? (what is the advantgae of ReLU over the linear function)    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.25) Neural Networks Playground\n",
    "How did Jay alammar made these beautiful interaction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.26) Mini Project Intro </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-3.27) Pre-Lab: IMDB Data in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-3.28) Lab: IMDB Data in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.29) Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read this tweet from ian goodfellow about OctConv which is a simple replacement for the traditional convolution operation that gets better accuracy with fewer FLOPs <br>\n",
    "https://twitter.com/goodfellow_ian/status/1117929612200120320?s=19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.1) Introducing Alexis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.2) Applications of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.3) How Computers Interprets Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.4) MLPs for Image Classification\n",
    "Q\\ what is the input shape in `model.add(Flatten(input_shape=X_train.shape[1:]))` ? i mean whay is [1:] does ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.5) Categorical Cross-Entropy\n",
    "- Q\\ `accuracy = 100*score[1]` what is returnd into the score var, so that we take its 2nd element  ?\n",
    "    - for the line `score = model.evaluate(X_test, y_test, verbose=0)` the documentation says: evaluate *\"Returns (1)the loss value & (2)metrics values for the model\"* \n",
    "    - this makes sense, but i still want to know what is the *\"the loss value\" is it the error value ? how does it compare with the metric value ? why would i need both numbers?*\n",
    "        - answer, in an upcoming video, i knew that : Overfitting is detected by comparing the validation loss to the training loss. If the training loss is much lower than the validation loss, then the model might be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.6) Model Validation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.7) When do MLPs (not) work well?\n",
    "- unitl perior of here, all video were about MLPs (Deep nn).\n",
    "- here the video started to introduce CNN and compare it with DNN (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-4.8) Mini project: Training an MLP on MNIST </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.9) Local Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.10) Convolution Layers (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.11) Convolution Layers (Part 2) \n",
    "- Q\\ why does she use ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.12) Stride and Padding  \n",
    "(stride = |noun| a long, decisive step. |verb|walk with long, decisive steps in a specified direction.  ) <br>\n",
    "(Padding = حشوة أو حشو)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.13) Convolutional Layers in Keras\n",
    "- Q\\ in the reading it says \"You are **strongly encouraged** to add a ReLU activation function to **every** convolutional layer in your networks.\" why is that ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.14) Quiz: Dimensionality\n",
    "- I will demonstrate the equation of the width of the conv layer **in the case of `padding = 'valid'`**\n",
    "\\begin{equation*}\n",
    "width = ceil(float( \\frac{W_{in} - F + 1}{float(S)}))\n",
    "\\end{equation*}\n",
    "- for the nomenator `W_in - F + 1` , i was able to think about it like this : \n",
    "    - `W_in - (F - 1)` which says: if i have a width W_in , and a filter with with width F, how many steps are required for the filter to slide over the W_in (assuming that stride (طول الخطوة) is 1)? the answer is `W_{in} - (F - 1)` , and for the `-1` you can understand it by thinking of some examples :\n",
    "        - if W_in =6 , and F=3  , it will require the filter to do 4 steps to reach the end of the image (try to do it manually to see it)\n",
    "        - also think of W_in=6 and F=2, it will take you 5 steps\n",
    "    - now the previous assumed that the stride is 1, if the stride (طول الخطوة) was S, simply devide on it to get the number of steps using this new stride. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.15) Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.16) Max Pooling Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.17) CNNs for Image Classification\n",
    "- at $1:16$, why she said \"where the spatial dimension is a **power of 2** or else a number that is **divisible by a large power of 2**\" . why exactly is these two constrains ?:\n",
    "    - **power of 2**\n",
    "    - **divisible by a large power of 2** (also why **large** power of 2 , not any power ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.18) CNNs in Keras: Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-4.19) Mini project: CNNs in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.20) Image Augmentation in Keras\n",
    "- Q\\ when the reading says : \"where x_train.shape[0] corresponds to the number of unique samples in the training dataset x_train.\" , i think by unique means \"not including the augmentation\" right ?\n",
    "- i don't understand the reading that says \"By setting steps_per_epoch to this value, we ensure that the model sees x_train.shape[0] augmented images in each epoch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-4.21)  Mini project: Image Augmentation in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.22) Groundbreaking CNN Architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.23) Visualizing CNNs (Part 1)\n",
    "- Q\\ at $1:06$, i do not whay does \"take a filter from a cnn, then construct images that maximize the activation of that filter \" mean ? \n",
    "    - answer found in this link , under the title \"Retrieving images that maximally activate a neuron\" -> http://cs231n.github.io/understanding-cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.24) Visualizing CNNs (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.25) Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.26) Transfer Learning in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Deep Learning for Cancer Detection with Sebastian Thrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.2) Skin Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.3) Survival Probability of Skin Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.4) Medical Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.5) The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.6) Image Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.7) Quiz: Data Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.8) Solution: Data Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.9) Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.10) Quiz: Random vs Pre-initialized Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.11) Solution: Random vs Pre-initialized Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.12) Validation the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.13) Quiz: Sensitivity and Specifity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.14) Solution: Sensitivity and Specifity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.15) More on Sensitivity and Specifity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.16) Quiz: Diagnosing Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.17) Solution: Diagnosing Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.18) Refresh on ROC curves\n",
    "عجبنى جدا الأنيميشن اللى فى فى آخر فيديو فى الصفحة ، برأيى هعمل زيه و أكتب (فى الريبو بتاع الإنتويشن) الاستفادة من الأنيميشن دا إيه أو الرسمة يعنى دى ناخد منها (نبنى عليها) قرارات إزاى ،،، و هشير الفيديو دا فى لنكد إن وفى تويتر و فى جروب الفيس بتاع الذكاء الاصطناعى"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.19) Quiz: ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.20) Solution: ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.21) Comparing our Results with Doctors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.22) Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.23) What is the network looking at?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.24) Refresh on Confusing Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.25) Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.26) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.27) Useful Resoucres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.28) Mini Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-5.29) Mini Project: Dermatologist AI </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.30) Share your Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Deep Learning Assesment\n",
    "QUESTION 2 OF 5: <br>\n",
    "my asnwer is calculating : $3*2 + 3*1$\n",
    "\n",
    "QUESTION 3 OF 5: <br>\n",
    "I need to understand why!\n",
    "\n",
    "QUESTION 4 OF 5: <br>\n",
    "what is the kernel ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Unsupervised Learning \n",
    "### introduction and sub-summary\n",
    "in this unit we have 9 lessons: <br>\n",
    "- lesson 9: we learn two concepts\n",
    "    - Random Projection : \n",
    "        - same as PCA but the line is chosen randomly.\n",
    "        - it is faster than PCA\n",
    "        - we can specify epslon or n_compeonets\n",
    "    - ICA : (Independit component analysis)\n",
    "        - statistal method applied when we think that features are statistically independet\n",
    "        - common use cases \n",
    "            - in seprating voice in the cocktail party problem\n",
    "            - in seperating signals in EEG\n",
    "            - some finance application but it is rarely useful to use ICA in finance since financial systems are very complex.\n",
    "        - it has a mathematical derivation, but the most importnat thing that you have to know is the assumptions that is considered to make this algo work, they are :\n",
    "            - a\n",
    "            - a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Clustreing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-.6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.11) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.13) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.14) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.15) \n",
    "note that the K-means algorithm is a hill climbing algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.17) \n",
    "important rule of thumps : the more classes u have, the more local minimum will exists, the more you need to run K-means algorithm to reach to the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : Clustering Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.2) k-means Clustering of Movie Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.3) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : Hierarchical Density-based Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.1) K-means considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.2) Overview of other clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.3) Hierarchical clustering: single-link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.4) Examining single-link clustering\n",
    "- i was wondering how the coloring of clusters is done in this algotithm (the final result) (b/c the dendrogram goes all the way to 1 cluster!)? \n",
    "    - and i think the answer is that the algortithm stops depending on a threshold that is set to tell us that two cluster should not be merged. I think this is mapped to the dendrogram when we see a long vertical distance before two clusters are merged (as Jay said in the video about the long vertical distance in the dendrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.5) complete-link, average-link, Ward\n",
    "- I don't understand how subtracting $A_1^2, A_2^2, B_1^2, B_2^2$ reduces variance ! what is the variance anyway in the context of clustering? (i want to visualize clusters with high variance and with low variance to compare the two.)\n",
    "- Q\\ $A_1 = A_2$ and $B_1 = B_2$ right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.6) Hierarchical clustering implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.7) [Lab] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.8) [Lab Solution] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.9) HC examples and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.10) [Quiz] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.11) DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.12) DBSCAN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.13) [Lab] DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.14) [Lab Solution] DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.15) DBSCAN examples & applications\n",
    "- it is very important to me to know that DBSCAN is used as anomaly detection (detecting outliers in a dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.16) [Quiz] DBSCAN\n",
    "- Question 2 of 2 : imporntant to note that DBSCAN is not a hierarchical clustering method, but is a density-based clustering method that is robust against noisy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Gaussian Mixture models and Cluster Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.2) Gaussian Mixture Model (GMM) Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.3) Gaussian Distribution in One Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.4) GMM Clustering in One Dimension\n",
    "Q\\ how did he know that the physics test was hard by looking at its distribution of scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.5) Gaussian Disrtibution in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.6) GMM in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.7) Quiz: Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.8) Overview of The Expectation Maximization (EM) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.9) Expectation Maximization Part 1\n",
    "- I want to study the things in statistics and probability that was mentioned in this video\n",
    "- i want to define soft clustering, is it assigning probabilities to points of membership to the available clusters? i will search for the proper definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.10) Expectation Maximization Part 2\n",
    "- in the weighted average calculation, i was surprised that we did not devide by the number of elements, but we devided by the sum of the probabilities (since all probabilities are less than 1, so this sum is sure less than the number of points).\n",
    "    - 1- i want to know why\n",
    "    - 2- i want to compare this with if i had devided by the number of points, and i want to visualize both and get an intuition about the difference between the right method and my wrong assumption.\n",
    "- i want to know what is the log-likelihood, an what is mixing coefficient (note: they are in statistics)\n",
    "- Q\\ what is the \"1n\" that is in the video that precedes the equation of evaluating the log-likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.11) Visual Example of EM Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.12) Quiz: Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.13) GMM Implementation\n",
    "- Q\\ i want to something about the content of the array that contains the predicted labels, what if a point is inside two clusters (i.e when the two clusters overlap as the case in the previous video), to which cluster will it be assigned ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note:** on the difference between probability and membership\n",
    "- The term \"membership\" got me thinking if there is a difference between the two terms : membership and probability. \n",
    "    - first i assumed that i will say \"probabiliy of X\" when X is an event in the future that may have some kind of probability to happen, while i will say \"the membership of X\" when i talk/analyse something that already exists, such as saying that the movie (which is something already exists) is a member of (or belonging to) the genre action (by 80%) and romance (by 20%).\n",
    "    - i also assumed that probability and membership have the same mathematical tools, but we say probabilty of membership depending on the thing that we study\n",
    "- my assumptions were not so bad, as i found out that there are two fields that are related alot : probability and fuzzy logic (the latter has the membership function). and with little search i found that there are differences. look at the following references : <br>\n",
    "https://goodmath.scientopia.org/2011/02/02/fuzzy-logic-vs-probability/ <br>\n",
    "https://www.researchgate.net/post/Difference_between_fuzzy_logic_and_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.14) GMM Examples & Applications\n",
    "- I am wondering if i can mix up GMM with CNN and come up with useful things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.15) Cluster Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.16) Cluster Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.17) External Validation indices\n",
    "- i want to compare the `external validation idnex` to calculating the `accuracy` of of the clustering (the ratio of the points that was correctly clustered). \n",
    "    - Q\\ why we needed the `external validation idnex` when we could just use the `accuracy`?\n",
    "        - i think the answer lies in the objective we need when we asses the clustering, it is not just how many points are correctly clustered, but rather there were two objectives that was mentioned in the previous video that assess the clustering, namely: being compact and separated.\n",
    "    -Q\\ what does it mean for `ARI` to be equal to `0` ? what about `-1` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.18) Quiz: Adjusted Rand index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.19) Internal Validation Indices\n",
    "- Q\\ why the formula devides by $max(a_i,b_i)$ , i mean 1- why we devide? 2- why we use the max (what is the benefit of using the maximum?)!? \n",
    "    - is it because this normailize the output of this index to be between `-1` and `1` ? (i still need to see when it becomes `-1` and when it becomes `1` and when it becomes `0` or `-0.5` etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.20) Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.21) GMM & Cluster Validation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.22) GMM & Cluster Validation Lab Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.1) Chris's T-shirt Size (intuition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.2) A Metric for Chris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.3) Height + Weight for Cameron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.4) Sarah's Height + Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.5) Chris 's Shirt Size by Our Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.6) Comparing Features with Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.7) Feature Scaling Formula Quiz 1\n",
    "- For the formula of feature scaling, I, Ahmed, want to demonstrated this formula.\n",
    "- so the formula is as follows:\n",
    "\\begin{equation*}\n",
    "x' = \\frac{x-x_{min}}{x_{max}-x_{min}}\n",
    "\\end{equation*}\n",
    "- take for example the array [10 , 15 , 20]\n",
    "- the numerator works on shifting the array so that the minimum number becomes zero, so when we do $x-x_{min}$ the resulting array is [0 , 5 , 10]\n",
    "- and the denominator works on scaling the result array by deviding on its range, the range would be max of the resulting new array (=10) or it could be calculated from the old array as $x_{max}-x_{min}$,\n",
    "- so the final result becomes [0 ,  0.5 ,  1]\n",
    "- note : \n",
    "    - $x$ represents each element in the old array (the old one),  $x_{min}$ and $x_{max}$ are single elements in the array (the old one)\n",
    "    - $x'$ represents each new element in the final result (i.e each rescaled element)\n",
    "- when I demonstrate this concept to students, i may say that different range of values introduce a problem in our ML algorithm, since our algorithms usually use \"addition\" [i need to give example here], and we already know (in the course of numercal  analysis for example) that when we add large number with small number, the small number contribute to the result in an non-sensible way (we actuall sometimes ignore the small term in the addition when it is added to a large number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.8) Feature Scaling Formula Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.9) Feature Scaling Formula Quiz 3\n",
    "- it is important to note the advantage and disadvante of rescaling to the range of [0 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(5-5.10) Min/Max Rescaling Codeing Quiz</font>\n",
    "- a very important note was given in that quiz which is, the straight forward solution will be prone to divide-by-zero error.\n",
    "    - i should always think of this error whenever i implement anything that has devision operation in it\n",
    "- look here to see several ways to make an operation (such as subtracting a number) on every element on a python list : \n",
    "    - https://stackoverflow.com/questions/4918425/subtract-a-value-from-every-number-in-a-list-in-python\n",
    "    - https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.11) Min/Max Scaler in sklearn\n",
    "- i did not understand the part when she talked about the content of the numpy array! (she said things about each training point and each feature, and i did not understand that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.12) Quiz on Algorithms Requiring Rescaling\n",
    "- important video!! and i want analyse (maybe search online for fast answers) the other ML algorithms i took so far to see which ones are affected by features ranges (so we have to do feature scaling with them) and which ones do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.1) Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.2) Trickier Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.3) One-Dimensional, or Two?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.4) Slightly Less Perfect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.5) Trickiest Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.6) PCA for Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.7) Center of a New Coordinate System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.8) Principal Axis of New Coordinate System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.9) Second Principal Component of New System\n",
    "- Q\\ i did not understand how did he made the $-1$ !!? i do not see how should i look at the new components to conclude the $\\Delta y$ and $\\Delta x$\n",
    "    - answer, assume that the vectors are on the origin point of the old coordinates, and try to find out the values of the new vectors that represent the new coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.10) Practice Finding Centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.11) Pactice Finding New Axes\n",
    "- after finding the chagne for the new x vector, getting the new y vector may be hard at first, but i know two ways to get it :\n",
    "    - recall that to get the slope of perpendicular  - > (اقلب الميل و غير إشارته)\n",
    "    - i can also draw an imaginary triangle of the change in x, and then rotate this triangle by +90 degrees and hence i will get the change in y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.12) Which Data is Ready for PCA\n",
    "- Q\\ for the third data set :\n",
    "    - why he said it is impossible to do regression for this dataset (why he said it is impossible to build a regression that goes vertically?) ?! (note he said \"remember\" so it might be mentioned in the older version of the course, i may see it, but is suggest to search for the answer online to not waste my time searching for small piece of info in tens of vides)\n",
    "    - why the PCA is not as follows : the x' is to the bottom and y' is to the right ?\n",
    "    \n",
    "- note : he said that the circualr dataset will always give a deterministic result of PCA, i need to know why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.13) When Does an Axis Dominate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.14) Measurable vs. Latent Features Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.15) From Four Features to Two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.16) Compression While Preserving Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.17) Composote Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.18) Maximal Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.19) Advantages of Maximal Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.20) Maximal Variance and Information Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.21) Info Loss and Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.22) Neighborhood Composite Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.23) PCA for Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.24) Maximum number of PCs Quiz\n",
    "Q\\ why it is the $min(n_{samples}, n_{features})$ !! i thought it would be always the $n_{features}$ ! i need to understand that and maybe make an example where $n_{samples}<n_{features}$ so that i can see how the $n_{samples}$ will set the max num of PCs   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.25) Review/Definition of PCA\n",
    "- This is an important video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.26) Applying PCA to Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.27) PCA on the Enron Finance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.28) PCA in sklearn\n",
    "Q\\ i do not understand the writing : \"The projection step of PCA can be easiest to understand when you subtract out the mean shift of the new principal components, so the new and old dimensions have the same mean:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.29) When to Use PCA\n",
    "- Q\\ her talking about that PC made me wonder, how could it be to have a 2nd PC stronger than the 1st PC, i need to understand that b/c the demonstration so far implied that the 1st PC is at the dirction of maximum variance, and the 2nd PC is orthogonal to the 1st PC.\n",
    "    - note: in the PCA mini-Project, it is written : \"We mentioned that PCA will order the principal components, with the first PC giving the direction of maximal variance, second PC has second-largest variance, and so on. How much of the variance is explained by the first principal component? The second?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.30) PCA for Facial Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.31) Eigenfaces Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 7 : PCA Mini-Project\n",
    "- i need to answer the four questions at the end of that jupyter project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8 : Random Projection and ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.1) Random Projection\n",
    "- looking at $0.51$ then $2:11$ i conclude that a vertical \"in the table of the dataset\" column from top to bottom (which represents a single feature, but in several samples) is put in a horizontal row in the Matrix .\n",
    "    - similarly, each row in the table (represinting one point / one sample) is a column in the matrix\n",
    "    - in other words, going from table to matrix is as if we done a transpose to the table (خلى كل صف عمود و كل عمود صف)\n",
    "    \n",
    "- in the video, and also in the scikit-learn documentation, they talk about \"conservative estimation\"!! what is this ? for exapmle, at the sklearn site , the following is written \"It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.\" -> https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.2) Quiz: Random Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.3) Random Projection in sklearn\n",
    "- i've noticed that there are three types of \"fit\" functions so far in sklearn:\n",
    "    - fit\n",
    "    - fit_predict\n",
    "    - fit_transform\n",
    "- i want to summariez the difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.4) Independent Componenet Analysis (ICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.5) FastICA Algorithm\n",
    "- Q\\ what does it mean to center the dataset ? and whitening the dataset ?\n",
    "- i did not understand anything regarding the math :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.6) Quiz: ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.7) ICA in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.8) [Lab] Independent Componenet Analysis\n",
    "- it is written \"Map the values to the appropriate range for int16 audio. That range is between -32768 and +32767. A basic mapping can be done by multiplying by 32767.\"\n",
    "    - Q\\ how come that a basic mapping can be done by multiplying by 32767 ??!!!!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.9) [Solution] Independent Componenet Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.10) ICA Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 9 : Unsupervised Learning Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QUESTION 2 OF 3\n",
    "    - Note to not confuse K-nearest neighbors and K-means clustering. K-nearest neighbors is a classification algorithm, which is a subset of supervised learning. K-means is a clustering algorithm, which is a subset of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- References to check during the study\n",
    "    - post by mohamed hammad, in which he gave us three lecture that can make us sense more the way that RL learn : https://www.facebook.com/100001876777351/posts/2627523623986838\n",
    "    - a paper tweet by DeepMindAi, in which they reviews recent techniques in deep RL that narrow the gap in learning speed between humans and agents, & demonstrate an interplay between fast and slow learning w/ parallels in animal/human cognition\n",
    "        - https://twitter.com/deepmindai/status/1123979484485570566?s=19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Introduction to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is a type of machine learning where the machine or software agent learns how to maximize its performance at a task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (6 concepts)\n",
    "- 1- Introduction\n",
    "- 2- Applications\n",
    "- 3- The Setting\n",
    "- 4- OpenAI Gym\n",
    "- 5- Resources\n",
    "- 6- Reference Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.1) Introduction\n",
    "- In which, the author shows that we (humans) learn from interacting with the environment.\n",
    "    - so as we grow up, we learn about \"cause and effect\" (or how the world responds to our actions)\n",
    "        - once we know how the world works, we use our knowledge to accomplish specific goals. <br><br>\n",
    "        \n",
    "- In this section (of RL), we will take a computational approach called \"Reinforcement Learning\" to mimic the way humans learn.<br><br>\n",
    "\n",
    "- Since the world is complicated, we will simplify the world to study well defined rules and dynamics.\n",
    "    - we will then construct algorithms to teach an individual (in that simple world) to learn from interaction.\n",
    "        - we will study many of these algorithms to understand the strengths and limitations of each. <br><br>\n",
    "        \n",
    "- We will start simple and build up the complexity as we go.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.2) Applications\n",
    "- Self-driving cars, ships, and airplanes\n",
    "- Robotics (e.g walking , flying a quadcopter, etc)\n",
    "- Business, trading and finance\n",
    "- Biology\n",
    "- Telecommunications\n",
    "- Board games and online strategy games\n",
    "- And others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.3) The Setting\n",
    "- Agent: the learner, or decision maker.\n",
    "    - the agent continuously proposes and tests hypotheses (language note, \"hypotheses\" is a plural noun of hypothesis).\n",
    "    - the agent will need to deal with the dilemma of exploration vs exploitation.\n",
    "    - the agent will want to maximize its reward on the long term (not just the current moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.4) OpenAI Gym\n",
    "- It is an open source toolkit for developing and comparing RL algorithms.\n",
    "    - we will use it extensively in this section (of RL) <br>\n",
    "    \n",
    "- One of the really cool things about OpenAI Gym is that you can record your performance. So your agent might start off just behaving randomly but as it learns from interaction, you'll be able to see it choose actions in a much more intelligent way.   - What's also really cool is that if you're happy with how smart you've made your agents or how quickly they learn, you can upload your implementations to share your knowledge with the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.5) Resources\n",
    "-  Optional but encouraged reading: The most popular textbook on RL is available for free online.\n",
    "    - Reinforcement Learning - an introduction - 2nd ed- Richard S. Sutton and Andrew G. Barto <br><br>\n",
    "\n",
    "- **Whenever u face an euqation in this section (of RL), after u understand it, make sure to revist it and describe it in your own words** <br><br>\n",
    "\n",
    "- Check out this [GitHub repository](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction) to see Python implementations of most of the figures in the book.    \n",
    "----\n",
    "Before transitioning to the next lesson, you are encouraged to read Chapter 1 (especially 1.1-1.4) of the textbook to get a nice introduction to the field of reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.6) Reference Guide\n",
    "- You are encouraged to download [this sheet](https://github.com/udacity/rl-cheatsheet/blob/master/cheatsheet.pdf), which contains all of the notation and algorithms that we will use in this course. Please only use this sheet as a supplement to your own notes! :)\n",
    "- Another useful notation guide can be found in the pages immediately preceding (يسبق) Chapter 1 of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.+1) my reading\n",
    "- Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may a↵ect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—**trial-and-error** search and **delayed reward**—are the two most important distinguishing features of reinforcement learning. <br><br>\n",
    "\n",
    "- Reinforcement learning, like many topics whose names end with “ing,” such as machine learning and mountaineering, is simultaneously a (1) problem, (2) a class of solution methods that work well on the problem, and (3) the field that studies this problem and its solution methods. It is convenient to use a single name for all three things, but at the same time essential to keep the three conceptually separate. \n",
    "    - **In particular, the distinction between problems and solution methods is very important in reinforcement learning; failing to make this distinction is the source of many confusions.** <br><br>\n",
    "    \n",
    "- We formalize the problem of reinforcement learning using ideas from **dynamical systems theory**, specifically, as the optimal control of incompletely-known Markov decision processes. The details of this formalization must wait until Chapter 3, but the basic idea is simply to capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal. A learning agent must be able to **sense** the state of its environment to some extent and must be able to **take actions** that affect the state. The agent also must **have a goal or goals** relating to the state of the environment. \n",
    "- **Markov decision processes are intended to include just these three aspects—sensation, action, and goal—**in their simplest possible forms without trivializing any of them. \n",
    "- Any method that is well suited to solving such problems we consider to be a reinforcement learning method. <br><br>\n",
    "\n",
    "- look at [page 24] to see how RL is different from Suprevised and unsupervised learning... <br><br>\n",
    "\n",
    "- One of the challenges that arise in reinforcement learning, and not in other kinds of learning, is the trade-off between exploration and exploitation.\n",
    "    - The agent must try a variety of actions and progressively favor those that appear to be best. On a stochastic task, each action must be tried many times to gain a reliable estimate of its expected reward.\n",
    "- Another key feature of reinforcement learning is that it explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment. This is in contrast to many approaches that consider subproblems without addressing how they might fit into a larger picture. Although these approaches have yielded many useful results, their focus on isolated subproblems is a significant limitation. <br><br>\n",
    "\n",
    "- One of the most exciting aspects of modern reinforcement learning is its substantive and fruitful interactions with other engineering and scientific disciplines. Reinforcement learning is part of a decades-long trend within artificial intelligence and machine learning toward greater integration with statistics, optimization, and other mathematical subjects. For example, the ability of some reinforcement learning methods to learn with parameterized approximators addresses the classical “curse of dimensionality” in operations research and control theory. More distinctively, reinforcement learning has also interacted strongly with psychology and neuroscience, with substantial benefits going both ways. Of all the forms of machine learning, reinforcement learning is the closest to the kind of learning that humans and other animals do, and many of the core algorithms of reinforcement learning were originally inspired by biological learning systems. <br><br>\n",
    "\n",
    "- Finally, reinforcement learning is also part of a larger trend in artificial intelligence back toward simple general principles.\n",
    "    - Modern artificial intelligence now includes much research looking for general principles of learning, search, and decision making. It is not clear how far back the pendulum will swing, but reinforcement learning research is certainly part of the swing back toward simpler and fewer general principles of artificial intelligence. <br><br>\n",
    "\n",
    "**1.3 elements of RL** <br>\n",
    "- Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: (1) a policy, (2) a reward signal, (3) a value function, and, optionally, (4) a model of the environment.\n",
    "    - (1) **policy**\n",
    "        - defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.\n",
    "        -  In some cases the policy may be a simple function or lookup table, whereas in others it may involve extensive computation such as a search process. \n",
    "        - The policy is the core of a reinforcement learning agent in the sense that it alone is sufficient to determine behavior. \n",
    "        - In general, policies may be stochastic, specifying probabilities for each action.\n",
    "    - (2) **reward signal**\n",
    "        - defines the goal of a reinforcement learning problem. \n",
    "        - On each time step, the environment sends to the reinforcement learning agent a single number called the reward. \n",
    "        - The agent’s sole objective is to maximize the total reward it receives over the long run. The reward signal thus defines what are the good and bad events for the agent.\n",
    "        - The reward signal is the primary basis for altering the policy; if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future. \n",
    "        - In general, reward signals may be stochastic functions of the state of the environment and the actions taken.\n",
    "    - (3) **value function**\n",
    "        - Whereas the **reward signal** indicates what is good in an immediate sense, a **value function** specifies what is good in the long run. \n",
    "        - Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.\n",
    "        - Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states. \n",
    "        - For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards. Or the reverse could be true.\n",
    "        - Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward. Nevertheless, it is values with which we are most concerned when making and evaluating decisions. Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run.\n",
    "        - <u> Unfortunately, it is much harder to determine values than it is to determine rewards. Rewards are basically given directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime. In fact, the most important component of almost all reinforcement learning algorithms we consider is a method for efficiently estimating values. </u> The central role of value estimation is arguably the most important thing that has been learned about reinforcement learning over the last six decades.\n",
    "    - (4) **model of the environment**\n",
    "        - This is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave. For example, given a state and action, the model might predict the resultant next state and next reward. \n",
    "        - Models are used for **planning**, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced. \n",
    "        - Methods for solving reinforcement learning problems that use *models and planning* are called **model-based** methods, as opposed to simpler **model-free** methods that are explicitly *trial-and-error* learners—viewed as almost the **opposite** of planning. \n",
    "        - In Chapter 8 we explore reinforcement learning systems that simultaneously learn by trial and error, learn a model of the environment, and use the model for planning. Modern reinforcement learning spans the spectrum from low-level, trial-and-error learning to high-level, deliberative planning.\n",
    "        \n",
    "**1.4 Limitaion and Scope** <br>        \n",
    "- We do not address the issues of constructing, changing, or learning the state signal in this book (other than briefly in  ection 17.3). We take this approach not because we consider state representation to be unimportant, but in order to focus fully on the decision-making issues. In other words, our concern in this book is not with designing the state signal, but with deciding what action to take as a function of whatever state signal is available.\n",
    "- Most of the reinforcement learning methods we consider in this book are structured around estimating value functions, but it is not strictly necessary to do this to solve reinforcement learning problems. For example, solution methods such as genetic algorithms, genetic programming, simulated annealing, and other optimization methods never estimate value functions. These methods apply multiple static policies each interacting over an extended period of time with a separate instance of the environment. The policies that obtain the most reward, and random variations of them, are carried over to the next generation of policies, and the process repeats. We call these evolutionary methods because their operation is analogous to the way biological evolution produces organisms with skilled behavior even if they do not learn during their individual lifetimes. If the space of policies is sufficiently small, or can be structured so that good policies are\n",
    "- common or easy to find—or if a lot of time is available for the search—then evolutionary methods can be e↵ective. In addition, evolutionary methods have advantages on problems in which the learning agent cannot sense the complete state of its environment.\n",
    "- Our focus is on reinforcement learning methods that learn while interacting with the environment, which evolutionary methods do not do. Methods able to take advantage of the details of individual behavioral interactions can be much more efficient than evolutionary methods in many cases. Evolutionary methods ignore much of the useful structure of the reinforcement learning problem: they do not use the fact that the policy they are searching for is a function from states to actions; they do not notice which states an individual passes through during its lifetime, or which actions it selects. In some cases this information can be misleading (e.g., when states are misperceived), but more often it should enable more efficient search. Although evolution and learning share many features and naturally work together, we do not consider evolutionary methods by themselves to be especially well suited to reinforcement learning problems and, accordingly, we do not cover them in this book.\n",
    "\n",
    "**1.5 ** An Extended Example: Tic-Tac-Toe<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : The RL Framework: The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to mathematically formulate tasks as Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (19 concepts)\n",
    "- 1- Introduction\n",
    "- 2- The Setting, Revisited\n",
    "- 3- Episodic vs. Continuing Tasks\n",
    "- 4- Quiz: Test Your Intuition\n",
    "- 5- Quiz: Episodic or Continuing?\n",
    "- 6- The Reward Hypothesis\n",
    "- 7- Goals and Rewards, Part 1\n",
    "- 8- Goals and Rewards, Part 2\n",
    "- 9- Quiz: Goals and Rewards\n",
    "- 10- Cumulative Reward\n",
    "- 11- Discounted Return\n",
    "- 12- Quiz: Pole Balancing\n",
    "- 13- MDPs, Part 1\n",
    "- 14- MDPs, Part 2\n",
    "- 15- Quiz: One Step Dynamics, Part 1\n",
    "- 16- Quiz: One Step Dynamics, Part 2\n",
    "- 17- MDPs, Part 3\n",
    "- 18- Finite MDPs\n",
    "- 19- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.1) Introduction\n",
    "- the author : \n",
    "    - In this lesson, we'll end with a rigorous definition for the reinforcement learning problem.\n",
    "        - Specifically, you'll learn how to take a real world problem and formulate it so it can be solved through reinforcement learning.\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.2) The Setting, Revisited\n",
    "- The RL framework is characterized by an agent learned to interact with its environment. (fig.1) \n",
    "    - We assume that time evolves and discrete timesteps.\n",
    "    - At the initial timestep,the agent observes the environment. (fig.2)\n",
    "    - You can think of this observation as a situation that the environment presents to the agent.\n",
    "    - Then, it (the agent) must select an appropriate action in response. (fig.3)\n",
    "    - Then at the next timestep in response to the agents action, the environment presents a new situation to the agent. (fig.4).  At the same time the environment gives the agent a reward which provides some indication of whether the agent has responded appropriately to the environment. (fig.5)\n",
    "    - Then the process continues where at each timestep the environment sends the agent an observation and reward (fig.6). And in response, the agent must choose an action. (fig.7) <br><br>\n",
    "    \n",
    "- In general, we don't need to assume that the environment shows the agent everything he needs to make well-informed decisions. But it greatly simplifies the underlying mathematics if we do. So in this course, we'll make the assumption that the agent is able to fully observe what ever state the environment is in. And instead of referring to the agent as receiving an observation, we will hence say that it receives the environment state. (fig.8 where \"observation\" is changed to \"state\"). <br><br>\n",
    "\n",
    "- But let's make this description a bit clearer with some added notation where we again start from the very beginning at timestep zero.\n",
    "    - The agent first receives the environment state which we denote by S0, where zero stands for a timestep zero of course. (fig.9)\n",
    "    - Then, based on that observation the agent chooses an action, A0 (fig.10). \n",
    "    - At the next timestep, in this case, at timestep one, and as a direct consequence of the agent's choice of action, A0, and the environments previous state, S0, the environment transitions to a new state, S1, and gives some reward, R1, to the agent. (fig.11)\n",
    "    - The agent then chooses an action, A1. (fig.12 where A0 is changed to A1 inside the square, and A1 is added at the far right)\n",
    "    - At timestep two, the process continues where the environment passes the reward in state, then the agent responds with an action (fig.13) and so on. <br><br>\n",
    "    \n",
    "- Whereas the agent interacts with the environment, this interaction is manifest as a sequence of states, actions, and rewards. That said, the reward will always be the most relevant quantity to the agent. To be specific, any agent has the goal to  aximize expected cumulative reward or the some of the rewards attained over all timesteps. In other words, it seeks to find the  trategy for choosing actions with the cumulative reward is likely to be quite high. And the agent can only accomplish this by  nteracting with the environment. This is because at every timestep, the environment decides how much reward the agent receives. In other words, the agent must play by the rules of the environment. But through interaction, the agent can learn those rules  nd choose appropriate actions to accomplish its goal. And this is essentially what we'll try to accomplish in this course. <br><br>\n",
    "\n",
    "- But it's important to emphasize that all of this is just a mathematical model for a real world problem. **So if you have a  roblem in mind that you think can be solved with reinforcement learning, you will have to specify the states, actions, and rewards, and you'll have to decide the rules of the environment**. \n",
    "- In this course. You'll see a lot of examples for how to accomplish this.\n",
    "\n",
    "\n",
    "| column 1 | column 2  |  column3 |\n",
    "| --- | --- | --- |\n",
    "| ![title](img/6_2_2_1.png) | ![title](img/6_2_2_2.png) | ![title](img/6_2_2_3.png) |\n",
    "| ![title](img/6_2_2_4.png) | ![title](img/6_2_2_5.png) | ![title](img/6_2_2_6.png) |\n",
    "| ![title](img/6_2_2_7.png) | ![title](img/6_2_2_8.png) | ![title](img/6_2_2_9.png) |\n",
    "| ![title](img/6_2_2_10.png) | ![title](img/6_2_2_11.png) | ![title](img/6_2_2_12.png) |\n",
    "| ![title](img/6_2_2_13.png) |  |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.3) Episodic vs. Continuing Tasks\n",
    "- **Episodic Tasks** are tasks that have a well defined ending point. \n",
    "    - i.e interaction ends at some time step $T$ as follows $S_0, A_0, R_1, S_1, A_1, ..., R_T, S_T$\n",
    "    - in this case, we refer to a complete sequence of interaction from start to finish as an **episode** (the previous line from $S_0$ to $S_T$).\n",
    "        - When the episode ends, the agent looks at the total amount of reward it received to figure out how well it did. \n",
    "        - It's then able to start from scratch as if it has been completely reborn into the same environment but now with the added knowledge of what happened in its past life. \n",
    "        - In this way, as time passes over its many lives, the agent makes better and better decisions and you'll see this for yourself in your coding implementations. \n",
    "        - Once your agents have spent enough time getting to know the environment, they should be able to pick a strategy where the cumulative reward is quite high. In other words, in the context of a game playing agent, it should be able to achieve a higher score.\n",
    "    - examples:\n",
    "        - an agent that plays a game, then the interaction ends win the agent wins or loses (or reaches a draw)\n",
    "        - a simulation to teach a car to drive, then the interaction ends if the car crashes\n",
    "        \n",
    "- **Continuing Tasks** goes on for ever without an end    \n",
    "    - i.e interaction continues without limit $S_0, A_0, R_1, S_1, A_1, ...$\n",
    "    - example:\n",
    "        - An algorithm that buys and sells stocks in response to the financial market would be best modeled as an agent in the continuing tasks. In this case, the agent lives forever. So it has to learn the best way to choose actions while simultaneously interacting with the environment.\n",
    "        - The algorithms for this case are slightly more complex and will be covered a bit later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.4) Quiz: Test Your Intuition\n",
    "- If we have a chess game, and say That the **reward** is only delivered at the end of the game, and, let’s say, is +1 if you win, and -1 if you lose. <br><br>\n",
    "\n",
    "- This is an episodic task, where an episode finishes when the game ends. The idea is that by playing the game many times, or by interacting with the environment in many episodes, you can learn to play chess better and better. <br><br>\n",
    "\n",
    "- It's important to note that this problem is **exceptionally difficult**, because the feedback is only delivered at the very end of the game. So, if you lose a game (and get a reward of -1 at the end of the episode), it’s unclear when exactly you went wrong: maybe you were so bad at playing that every move was horrible, or maybe instead … you played beautifully for the majority of the game, and then made only a small mistake at the end. <br><br>\n",
    "\n",
    "- When the reward signal is largely uninformative in this way, we say that the task suffers the problem of **sparse rewards**. There’s an entire area of research dedicated to this problem, and you’re encouraged to read more about it, if it interests you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.5) Quiz: Episodic or Continuing?\n",
    "- Remember:\n",
    "    - A **task** is an instance (Q\\ does \"instance\" mean \"an exapmle\" ?) of the reinforcement learning (RL) problem.\n",
    "    - Episodic tasks come to an end whenever the agent reaches a **terminal state**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.6) The Reward Hypothesis\n",
    "- since there are lots of applications of RL, different agents have differencet goals.\n",
    "    - It's truly amazing that all of these very different goals can be addressed with the same theoretical framework.\n",
    "- all agents formulate their goals in terms of maximizing expected cumulative reward. But what could reward mean in the context of something like a robot learning to walk?  \n",
    "    - Maybe we could think of the environment as a type of trainer that watches the robots movements and rewards it for having good walking form. But then the reward that it gives has the potential to be highly subjective and not scientific at all.I  ean, what makes a walk good? And what makes it bad? And how do we address this? or In general, how do we specify reward to describe any of a number of potential goals that our agents could have?\n",
    "    - Well, before we answer this question, let's take one step back. It's important to note that the word \"Reinforcement\" and \"Reinforcement Learning\" is a term originally from behavioral science. It refers to a stimulus that's delivered immediately after behavior to make the behavior more likely to occur in the future. The fact that this name is borrowed is no coincidence. In fact, it's an important defining hypothesis in reinforcement learning that we can always formulate an agents goal along the lines of maximizing **expected** cumulative reward. And we call this hypothesis, the \"Reward Hypothesis\". \n",
    "- the \"Reward Hypothesis\" : All goals can be frames as the maximization of **expected** cumulative reward. <br><br>\n",
    "\n",
    "- If this still seems weird or uncomfortable to you, you are not alone. But allow me to convince you in the next video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.7) Goals and Rewards, Part 1\n",
    "- So, I'd like to talk to you about some research that I find particularly interesting. And I think it's a great example to illustrate the reward hypothesis that was introduced in the previous video. \n",
    "- Google DeepMind recently addressed the problem of teaching a robot to walk. Among other problem domains, they worked with a physical simulation of a humanoid robot and they managed to apply some nice reinforcement learning to get great results. (fig.1) and (fig.2).\n",
    "- As you learned in an earlier video, in order to frame this as a reinforcement learning problem, we'll have to specify the state's actions and rewards. (fig.3)\n",
    "- We'll dedicate two videos to this example (me: i think they are the current video and the next one.) and we'll begin by detailing the actions. <br><br>\n",
    "\n",
    "- **Actions**:\n",
    "    - are the decisions that need to be made in order for the robot to walk.Now, the humanoid has several joints, and the actions are just the forces that the robot applies to its joints in order to move. Because if the robot has an intelligent method for deciding these forces at every point in time, that will be sufficient to get it walking. (fig.4) <br><br>\n",
    "    \n",
    "- **States**:\n",
    "    - The states are the context provided to the agent for choosing intelligent actions.In this context, the state at any point in time contain :\n",
    "        - the current positions and velocities of all of the joints, along with \n",
    "        - some measurements about the surface that the robot was standing on. These measurements captured how flat or inclined the ground was, if there was a large step along the path and so on.T\n",
    "        - The researchers at Google DeepMind also added contact sensor data, so that it could determine if the robot was still walking or if it had fallen over.\n",
    "    - The idea is that based on the information in the state,the agent has to plan its next action. After all, if there's a step along the path, that will require a different type of movement than if the ground were completely flat. <br><br>\n",
    "\n",
    "- **Rewards**:\n",
    "    - We'll design the reward as a feedback mechanism that tells the agent that it has chosen the appropriate movements.The reward will be our way of telling the agent, \"Good job, for not running into that wall or too bad, you missed that step and fell down.\" \n",
    "    - That's just the main idea and we'll go into depth in the next video.\n",
    "\n",
    "| column 1 | column 2  |  column3 |\n",
    "| --- | --- | --- |\n",
    "| ![title](img/6_2_7_1.png) | ![title](img/6_2_7_2.png) | ![title](img/6_2_7_3.png) |\n",
    "| ![title](img/6_2_7_4.png) |  |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.8) Goals and Rewards, Part 2\n",
    "- So far, we've been trying to frame the idea of a humanoid learning to walk in the context of reinforcement learning. We've detailed the states in actions, and we still need to specify the rewards. <br><br>\n",
    "\n",
    "- **Rewards**\n",
    "    - The reward structure from the DeepMind paper is surprisingly intuitive. The followimg line is pulled from the appendix of the DeepMind paper,and describes how the reward is decided at every time step. Each term communicates to the agent some part of what we'd like it to accomplish.\n",
    "    \n",
    "\\begin{equation*}\n",
    "r = min(v_x, v_{max}) - 0.005(v_y^2+v_z^2) - 0.05y^2-0.02||u||^2+0.02\n",
    "\\end{equation*}\n",
    "\n",
    "- _\n",
    "    - So let's look at each term individually. <br><br>\n",
    "    \n",
    "        - At every time step, the agent receives a reward proportional to its forward velocity.\n",
    "            - the corresponding term : $min(v_x, v_{max})$\n",
    "            - (fig.1)\n",
    "            - So if moves faster, it gets more reward, but up to a limit, here denoted Vmax.  \n",
    "            - my notes:\n",
    "                - i think the robot is moving (or the creators want it to move) in the x direction.\n",
    "                - the function $min(v_x, v_{max})$ will make the robot increase its velocity to reach $v_{max}$, otherwise if $v_x$ is smaller than $v_{max}$, the func will return $v_x$ which will be less reward. Also if the robot increased its speed above $v_{max}$, this will have no effect on the reward since the func will always return $v_{max}$ (the minimumm in that case). <br><br>\n",
    "                \n",
    "        - But the robot is penalized by an amount proportional to the force applied to each joint.\n",
    "            - the corresponding term : $-0.02||u||^2$\n",
    "            - (fig.2)\n",
    "            - So if the agent applies more force to the joints, then more reward is taken away as punishment. \n",
    "            - my notes:\n",
    "                - from the paper, $u$ corresponds to <br><br>\n",
    "                \n",
    "         - Since the researchers also wanted the humanoid to focus on moving forward, the agent is also penalized for moving left, right, or vertically.\n",
    "            - the corresponding term : $- 0.005(v_y^2+v_z^2)$\n",
    "            - (fig.3)\n",
    "            - my note:\n",
    "                - left and right movement are both included in the term $v_y^2$, also by vertically she means up or down (this is included in the term $v_z^2$) <br><br>\n",
    "\n",
    "        - It was also penalized if the humanoid moved its body away from the center of the track.\n",
    "            - the corresponding term : $- 0.05y^2$\n",
    "            - (fig.4)\n",
    "            - So the agent will try to keep the humanoid as close to the center as possible. <br><br>\n",
    "\n",
    "        - At every time step, the agent also receives some positive reward if the humanoid has not yet fallen.\n",
    "            - the corresponding term : $+0.02$\n",
    "            - (fig.5) <br><br>\n",
    "\n",
    "    - They frame the problem as an episodic task where if the human falls, then the episode is terminated. At this point, whatever cumulative reward the agent had at that time point is all it's ever going to get. In this way, the reward signal is designed, so if the robot focused entirely on maximizing this reward, it would also coincidentally learn to walk. To see this, note the following <br><br>\n",
    "    \n",
    "        - first note that if the robot falls, the episode terminates. And that's a missed opportunity to collect more of this positive reward. (fig.6), (my note: by falling, this term \" $+0.02$ \" will stop giving reward)\n",
    "            - And in general, if the robot walks for ten time steps, that's only 10 opportunities to get reward. And if it  tays walking for 100, that's a lot more time to collect more reward. So if we get the reward in this way, the agent will try to keep from falling for as long as possible. <br><br>\n",
    "        \n",
    "        - Next, since the reward is proportional to the forward velocity, this will ensure the robot also feels pressured to walk as quickly as possible in the direction of the walking track. (fig.7) <br><br>\n",
    "        \n",
    "        - but it also makes sense to penalize the agent for applying too much force to the joints. (fig.8) This is because otherwise, we could end up with a situation where the humanoid walks to erratically (language note from google : erratically = in a manner that is not even or regular in pattern or movement; unpredictably.). By penalizing large forces, we can try to keep the movements more smooth and elegant.\n",
    "\n",
    "        - Likewise, we want to keep the agent on the track and moving forward. (fig.9) Otherwise, who knows where it could end up walking off to. <br><br>\n",
    "        \n",
    "    - Of course, the robot can't focus just on walking fast (fig.10), or just on moving forward (fig.11), or only on walking smoothly (fig.12), or just on walking for as long as possible (fig.13). These are four somewhat competing requirements that the agent has to balance for all time steps towards its goal of maximizing expected cumulative reward.\n",
    "    - And Google DeepMind demonstrated that from this very simple reward function, the agent is able to learn how to walk in a very human like fashion. In fact, this rewards function is so simple that it may seem that deciding reward is quite straightforward, but in general, this is not the case. Of course, there are some counterexamples to this. For instance, if you're teaching an agent to play a video game, the reward is just the score on the screen. And if you're teaching an agent to play Backgammon, well, the reward is delivered only at the end of the game, and you could construct it to be positive if the agent wins, and negative, if it loses.\n",
    "- **The fact, that the reward is so simple is precisely what makes this research from DeepMind so fascinating.**\n",
    "    \n",
    "| column 1 | column 2  |  column3 |\n",
    "| --- | --- | --- |\n",
    "| ![title](img/6_2_8_1.png) | ![title](img/6_2_8_2.png) | ![title](img/6_2_8_3.png) |\n",
    "| ![title](img/6_2_8_4.png) | ![title](img/6_2_8_5.png) | ![title](img/6_2_8_6.png) |\n",
    "| ![title](img/6_2_8_7.png) | ![title](img/6_2_8_8.png) | ![title](img/6_2_8_9.png) |\n",
    "| ![title](img/6_2_8_10.png) | ![title](img/6_2_8_11.png) | ![title](img/6_2_8_12.png) |\n",
    "| ![title](img/6_2_8_13.png) |  |  |\n",
    "\n",
    "---\n",
    "If you'd like to learn more about the research that was done at [DeepMind](https://deepmind.com/), please check out [this link](https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/). The research paper can be accessed [here](https://arxiv.org/pdf/1707.02286.pdf). Also, check out this cool [video](https://www.youtube.com/watch?v=hx_bgoTF7bs&feature=youtu.be)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.9) Quiz: Goals and Rewards\n",
    "- So far, you've seen one example for how to frame an agent's goal as the maximization of expected cumulative reward. In this quiz, you will investigate several more examples. <br><br>\n",
    "\n",
    "- QUESTION 1 OF 3\n",
    "    - Consider an agent who would like to learn to escape a maze. Which reward signals will encourage the agent to escape the maze as quickly as possible? Select all that apply. \n",
    "        - (the right answers are the following)\n",
    "            - The reward is -1 for every time step that the agent spends inside the maze.  Once the agent escapes, the episode terminates.\n",
    "            - The reward is -1 for every time step that the agent spends inside the maze.  Once the agent escapes, it receives a reward of +10, and the episode terminates. \n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - The reward is +1 for every time step that the agent spends inside the maze.  Once the agent escapes, the episode terminates.\n",
    "                - my explanation : this way, the agent will try to stay at the maze indefinitely\n",
    "            - The reward is 0 for every time step that the agent spends inside the maze.  Once the agent escapes, it receives a reward of +1, and the episode terminates.\n",
    "                - my explanation : Although this reward system will let the agent escape the maze, it will not encourage it to escape as quickly as possible (which is required in the question above)<br><br>\n",
    "        \n",
    "- QUESTION 2 OF 3\n",
    "    - Consider an agent who would like to learn to play a board game (like backgammon, chess, or checkers). Which reward signals will encourage the agent to win the game? Select all that apply.        \n",
    "        - (the right answers are the following) \n",
    "            - The agent receives a reward only at the end of the game, and receives a reward of +1 if it wins, -1 if it loses, and 0 if the game is a draw.\n",
    "            - The agent receives a reward only at the end of the game, and receives a reward of +10 if it wins, -10 if it loses, and 0 if the game is a draw.\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - The agent receives a reward of -1 for every time step that it is still playing the game; once the game ends, the episode terminates.\n",
    "                 - my explanation : the agent will try to finish the game as quickly as possible (i think this will usually lead it to try to lose to finish the game fast)\n",
    "            - The agent receives a reward only at the end of the game, and receives a reward of -1 if it wins, +1 if it loses, and 0 if the game is a draw.\n",
    "                - my explanation : the agent goal will be to lose the game ! <br><br>\n",
    "                \n",
    "- QUESTION 3 OF 3\n",
    "    - Consider an agent who would like to learn to balance a plate of food on her head. Which reward signals will encourage the agent to keep the plate balanced for as long as possible? Select all that apply. \n",
    "        - (the right answers are the following) \n",
    "            - The reward is +1 for every time step that the agent keeps the plate balanced on her head.  If the plate falls, the episode terminates.\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - The reward is -1 for every time step that the agent keeps the plate balanced on her head.  If the plate falls, the episode terminates.\n",
    "                - my explanation : the agent will deliberately let go of the plate when the episode starts\n",
    "            - The agent receives a reward only when the plate falls.  If the plate does not break, the agent receives a reward of +1.\n",
    "                - my explanation : the agent's goal will be to let the plate fall in way such that the plate does not break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.10) Cumulative Reward\n",
    "- Built into the framework is the agent's goal which is to maximize cumulative reward.But what exactly does this mean and how does the agent accomplish this?\n",
    "- Could the agent just maximize the reward and each time step? The short answer to that question is, no. But I think a long answer would be a lot more satisfying. So let's try to understand this with the walking robot example.\n",
    "- Remember that in this case, the goal of the robot was to stay walking forward (first three terms) for as long (last term) and as quickly (first term) as possible while also exerting minimal effort (4th term, that let the robot walk smoothly).\n",
    "    - In this case, if the robot tried to maximize the reward it received at a single time step, that would look like trying to move as quickly as possible with as little effort without falling immediately. That could work well in the short term but it's possible, for instance, that the agents movement gets it moving quickly without falling initially. But that first movement was de-stabilising enough that it doomed the agent to fall in a short time. In this way, if the agent focused on individual time steps, it could learn actions that maximize initial rewards. But then the episode terminates quite quickly. And so the cumulative reward is quite small. And still worse, in this case, the agent will not have learned to walk. In this example then, it's clear that the agent cannot focus on individual time steps and instead, needs to keep all time steps in mind. But this also holds true for reinforcement learning agents in general. **Actions have short and long term consequences and the agent needs to gain some understanding of the complex effects its actions have on the environment.**\n",
    "    - Along these lines in the walking robot example, if the agent always has reward at all time steps in mind, it will learn to choose movement designed for long term stability. So in this way, the robot moves a bit slowly to sacrifice a little bit of reward but it will payoff because it will avoid falling for longer and collect higher cumulative reward. \n",
    "    - But now, what does all of this mean when the agent chooses an action at an arbitrary time step? How exactly does it keep all time steps in mind? Well, if we're looking at some time step, t, it's important to note that the rewards for all previous time steps have already been decided as they're in the past. Only future rewards are inside the agent's control (fig.1). We refer to the sum of rewards from the next time step onward as the **return** and denote it with a **capital G** (fig.2), and at an arbitrary time step, the agent will always choose an action towards the goal of maximizing the *return*. But it's actually more accurate to say that the agent seeks to maximize **expected return**. This is because it's generally the case that the agent can't predict with complete certainty what the future reward is likely to be. So it has to rely on a prediction or an estimate.\n",
    "        - We'll massage this a bit when we talk about **discounted return**. But this is the main idea.\n",
    "        \n",
    "| column 1 | column 2  | \n",
    "| --- | --- | \n",
    "| ![title](img/6_2_10_1.png) | ![title](img/6_2_10_2.png) |        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.11) Discounted Return\n",
    "- We've discussed how an agent might choose actions with the goal of maximizing expected return but we need to dig a bit deeper. For instance, consider our puppy agent, how does he predict how much reward he could get at any point in the future?Puppies can live for decades. Can he really be expected to have just as much of an idea of how much reward he'll get now as he does five years from now? Does it make more sense to consider that it's not entirely clear what the future holds especially if the puppy is still learning, proposing, and testing hypotheses and changing his strategy? It's unlikely that he'll know one thousand times steps in advance what his reward potential is likely to be. In general, the puppy is likely to have a much better idea of what's likely to happen in the near future than he does for a distant time points. Along these lines then, should present reward carry the same weight as future reward? Maybe it makes more sense to value rewards that come sooner more highly, since those rewards are more predictable. This situation motivates the idea of discounting and discounted return. <br><br>\n",
    "\n",
    "- Remember that the goal of the agent is always to maximize cumulative reward. And towards this end, at an arbitrary time step $t$, it can choose the action that maximizes the return. And currently, each time step from $t+1$ onward has an equal say in how the agent should make decisions. What if instead, we wanted time steps that occurred earlier in time to have a much greater say. Well, then instead of maximizing this sum, the idea is that we'll maximize a different sum with rewards that are farther along in time are multiplied by smaller values. We refer to this sum as discounted return. By discounted, we mean that we'll change the goal to care more about immediate rewards rather than rewards that are received further in the future. (fig.1) <br><br>\n",
    "\n",
    "- But how do we choose what values to use here? Well, in practice, we'll define what's called a discount rate, which is always denoted by the Greek letter gamma, and is always a number between zero and one. Then, as for the values, the first term is multiplied by gamma. The second term is multiplied by gamma square. Then, gamma to the third power and so on (fig.2). In this way, we have a nice decay where rewards that occur earlier in time are always multiplied by a larger number. <br><br>\n",
    "\n",
    "\n",
    "- It's **important to note that this gamma is not something that's learned by the agent**. It's something that you set to refine the goal that you have for the agent. So how exactly might you set the value of gamma?  <br>\n",
    "Let's begin by looking at what happens when we set gamma to one. So we plug in one everywhere we see gamma and we see it yields the original, completely un-discounted return from the previous videos. And what about when gamma is set to zero? In this case, every term in this sum disappears with the exception of the most immediate reward (fig.3). \n",
    "- In this way, we see that the larger you make gamma, the more the agent cares about the distant future. And as gamma gets smaller and smaller, we get increasingly extreme discounting, where in the most extreme case, the agent only cares about the most immediate reward.\n",
    "- It's **important to note that discounting is particularly relevant to continuing tasks.** Remember that a continuing task is one where the agent environment interaction goes on without end. In this case, if the agent wants to maximize cumulative reward while it's a pretty difficult task if the feature is limitless. So we use discounting to avoid having to look too far into the limitless future.\n",
    "- But it's **important to note that with or without discounting, the goal is always the same. It's always to maximize cumulative reward.**\n",
    "- The discount rate comes in when the agent chooses actions at an arbitrary time step. It uses the discount rate as part of its program for picking actions. And that program is more interested in securing rewards that come sooner and are more likely than the rewards that come later and are less likely.<br>\n",
    "\n",
    "- You'll learn more about how exactly the agent should select actions in the next lesson. (me: 6-2 RL frame work: the solution) <br>\n",
    "For now, we'll focus on fully specifying the reinforcement learning problem. <br>\n",
    "\n",
    "| column 1 | column 2  |  column3 |\n",
    "| --- | --- | --- |\n",
    "| ![title](img/6_2_11_1.png) | ![title](img/6_2_11_2.png) | ![title](img/6_2_11_3.png) |\n",
    "\n",
    "---\n",
    "Note: In this course, we will use \"return\" and \"discounted return\" interchangably. For an arbitrary time step tt, both refer to $G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$, where $\\gamma \\in [0,1]$. In particular, when we refer to \"return\", it is not necessarily the case that $\\gamma = 1$, and when we refer to \"discounted return\", it is not necessarily true that $\\gamma < 1$. (This also holds for the readings in the recommended textbook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.12) Quiz: Pole Balancing\n",
    "In this classic reinforcement learning task, a cart is positioned on a frictionless track, and a pole is attached to the top of the cart. The objective is to keep the pole from falling over by moving the cart either left or right, and without falling off the track. <br><br>\n",
    "\n",
    "In the OpenAI Gym implementation, the agent applies a force of +1 or -1 to the cart at every time step. It is formulated as an episodic task, where the episode ends when (1) the pole falls more than 20.9 degrees from vertical, (2) the cart moves more than 2.4 units from the center of the track, or (3) when more than 200 time steps have elapsed. The agent receives a reward of +1 for every time step, including the final step of the episode. You can read more about this environment in OpenAI's github. This task also appears in Example 3.4 of the textbook. <br><br>\n",
    "\n",
    "- QUESTION 1 OF 3\n",
    "    - Recall that the agent receives a reward of +1 for every time step, including the final step of the episode. Which discount rates would encourage the agent to keep the pole balanced for as long as possible? (Select all that apply.)\n",
    "        - (the right answers are the following)\n",
    "            - The discount rate is 1\n",
    "            - The discount rate is 0.9\n",
    "            - The discount rate is 0.5\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - (no wrong answers) <br><br>\n",
    "            \n",
    "- QUESTION 2 OF 3\n",
    "    - Say that the reward signal is amended (عُدلت) to only give reward to the agent at the end of an episode. So, the reward is 0 for every time step, with the exception of the final time step. When the episode terminates, the agent receives a reward of -1. Which discount rates would encourage the agent to keep the pole balanced for as long as possible? (Select all that apply.)\n",
    "        - (the right answers are the following)\n",
    "            - The discount rate is 0.9\n",
    "            - The discount rate is 0.5 \n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - The discount rate is 1 \n",
    "                - udacity explanation (then my comment) : Without discounting, the agent will always receive a reward of -1 (no matter what actions it chooses during the episode), and so the reward signal will not provide any useful feedback to the agent. With discounting, the agent will try to keep the pole balanced for as long as possible, as this will result in a return that is relatively less negative. \n",
    "                - my comment on udacity explanation: the it will be less negative because the last reward will be mutlplied by the discount rate. so if the agent fall at the $3^{rd}$ time step, the reward (-1) will be mutliplied by $\\gamma^2$ but if the agent fall ath the $100^{th}$ time step, the reward (-1) will be multiplied by $\\gamma^{101}$ which of course is better than at 3rd time step, so the agent will understand that it has to not fall as possible, so it will learn.\n",
    "                \n",
    "- QUESTION 3 OF 3\n",
    "    - Say that the reward signal is amended to only give reward to the agent at the end of an episode. So, the reward is 0 for every time step, with the exception of the final time step. When the episode terminates, the agent receives a reward of +1. Which discount rates would encourage the agent to keep the pole balanced for as long as possible? (Select all that apply.) \n",
    "        - (the right answers are the following)\n",
    "            - (None of these discount rates would help the agent, and there is a problem with the reward signal.)\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - The discount rate is 1\n",
    "            - The discount rate is 0.9\n",
    "            - The discount rate is 0.5\n",
    "                - udacity explanation (then my comment) : If the discount rate is 1, the agent will always receive a reward of +1 (no matter what actions it chooses during the episode), and so the reward signal will not provide any useful feedback to the agent. If the discount rate is 0.5 or 0.9, the agent will try to terminate the episode as soon as possible (by either dropping the pole quickly or moving off the edge of the track). Thus, you are correct - we must redesign the reward signal!\n",
    "                - my comment on udacity explanation : the reason that a $\\gamma=1$ will always give the same reward is that the rule says that at the end of the episode, the agent takes a reward =1 , so if the robot fall at the 1st, 2nd, 3rd or any other time step, it will always take that reward =1, so the robot will learn nothing. on the otherhand, a $\\gamma=0.9 \\; or \\;  0.5$ will result in a discount for the rewards (so the robot has to fall as soon as possible, so that the last reward (that eqauls 1) is not discounted too much). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.13) MDPs, Part 1\n",
    "Over the next several videos, you'll learn all about how to rigorously define a reinforcement learning problem as a Markov Decision Process (MDP). <br>\n",
    "Towards this goal, we'll begin with an example!\n",
    "\n",
    "---\n",
    "- So far, you've just started a conversation to set the stage for what we'd like to accomplish. We'll use the remainder of this lesson to specify a rigorous definition for the reinforcement learning problem. For context, we'll work with the example of a recycling robot from the Sutton textbook. <br><br>\n",
    "\n",
    "- So consider a robot that's designed for picking up empty soda cans. The robot is equipped with arms to grab the cans and runs on a rechargeable battery. There's a docking station set up in one corner of the room and the robot has to sit at the station if it needs to recharge its battery. Say, you're trying to program this robot to collect empty soda cans without human intervention. In particular, you want the robot to be able to decide for itself when it needs to recharge its battery. And whenever it doesn't need to recharge, you want it to focus on collecting as many soda cans as possible. **So let's see if we can frame this as a reinforcement learning problem.** <br><br>\n",
    "\n",
    "- We'll begin with the actions.\n",
    "- **Actions** :\n",
    "    - We'll say the robot is capable of executing three potential actions. It can search the room for cans, it can head to the docking station to recharge its battery, or it can stay put in the hopes that someone brings it a can. We refer to the set of possible actions as the action space, and it's common to denote it with a script A.\n",
    "    <img src=\"img/6_2_13_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br><br>\n",
    "    \n",
    "- Next, we consider the states.\n",
    "- **States** : \n",
    "    - Remember, the states are just the context provided to the agent for making intelligent actions. So the state, in this case, could be the charge left on the robot's battery. For simplicity, we'll assume that the battery has one of two states. One corresponding to a high amount of charge left, and the other corresponding to a low amount of charge. We refer to the set of possible states as the state space and it's common to denote with a script S. So intuition tells us that if the robot has a high amount of charge left on its battery, we'd like it to know to actively search for the room for cans. Searching the room should use up a lot of energy but this doesn't matter so much because the battery has a lot of charge anyway. But if the state is low, searching for cans has pretty high risk because the battery could get depleted mid-search and then the robot would be stranded and that wouldn't be so good because we don't want to have to come to its rescue. So if the battery is low, maybe we'd like the robot to know to wait for a can or to go to recharge its battery. \n",
    "<img src=\"img/6_2_13_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/><br><br>\n",
    "\n",
    "- In the next few concepts, we'll set up the problem with the ultimate goal of having the robots control equipment learn this behavior.\n",
    "- (me: i am still waiting to see how is the reward formulated ? if it is not mentioned here, i may look at the book)\n",
    "---\n",
    "**Notes :**\n",
    "- In general, the state space $\\mathcal{S}$ is the set of **all nonterminal states**.\n",
    "\n",
    "- In continuing tasks (like the recycling task detailed in the video), this is equivalent to the set of **all states**.\n",
    "\n",
    "- In episodic tasks, we use $\\mathcal{S}^+$ to refer to the set of **all states, including terminal states.**\n",
    "    - the follwing pic is a drawing of me illustrating $\\mathcal{S}$ and $\\mathcal{S}^+$ \n",
    "<img src=\"img/6_2_13_notes.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- The action space $\\mathcal{A}$ is the set of possible actions available to the agent.\n",
    "\n",
    "- In the event that there are some states where only a subset of the actions are available, we can also use \\mathcal{A}(s)A(s) to refer to the set of actions available in state s\\in\\mathcal{S}s∈S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.14) MDPs, Part 2\n",
    "- So we're working with an example of a recycling robot and we've already detailed the states and actions.\n",
    "- In this example, remember that the state corresponds to the charge left on the robot's battery. And there are two potential states, high and low. <br><br>\n",
    "\n",
    "- As a first step, consider the case of the charge on the battery is <u>high</u>. \n",
    "    - Then, the robot could choose to search, wait, or recharge. But actually, recharging doesn't make much sense if the battery is already high, so we'll say that the only options are to search or wait. All right, so if the agent chooses to search, then at the next time step, the state could be high or low. Let's say that with 70 percent probability, it stays high. So there's a 30 percent chance the battery switches to low. In both cases, we'll say that this decision to search led to the robot collecting exactly four cans. And in line with this, the environment gives the agent a reward of four. The other option is to wait. If the robot has a high battery and then decides to wait, well, waiting doesn't use any battery at all and we'll say that then, it's guaranteed that the battery will again be high at the next time step. In this case, we'll suppose that since the robot wasn't out actively searching, it's able to collect fewer cans and say it's delivered just one can. And again in line with this, the environment gives the agent a reward of one. <br><br>\n",
    "    \n",
    "- Onto the case where the battery is <u>low</u>. \n",
    "    - Again, the robot has three options. If the battery is low and it chooses to wait for people to bring cans, that doesn't use any battery until the state at the next time step is going to be low. And just like when the robot decided to wait when the battery was high, the agent gets a reward of one. If the robot recharges, then it goes back to the docking station and the state of that the next time step is guaranteed to be high. Say it collects no cans along the way and gets a reward of zero. And if it searches, well, that's risky. It's possible that it gets away with this and then at the next time step, the battery is still low but not entirely depleted. But it's probably more likely that the robot depletes its battery, has to be rescued and is carried to a docking station to be charged. So the charge on its battery at the next time step is high. So the robot depletes its battery with 80 percent probability and otherwise gets away with that risky action with 20 percent probability. As for the reward, if the robot needs to be rescued, we want to make sure we're punishing the robot in this case, so say we don't look at all at the number of cans it was able to collect and we just give the robot a reward of negative three for that. But if the robot gets away with it, he collects four cans and get the reward of four. \n",
    "    <img src=\"img/6_2_14_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "    \n",
    "- This picture completely characterizes one method that the environment could use to decide the next state in reward at any point in time. To see this, let's look at a concrete example. <br>\n",
    "    - Say for instance, the last state was high and the agent decided to search. Then, the environment would flip a theoretical coin with 70 percent probability of landing heads, and if that coin landed heads, the environment would decide that the next state was high and the agent would get a reward four. Otherwise, if it landed tails, the next state would be low and the reward would be four. As another example, if the last state was low and the agent decided to search, the environment would again flip a theoretical coin now with 80 percent probability of landing heads. If it landed heads, the environment would decide the next state was high and the agent would get a reward of negative three. Otherwise, if it landed tails, the next state would be low and the reward would be four. <br><br>\n",
    "    \n",
    "- But **what's important to emphasize here is how little information the environment uses to make decisions (me: note that the author now is talking about the response/actions of the environment, not the agent ). It doesn't care what situation was presented to the agent 10 or 100 or even two steps prior. And it doesn't look at the actions that the agent took prior to the last one. And how well the agent is doing or how much reward it's collected has no effect on how the environment chooses to respond to the agent. Of course, it's possible to design environments that have much more complex procedures for interacting with the agent, but this is how it's done in reinforcement learning, and you'll see soon for yourself in your implementations just how powerful this framework is.**\n",
    "\n",
    "Q\\ are all frame works in RL like this !!? aren't there environments that consider the previous states of the agent ??\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.15) Quiz: One Step Dynamics, Part 1\n",
    "- Consider the recycling robot example. In the previous concept, we described one method that the environment could use to decide the state and reward, at any time step. <br><br>\n",
    "\n",
    "- QUESTION 1 OF 2\n",
    "    - Say the current state is high, and the agent decides to wait. How does the environment decide the next state and reward?\n",
    "        - (the right answers are the following)\n",
    "            - The next state is high, and the reward is 1.\n",
    "            \n",
    "- QUESTION 2 OF 2        \n",
    "    - Say the current state is low, and the agent decides to recharge. How does the environment decide the next state and reward?\n",
    "        - (the right answers are the following)\n",
    "            - The next state is high, and the reward is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.16) Quiz: One Step Dynamics, Part 2\n",
    "- It will prove convenient to **represent** the **environment's dynamics** <u>using</u> **mathematical notation**. In this concept, we will introduce this notation (which can be used for any reinforcement learning task) and use the recycling robot as an example.\n",
    "- (my note : they will say the same note that the author said in the video on the graph, but now with mathematical notation) <br><br>\n",
    "\n",
    "At an arbitrary time step tt, the agent-environment interaction has evolved as a sequence of states, actions, and rewards $(S_0, A_0, R_1, S_1, A_1, \\ldots, R_{t-1}, S_{t-1}, A_{t-1}, R_t, S_t, A_t)$\n",
    "\n",
    "When the environment responds to the agent at time step $t+1$, it considers only the state and action at the previous time step $(S_t, A_t)$\n",
    "\n",
    "In particular, it does not care what state was presented to the agent more than one step prior. (In other words, the environment does not consider any of $\\{ S_0, \\ldots, S_{t-1} \\}$\n",
    "\n",
    "And, it does not look at the actions that the agent took prior to the last one. (In other words, the environment does not consider any of $\\{ A_0, \\ldots, A_{t-1} \\}$ \n",
    "\n",
    "Furthermore, how well the agent is doing, or how much reward it is collecting, has no effect on how the environment chooses to respond to the agent. (In other words, the environment does not consider any of $\\{ R_0, \\ldots, R_t \\}$ <br>\n",
    "**(my note: and that's exatlyc what the graph shows, u only need to know the current state and the action, to know the the probability of transitioning to a next particular state with a particular reward),,, i am curios though to know, how could we represent the system if the environment take into consideration the history of the actions and states and rewards of the agent ?? is there a way to represnt that in a graph ? or is it only applicable mathematically ? also how is this described mathematically ?**\n",
    "\n",
    "Because of this, we can completely define how the environment decides the state and reward by specifying\n",
    "\n",
    "$p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a)$ (<u>a note about the meaning of this, below</u>)\n",
    "\n",
    "for each possible $s', r, s, \\text{and } a$. These conditional probabilities are said to specify the **one-step dynamics** of the environment. <br>\n",
    "(**my note: in my mind, i can call** $s' \\text{as } s^{next}$ , so that i understand it better)\n",
    "\n",
    "#### An Example\n",
    "\n",
    "<img src=\"img/6_2_14_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Let's return to the case that $S_t = \\text{high}$, and $A_t = \\text{search}$\n",
    "\n",
    "- Then, when the environment responds to the agent at the next time step,\n",
    "\n",
    "    - with 70% probability, the next state is high and the reward is 4. In other words, <br>\n",
    "    $p(\\text{high}, 4|\\text{high},\\text{search}) = \\mathbb{P}(S_{t+1}=\\text{high}, R_{t+1}=4|S_{t} = \\text{high}, A_{t}=\\text{search}) = 0.7$\n",
    "        - (**my note: i can read this LHS of the previous line as follows: if the agent is at the state \"high\" and decided to take the action \"search\", what is the probability of transation to the next state \"high\" with a reward of 4 ? it is 0.7**)\n",
    "\n",
    "    - with 30% probability, the next state is low and the reward is 4. In other words, <br>\n",
    "    $p(\\text{low}, 4|\\text{high},\\text{search}) = \\mathbb{P}(S_{t+1}=\\text{low}, R_{t+1}=4|S_{t} = \\text{high}, A_{t}=\\text{search}) = 0.3$.\n",
    "    \n",
    "<br>    \n",
    "#### Now the quiz :\n",
    "\n",
    "- Question 1\n",
    "    - What is $p(\\text{high}, -3|\\text{low},\\text{search}$)?\n",
    "        - (the right answers are the following)\n",
    "            - 80% <br><br>\n",
    "        \n",
    "- Question 2\n",
    "    - What is $p(\\text{high}, 0|\\text{low},\\text{recharge}$)?\n",
    "        - (the right answers are the following)\n",
    "            - 1 <br><br>\n",
    "            \n",
    "- Questions 3 and 4      \n",
    "    - Consider the following probabilities:\n",
    "        - (1) $p(\\text{low}, 1|\\text{low},\\text{search}$)\n",
    "        - (2) $p(\\text{high}, 0|\\text{low},\\text{recharge}$)\n",
    "        - (3) $p(\\text{high}, 1|\\text{low},\\text{wait}$)\n",
    "        - (4) $p(\\text{high}, 1|\\text{high},\\text{wait}$)\n",
    "        - (5) $p(\\text{high}, 1|\\text{high},\\text{search}$) <br><br>\n",
    "        \n",
    "    - QUESTION 3 OF 4  \n",
    "        - Which of the above probabilities is equal to 0? (Select all that apply.) \n",
    "            - (the right answers are the following)\n",
    "                - 1, 3, and 5. <br><br>\n",
    "\n",
    "    - QUESTION 4 OF 4\n",
    "        - Which of the above probabilities is equal to 1? (Select all that apply.)\n",
    "            - (the right answers are the following)\n",
    "                - 2, and 4. <br><br>\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- About this line : $p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a)$ \n",
    "    - i don't know if it is a notation in the book or it is my lack of study of probabilty (i think it is the latter, so i must take an intensive course on probability), but anyway i did not understand the difference between the two sides of the equation, so i asked this question in the \"Knowledge\" section of udacity. <br>\n",
    "<img src=\"img/6_2_16_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>  \n",
    "\n",
    "- here are the two links\n",
    "    - http://www.incompleteideas.net/book/first/ebook/node40.html\n",
    "    - https://www.utdallas.edu/~jjue/cs6352/markov/node3.html <br><br>\n",
    "    \n",
    "- from the links I figured out the following.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.17) MDPs, Part 3\n",
    "\n",
    "- Now that we've looked at an example, you should have the necessary intuition to understand the formal definition of the reinforcement learning framework. <br><br>\n",
    "\n",
    "- So, formally, a **Markov decision process** or **MDP** is **defined** by <br>\n",
    "<u>the set of states, the set of actions, and the set of rewards along with the one-step dynamics of the environment and the discount rate. </u>  <br>\n",
    "    - (note : i still need to know what is the MDP, and from where it got that name!!)\n",
    "        - read the wiki article : https://en.wikipedia.org/wiki/Markov_decision_process\n",
    "            - but as a summary : A Markov decision process (MDP) is a discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are <u>partly random</u> and <u>partly under the control of a decision maker</u>.\n",
    "    - also look at this link, which tells u the difference between RL and MDP : https://www.quora.com/What-is-the-main-difference-between-reinforcement-Learning-and-Markov-Decision-Process\n",
    "\n",
    "<img src=\"img/6_2_17_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "- We've detail the states, actions, rewards, and one-step dynamics of the environment, but we will also need to talk about the discount rate. And towards this end, **it is important to notice that we've detailed a <u>continuing task</u>**. So, it will prove useful to make the discount factor less than one because otherwise, the agent would have to look infinitely far into the limitless future.  <br><br>\n",
    "\n",
    "- It's common to set the discount rate to **0.9** And that feels like a good choice here. <br><br>\n",
    "\n",
    "- Throughout this course, you'll have the opportunity and your implementations to build some intuition for how to set the discount rate. But **it's important to note now that the discount rate is always set to some number much closer to one than to zero. Otherwise, the agent becomes excessively short-sighted to a fault.** <br><br>\n",
    "\n",
    "- And now, you have fully specified your first MDP. In general, when you have a real world problem in mind, you will need to specify the MDP and that will fully and formally define the problem that you want to your agent to solve. <br><br>\n",
    "\n",
    "- This framework works for continuing and episodic tasks <br><br>\n",
    "\n",
    "- whenever you have a problem that you want to solve with reinforcement learning, whether it entails a self-driving car, a walking robot, or a stock trading agent, this is the framework that you will use. <br><br>\n",
    "\n",
    "- The agent will know the states and actions along with the discount factor. As for the set up rewards and the one-step dynamics, those specify how the environment works and will be unknown to the agent. Despite not having this information, the agent will still have to learn from interaction how to accomplish its goal.\n",
    "\n",
    "<img src=\"img/6_2_17_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.18) Finite MDPs\n",
    "- (my note: this concpet is a reading about the openAi Gym, and CartPole-v0, and Finite MDPs) <br><br>\n",
    "\n",
    "- Every environment comes with first-class `Space` objects that describe the valid actions and observations.\n",
    "    - The `Discrete` space allows a fixed range of non-negative numbers.\n",
    "    - The `Box` space represents an n-dimensional box, so valid actions or observations will be an array of n numbers.\n",
    "    \n",
    "- in the observation (state) space of the CartPole-v0, you will notice that each in the array can be any real number, thus the state space $\\mathcal{S}^+$ is infinite!\n",
    "    - but regarding the **Finite MDPs** :\n",
    "        - Recall from the previous concept that in a finite MDP, the state space $\\mathcal{S}$ (or $\\mathcal{S}^+$, in the case of an episodic task) and action space $\\mathcal{A}$ must both be finite.\n",
    "            - Thus, while the CartPole-v0 environment does specify an MDP, it does not specify a **finite** MDP. In this course, we will first learn how to solve finite MDPs. Then, later in this course, you will learn how to use neural networks to solve much more complex MDPs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.19) Summary\n",
    "<img src=\"img/6_2_19_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "#### The Setting, Revisited\n",
    "- The reinforcement learning (RL) framework is characterized by an **agent** learning to interact with its **environment**.\n",
    "- At each time step, the agent receives the environment's **state** (the environment presents a situation to the agent), and the agent must choose an appropriate **action** in response. One time step later, the agent receives (1) a **reward** (the environment indicates whether the agent has responded appropriately to the state) and (2) a new **state**.\n",
    "- All agents have the goal to maximize expected **cumulative reward**, or the expected sum of rewards attained over all time steps.\n",
    "\n",
    "#### Episodic vs. Continuing Tasks\n",
    "- A **task** is an instance of the reinforcement learning (RL) problem.\n",
    "- **Continuing tasks** are tasks that continue forever, without end.\n",
    "- **Episodic tasks** are tasks with a well-defined starting and ending point.\n",
    "    - In this case, we refer to a complete sequence of interaction, from start to finish, as an **episode**.\n",
    "    - Episodic tasks come to an end whenever the agent reaches a **terminal state**.\n",
    "    \n",
    "#### The Reward Hypothesis\n",
    "- **Reward Hypothesis**: All goals can be framed as the maximization of (expected) cumulative reward.\n",
    "\n",
    "#### Goals and Rewards\n",
    "- (Please see **Part 1** and **Part 2** (my note: i think he means 6-2.13 and 6-2.14) to review an example of how to specify the reward signal in a real-world problem.)\n",
    "\n",
    "#### Cumulative Reward\n",
    "- The **return at time step** $t$ is $G_t := R_{t+1} + R_{t+2} + R_{t+3} + \\ldots$\n",
    "- The agent selects actions with the goal of maximizing expected (discounted) return. (Note: discounting is covered in the next concept.) (my note: i think, by \"next concept\" he means \"the next line\" )\n",
    "\n",
    "#### Discounted Return\n",
    "- The **discounted return at time step** $t$ is $G_t := R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots$.\n",
    "- The discount rate $\\gamma$ is something that you set, to refine the goal that you have the agent.\n",
    "    - It must satisfy $0 \\leq \\gamma \\leq 1$.\n",
    "    - If $\\gamma=0$, the agent only cares about the most immediate reward.\n",
    "    - If $\\gamma=1$, the return is not discounted.\n",
    "    - For larger values of $\\gamma$, the agent cares more about the distant future. Smaller values of $\\gamma$ result in more extreme discounting, where - in the most extreme case - agent only cares about the most immediate reward.\n",
    "    \n",
    "#### MDPs and One-Step Dynamics\n",
    "- The **state space** $\\mathcal{S}$ is the set of all (nonterminal) states.\n",
    "- In episodic tasks, we use $\\mathcal{S}^+$ to refer to the set of all states, including terminal states.\n",
    "- The action space $\\mathcal{A}$ is the set of possible actions. (Alternatively, $\\mathcal{A}(s)$ refers to the set of possible actions available in state $s \\in \\mathcal{S}$.)\n",
    "- (Please see **Part 2** (my note: i think he means 6-2.14) to review how to specify the reward signal in the recycling robot example.)\n",
    "- The **one-step dynamics** of the environment determine how the environment decides the state and reward at every time step. The dynamics can be defined by specifying $p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_{t} = s, A_{t}=a)$.\n",
    "- A (finite) Markov Decision Process (MDP) is defined by:\n",
    "    - a (finite) set of states $\\mathcal{S}$ (or $\\mathcal{S}^+$, in the case of an episodic task)\n",
    "    - a (finite) set of actions $\\mathcal{A}$\n",
    "    - a set of rewards $\\mathcal{R}$\n",
    "    - the one-step dynamics of the environment\n",
    "    - the discount rate $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : The RL Framework: The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, agents learn to prioritize different decisions based on the rewards and punishments associated with different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (13 concepts)\n",
    "- 1- Introduction\n",
    "- 2- Policies\n",
    "- 3- Quiz: Interpret the Policy\n",
    "- 4- Gridworld Example\n",
    "- 5- State-Value Functions\n",
    "- 6- Bellman Equations\n",
    "- 7- Quiz: State-Value Functions\n",
    "- 8- Optimality\n",
    "- 9- Action-Value Functions\n",
    "- 10- Quiz: Action-Value Functions\n",
    "- 11- Optimal Policies\n",
    "- 12- Quiz: Optimal Policies\n",
    "- 13- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.1) Introduction\n",
    "- you've already learned about how to formulate a real-world problem so it can be solved with reinforcement learning in this lesson you'll begin to think about ways to **solve** this problem.\n",
    "\n",
    "- it's important to note that **this lesson is significantly more technical** than the previous one if this is initially uncomfortable to you don't worry and feel free to take the videos at your own pace \n",
    "\n",
    "- we've added extensive examples and quizzes to help you through this relatively difficult content but you'll likely still need to **take your own notes to make sure that the terminology becomes natural to you** \n",
    "\n",
    "- **you should think of reinforcement learning as a language that takes time to learn so don't be too hard on yourself** with all of this in mind let's get started\n",
    "---\n",
    "This lesson covers material in **Chapter 3** (especially 3.5-3.6) of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.2) Policies\n",
    "- We've seen that we use a Markov decision process or MDP as a formal definition of the problem that we'd like to solve with reinforcement learning. <br><br>\n",
    "\n",
    "- **In this video, we specify a formal definition for the <u>solution to this problem</u>.** \n",
    "\n",
    "- We can start to think of the solution as a series of actions that need to be learned by the agent towards the pursuit of a goal. \n",
    "    - For instance, in order to walk, the humanoid robot needs to learn the appropriate way to apply forces to its joint. But as we've seen, the correct action changes the situation. If a robot encounters a wall, the best series of actions will be different than if it had nothing blocking its path. \n",
    "    - Reward is always decided in the context of the state that it was decided in along with the state that follows. With this in mind, as long as the agent learns an appropriate action response to any environment state that it can observe, we have a solution to our problem. \n",
    "    - This motivates the idea of a policy. <br><br>\n",
    "    \n",
    "- The simplest kind of policy is a mapping from the set of environment states to the set of possible actions. \n",
    "<img src=\"img/6_3_2_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "- In case you're new to the idea of a mapping, you can think of a policy as a factory that takes any environment state as input and outputs the corresponding action that the agent will take. \n",
    "    - If the agent wants to keep track of its strategy, all it needs to do is to build this factory or to specify this mapping.  <br><br>\n",
    "\n",
    "- We call this kind of policy a deterministic policy. \n",
    "<img src=\"img/6_3_2_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "- Income is the state, outcome is the action to take. \n",
    "    - And as you can see, it's most common to denote the policy with the Greek letter pi. <br><br><br>\n",
    "    \n",
    "- Another type of policy that we we'll examine is a stochastic policy. The stochastic policy will allow the agent to choose actions randomly. \n",
    "<img src=\"img/6_3_2_3.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "- We define a stochastic policy as a mapping that accepts an environment state S and action A and returns the probability that the agent takes action A while in state S. \n",
    "<img src=\"img/6_3_2_4.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "- For clarity, let's revisit the recycling robot example from the previous lesson. The deterministic policy would specify something like whenever the battery is low, recharge it. And whenever the battery has a high amount of charge, search for cans. The stochastic policy does something more like whenever the battery is low, recharge it with 50 percent probability, wait where you are with 40 percent probability. And otherwise, search for cans. Whenever the battery is high, search for cans with 90 percent probability. And otherwise, wait for a can. \n",
    "<img src=\"img/6_3_2_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "- my note here At $02:36$ :\n",
    "    - I want to make it clear by differentiate between these probabalities (Stocastic Policy) and the probabilities that are on the graph. I will make an example\n",
    "        - the stocastic probabilities \"$\\pi (search|high) = 0.9 $\" and \"$\\pi (wait|high) = 0.1 $\" means that if the state is high, you have 0.9 <u>**probability to do the \"action\":search and 0.1 probability to do the (other) \"action\":wait**</u> (note that they add up to one)\n",
    "        - on the other hand, the probabilities on the graph \"say the .7 and .3 below & below-left-corner respectively\" these are the <u>**probabilities of going to a particular next state given that you took the action \"a\"**</u> (here it is search) . (note that those also add up to 1) \n",
    "        - as a summary, \n",
    "            - the probabilities that are on the RHS of the pictures are $\\pi(a|s^{current})$ , but the probabilities on the graph are $p(s^{next},r|s^{current},a)$\n",
    "            - also, the first probability has a special name : policy (here stochastic policy), but the second probability is named : the one-step-dynamic, which specifies the transations<br><br>\n",
    "\n",
    "- It's important to note that any deterministic policy can be expressed using the same notation that we generally reserve for a stochastic policy. For instance, this policy can be expressed as, whenever the battery is low, recharge it with 100 percent probability. Whenever the battery has a high amount of charge, search for cans with 100 percent probability  <br><br>\n",
    "<img src=\"img/6_3_2_6.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- we will explore policies with varying levels of randomness or stochasticity in this course. Now, at this point you might be wondering. \n",
    "- Now that we know how to specify a policy, what steps can we take to make sure that the agent's policy is the best one. We will work towards answering this question in the next few concepts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.3) Quiz: Interpret the Policy\n",
    "- From the looking at the quiz i noticed something and I also have a question:\n",
    "    - consider the notation for deterministic policy $π:S \\rightarrow A$  and stochastic policy $π:S \\times A \\rightarrow [0,1]$\n",
    "        -  note that the first formula mapps from  $S$ to $A$ , while the second one maps from $S \\times A$ to $[0,1]$ (I stress on this last mapping because I mistakenly thought that it maps from S to A via an operator x !!)\n",
    "- now here comes a question, what does $S \\times A$ mean ?     \n",
    "    - is it a cartesian product ? https://en.wikipedia.org/wiki/Cartesian_product\n",
    "        - even if it is like this, why do we do cartesian product?\n",
    "        - i asked on the ai group and found that :\n",
    "---\n",
    "- A **policy** determines how an agent chooses an action in response to the current state. In other words, it specifies how the agent responds to situations that the environment has presented. <br><br>\n",
    "\n",
    "- Consider the recycling robot MDP from the previous lesson.\n",
    "<img src=\"img/6_2_14_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "#### Deterministic Policy: Example\n",
    "- An example deterministic policy $\\pi: \\mathcal{S}\\to\\mathcal{A}$ can be specified as:\n",
    "    - $\\pi(low)=recharge$\n",
    "    - $\\pi(high)=search$\n",
    "- In this case,     \n",
    "    - if the battery level is low, the agent chooses to recharge the battery.\n",
    "    - if the battery level is high, the agent chooses to search for cans.\n",
    "    \n",
    "#### Question 1    \n",
    "- Consider a different deterministic policy $\\pi: \\mathcal{S}\\to\\mathcal{A}$, where:\n",
    "    - $\\pi(low)=search$\n",
    "    - $\\pi(high)=search$ <br><br>\n",
    "    \n",
    "- QUESTION 1 OF 2    \n",
    "    - Which of the following statements are true, if the agent follows the policy? (Select all that apply.)\n",
    "        - (the right answers are the following)\n",
    "            - If the state is *low*, the agent chooses the action *search*.\n",
    "            - The agent will always *search* for cans at every time step (whether the battery level is *low* or *high*).<br><br>\n",
    "            \n",
    "#### Stochastic Policy: Example            \n",
    "- An example stochastic policy $\\pi: \\mathcal{S}\\times\\mathcal{A}\\to [0,1]$ can be specified as:\n",
    "    - $\\pi(recharge∣low)=0.5$\n",
    "    - $\\pi(wait∣low)=0.4$\n",
    "    - $\\pi(search∣low)=0.1$\n",
    "    - $\\pi(search∣high)=0.9$\n",
    "    - $\\pi(wait∣high)=0.1$\n",
    "- In this case,    \n",
    "    - if the battery level is *low*, the agent *recharges* the battery with 50% probability, *waits* for cans with 40% probability, and *searches* for cans with 10% probability.\n",
    "    - if the battery level is *high*, the agent searches for cans with 90% probability and *waits* for cans with 10% probability.\n",
    "    \n",
    "#### Question 2\n",
    "- Consider a different stochastic policy $\\pi: \\mathcal{S}\\times\\mathcal{A}\\to [0,1]$, where:\n",
    "    - $\\pi(recharge∣low)=0.3$\n",
    "    - $\\pi(wait∣low)=0.5$\n",
    "    - $\\pi(search∣low)=0.2$\n",
    "    - $\\pi(search∣high)=0.6$\n",
    "    - $\\pi(wait∣high)=0.4$ <br><br>\n",
    "    \n",
    "- QUESTION 2 OF 2    \n",
    "    - Which of the following statements are true, if the agent follows the policy? (Select all that apply.)\n",
    "        - (the right answers are the following)\n",
    "            - If the battery level is *high*, the agent chooses to *search* for a can with 60% probability, and otherwise *waits* for a can.\n",
    "            - If the battery level is *low*, the agent is most likely to decide to *wait* for cans<br><br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.4) Gridworld Example\n",
    "- To understand how to go about searching for the best policy, it will help to have a running example. <br><br>\n",
    "\n",
    "- So consider this very very small world and an agent who lives in it. \n",
    "- Say the world is primarily composed of nice patches of grass, but two out of the nine locations in the world have large mountains. \n",
    "- Well think of each of these nine possible locations in the world as states and the environment. \n",
    "- At each point in time, let's say the agent can only move up, down, left or right, and can only take actions that lead it to not fall off the grid. \n",
    "    - Here, the arrows show the possible movements that will allow the agent. \n",
    "- Let's also say, the goal of the agent is to get to the bottom right hand corner of the world as quickly as possible. \n",
    "    - Then we'll think of this as an episodic task where an episode finishes when the agent reaches the goal. So we won't have to worry about transitions away from this goal state. \n",
    "- Furthermore, say that the agent receives a reward of negative one for most transitions. But if an action leads it to encounter a mountain, it receives some reward of negative three. And if it reaches the goal state, it gets a reward of five. So we can think of the reward signal as punishing the agent for every timestep that it spends away from the goal. You can think of the mountains as having a special larger punishment because they take even longer to cross than the patches of grass. \n",
    "    - The reward structure encourages the agent to get to the goal as quickly as possible. And when it reaches the goal, it gets a reward of five, and the episode ends. We'll use this example in our search for the best policy. <br><br>\n",
    "\n",
    "- (my note : can u say again the what are the states, actions, and rewards for this example? :D do it to make sure that u have understanded the example correctly)\n",
    "\n",
    "<img src=\"img/6_3_4_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.5) State-Value Functions\n",
    "\n",
    "- So we're working with this grid world example and looking for the best policy that leads us to a goal state as quickly as possible. <br><br> \n",
    "\n",
    "- So, let's start with a very, very bad policy so that we can understand why it's bad, and then work to improve it. \n",
    "    - Specifically, we'll look at a policy where the agent visits every state in this very roundabout manner, and we can ignore the transition that the agent will never take under the policy. So now, towards understanding why this policy is bad, let's calculate the cumulative reward that will result. If the agent starts in the top left corner of the world and follows this policy to get to the goal state, it just collects all of the reward along the way. So that's negative one plus negative one, plus negative one again, and so on, where if I add up all the rewards along the way, I get negative six. Let's say we're not discounting or that the discount rate is one. \n",
    "    \n",
    "<img src=\"img/6_3_5_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "    \n",
    "- _    \n",
    "    - we'll keep track of this negative six and remember, that it represents the fact that if we start at the state at the top left corner, and then just follow the policy for all time steps, that results in a return of negative six. \n",
    "    \n",
    "<img src=\"img/6_3_5_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "    \n",
    "- _    \n",
    "    - But now, say, instead the agent started one location over to the right. Then, what return would be likely to follow under the same policy? Again, we just sum up all the rewards that the agent receives along the way, and when we do that, we get a return of negative five, and let's also keep track of that. We can continue and do this for every state in the world. It makes sense to think of the goal state as resulting in the return of zero. After all, if the agent starts at the goal the episode ends immediately and no reward is received. \n",
    "    \n",
    "<img src=\"img/6_3_5_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- In this way, no matter where the agent starts in the world, we have a way of keeping track of the return that follows. This way of analyzing this horrible policy will help us to improve it. But before we get into exactly how to do that, let's attach a bit of notation and terminology to this process we just followed. \n",
    "    - You can think of this grid of numbers as a function of the environment state. \n",
    "    - For each state, it has a corresponding number, and we refer to this function as the state-value function. \n",
    "    - For each state, it yields the return that's likely to follow if the agent starts in that state and then follows the policy for all time steps. \n",
    "\n",
    "<img src=\"img/6_3_5_4.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- _\n",
    "   - but it's more common to see it equivalently expressed but with a bit more notation. Before I show you that notation, I warn you that it looks a bit complicated, but it's equivalent to what we've already discussed. \n",
    "   - And here it is. \n",
    "\n",
    "<img src=\"img/6_3_5_5.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- _   \n",
    "   - The state-value function for a policy $\\pi$ is a function of the environment state. \n",
    "   - For each state s, it tells us the expected discounted return, if the agent started in that state s, and then use the policy to choose its actions for all time steps\n",
    "   - the state value function will always correspond to a particular policy. So if we change the policy, we change the state-value function, \n",
    "   - we typically denote the function (state-value function) with the lowercase v with the corresponding policy in the subscript.\n",
    "\n",
    "---\n",
    "Note #1: The notation $\\mathbb{E}_\\pi[\\cdot]$ is borrowed from the suggested textbook. $\\mathbb{E}_\\pi[\\cdot]$ is defined as the expected value of a random variable, given that the agent follows policy $\\pi$. \n",
    "\n",
    "Q\\ since the state-value func is definded as a probability, does that mean that the state-value function can be stochastic ? (not deterministic as the previous example)\n",
    "\n",
    "Note #2: In this course, we will use \"return\" and \"discounted return\" interchangably. For an arbitrary time step tt, both refer to $G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$, where $\\gamma \\in [0,1]$. In particular, when we refer to \"return\", it is not necessarily the case that $\\gamma = 1$, and when we refer to \"discounted return\", it is not necessarily true that $\\gamma < 1$. (This also holds for the readings in the recommended textbook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.6) Bellman Equations\n",
    "- If you take the time yourself to calculate the value function for this policy, you might notice that you don't need to start your calculations from scratch every time. In particular, you don't need to look at the first state then add up all the rewards along the way. Then look at the second state, add up all those rewards. Then the third, add up all those and so on. It turns out to be redundant effort. \n",
    "\n",
    "<img src=\"img/6_3_6_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- Instead there's something much faster that you can do. So, let's erase most of these values with the exception of the ones at the bottom. And let's see how we might work backwards to recalculate those values. And along the way we'll discover that the value function has a nice recursive property. \n",
    "\n",
    "<img src=\"img/6_3_6_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- To see that, let's say we're trying to calculate the value of the state that I've highlighted here. \n",
    "\n",
    "<img src=\"img/6_3_6_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- And, the value of that state is just the sum of rewards that follows until we reach the terminal state. And in this case, the agent starts in the state, then follows the policy, gets a reward of negative one, and lands at this next state with the value of two. And what's important to notice here is that this two already corresponds to the sum of all the rewards that follow all the way to the end. So, instead of recalculating that sum, we could just use that value of two to get the value of the state as negative one plus two or one. \n",
    "\n",
    "<img src=\"img/6_3_6_4.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- For the same reason, the value of the next state is negative one plus one or zero, then negative three plus zero or negative three and so on. \n",
    "- In this way, **we see that we can express the value of any state as the sum of the immediate reward plus the value of the state that follows.** \n",
    "\n",
    "<img src=\"img/6_3_6_5.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- **And what's important to note, is that for simplicity, I set the discount rate in this example to one but in general we want to have a framework that takes discounting into account.** So, we'll need to use the discounted value of the state that follows. \n",
    "\n",
    "<img src=\"img/6_3_6_6.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- We can express this idea in terms of what's known as the Bellman expectation equation where for a general MDP we have to calculate **the expected value of the sum.** This is because, in general, with more complicated worlds, the immediate reward and next state cannot be known with certainty. \n",
    "\n",
    "<img src=\"img/6_3_6_7.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- This equation is very important and we'll use it extensively in future lessons. **But for now, all that's important to remember is the main idea. And that idea is, we can express the value of any state in the MDP in terms of the immediate reward and the discounted value of the state that follows.**\n",
    "\n",
    "<img src=\"img/6_3_6_8.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ Reading  after the video~\n",
    "\n",
    "---\n",
    "before the reading, here are my questions in the reading below : \n",
    "- Q\\ there is a possible error at \"n the event that the agent's policy $\\pi$ is deterministic, the agent selects action $\\pi(s)$\"  i think it should be \" action a(s)\" right?\n",
    "- بس من الكويز اللى بعده أنا شفت حاجة خلتنى أشك إن <br>\n",
    "$\\pi(s)$ is an action ?!!  <br>\n",
    "ولا أنا كدا لخبطت كله فى بعضه؟\n",
    "     - $\\checkmark \\; \\;$  no i am right, recall that the deterministic policy is the mapping $\\pi: \\mathcal{S}\\to\\mathcal{A}$\n",
    "     \n",
    "- Q\\ in \"Bellman Expectation Equation\", the term $R_{t+1}$, does it refere to immediate reward or next reward ?\n",
    "    - $\\checkmark \\; \\;$  i think it is confusing, but $R_{t+1}$ is the immediate reward !!\n",
    "---\n",
    "\n",
    "#### Bellman Equations\n",
    "- In this gridworld example, once the agent selects an action,\n",
    "    - it always moves in the chosen direction (contrasting general MDPs where the agent doesn't always have complete control over what the next state will be), and\n",
    "    - the reward can be predicted with complete certainty (contrasting general MDPs where the reward is a random draw from a probability distribution). <br><br>\n",
    "    \n",
    "- In this simple example, we saw that the value of any state can be calculated as the sum of the immediate reward and the (discounted) value of the next state. <br><br>\n",
    "\n",
    "- Alexis mentioned that for a general MDP, we have to instead work in terms of an expectation, since it's not often the case that the immediate reward and next state can be predicted with certainty. Indeed, we saw in an earlier lesson that the reward and next state are chosen according to the one-step dynamics of the MDP. In this case, where the reward $r$ and next state $s'$ are drawn from a (conditional) probability distribution $p(s',r|s,a)$, the **Bellman Expectation Equation (for** $v_\\pi$**)** expresses the value of any state ss in terms of the expected immediate reward and the expected value of the next state:\n",
    "\n",
    "\\begin{equation*}\n",
    "v_\\pi(s)=E_\\pi[R_{t+1}+\\gamma v_\\pi(S_{t+1})∣S_t=s].\n",
    "\\end{equation*}\n",
    "\n",
    "#### Calculating the Expectation\n",
    "In the event that the agent's policy $\\pi$ is **deterministic**, the agent selects action $\\pi(s)$ when in state $s$, and the Bellman Expectation Equation can be rewritten as the sum over two variables ($s'$ and $r$):\n",
    "\n",
    "\\begin{equation*}\n",
    "v_\\pi(s) =\\sum_{s'\\in\\mathcal{S}^+, \\; r\\in\\mathcal{R}}p(s',r|s,\\pi(s))(r+\\gamma v_\\pi(s'))\n",
    "\\end{equation*}\n",
    "\n",
    "- In this case, we multiply: \n",
    "    - the sum of the reward and discounted value of the next state, (i.e $(r+\\gamma v_\\pi(s'))$) by \n",
    "    - its corresponding probability $p(s',r|s,\\pi(s))$ and \n",
    "- sum over all possibilities to yield the expected value.\n",
    "    - (**my note** : the action of multiplying the probability $p$ by the $(r+\\gamma v_\\pi(s'))$ is what defines the \"expected value\", since expected value means that there is a probability, (do not confuse this with $\\pi$ in this case b/c it is **deterministic** and we will see the equation shortly when it is **stochastic**)\n",
    "    - summing over all possibilities (i.e $\\sum_{s'\\in\\mathcal{S}^+, \\; r\\in\\mathcal{R}}$ ) is because, unlike the example in of the grid world where the value function depends only on the next state and immediate reward, here, the more general is that the value function will depend on all the possible available next states (with thier corresponding immediate reward). so it is also a probability\n",
    "        - this (if my conclusion is right) may asnwer a questio of me earlier , when i said \"can $v_\\pi(s)$ be stochastic\"\n",
    "        - again do not confuse $v_\\pi(s)$ which is in general stochastic (can be deterministic like in the grid world example) , and $\\pi$ which is also in general stochastic but can be deterministic \n",
    "    \n",
    "    \n",
    "\n",
    "If the agent's policy $\\pi$ is **stochastic**, the agent selects action aa with probability $\\pi(a|s)$ when in state $s$, and the Bellman Expectation Equation can be rewritten as the sum over three variables ($s'$, $r$, and $a$):\n",
    "\n",
    "\\begin{equation*}\n",
    "v_\\pi(s) =  \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma v_\\pi(s'))\n",
    "\\end{equation*}\n",
    "\n",
    "- In this case, we multiply the sum of the reward and discounted value of the next state $(r+\\gamma v_\\pi(s'))$ by its corresponding probability $\\pi(a|s)p(s',r|s,a)$ and sum over all possibilities to yield the expected value.\n",
    "    - **my note**: here again u notice that when we convert a variable from deterministic to stochastic, we leave the variable in its place and multiply by its probability (here we introduced the multiplication , $\\pi(a|s)$, that represents a probability)\n",
    "\n",
    "#### There are 3 more Bellman Equations!\n",
    "- In this video, you learned about one Bellman equation, but there are 3 more, for a total of 4 Bellman equations.\n",
    "\n",
    "- **All of the Bellman equations attest to the fact that value functions satisfy recursive relationships.**\n",
    "    - For instance, the **Bellman Expectation Equation (for** $v_\\pi$**)** shows that it is possible to relate the value of a state to the values of all of its possible successor states.\n",
    "\n",
    "After finishing this lesson, you are encouraged to read about the remaining three Bellman equations in sections 3.5 and 3.6 of the textbook. The Bellman equations are incredibly useful to the theory of MDPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.7) Quiz: State-Value Functions\n",
    "In this quiz, you will calculate the value function corresponding to a particular policy.\n",
    "\n",
    "Each of the nine states in the MDP is labeled as one of $\\mathcal{S}^+ = \\{s_1, s_2, \\ldots, s_9 \\}$, where $s_9$ is a terminal state.\n",
    "\n",
    "Consider the (deterministic) policy that is indicated (in orange) in the figure below.\n",
    "\n",
    "<img src=\"img/6_3_7_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "- The policy $\\pi$ is given by:\n",
    "    - $\\pi(s1) = right$\n",
    "    - $\\pi(s2) = right$\n",
    "    - $\\pi(s3) = down$\n",
    "    - $\\pi(s4) = up$\n",
    "    - $\\pi(s5) = right$\n",
    "    - $\\pi(s6) = down$\n",
    "    - $\\pi(s7) = right$\n",
    "    - $\\pi(s8) = right$\n",
    "- Recall that since $s_9$ is a terminal state, the episode ends immediately if the agent begins in this state. So, the agent will not have to choose an action (so, we won't include $s_9$ in the domain of the policy), and $v_\\pi(s_9) = 0$ Take the time now to calculate the state-value function $v_\\pi$ that corresponds to the policy. (You may find that the Bellman expectation equation saves you a lot of work!)\n",
    "\n",
    "Assume $\\gamma = 1$.\n",
    "\n",
    "Once you have finished, use $v_\\pi$ to answer the questions below.  \n",
    "\n",
    "- my solution :\n",
    "    - $v_\\pi(s_9) = 0$\n",
    "    - $v_\\pi(s_8) = 5 + v_\\pi(s_9) = 5 + 0 = 5$\n",
    "    - $v_\\pi(s_7) = -3 + v_\\pi(s_8) = -3 + 5 = 2$\n",
    "    - $v_\\pi(s_6) = 5 + v_\\pi(s_9) = 5 + 0 = 5$\n",
    "    - $v_\\pi(s_5) = -1 + v_\\pi(s_6) = -1 + 5 = 4$\n",
    "    - $v_\\pi(s_5) = -1 + v_\\pi(s_6)  = -1 + 5 = 4$ ... note, i can't calc $v_\\pi(s_4)$ now. it can be calculated after knowing $v_\\pi(s_1)$\n",
    "    - $v_\\pi(s_3) = -1 + v_\\pi(s_6) = -1 + 5 = 4$ \n",
    "    - $v_\\pi(s_2) = -1 + v_\\pi(s_3) = -1 + 4 = 3$\n",
    "    - $v_\\pi(s_1) = -1 + v_\\pi(s_2) = -1 + 3 = 2$\n",
    "    - $v_\\pi(s_4) = -1 + v_\\pi(s_1) = -1 + 2 = 1$\n",
    "\n",
    "#### Question 1\n",
    "- QUESTION 1 OF 3\n",
    "    - What is $v_\\pi(s_4)$?\n",
    "        - (the right answers are the following)\n",
    "            - 1 <br><br>\n",
    "            \n",
    "#### Question 2\n",
    "- QUESTION 2 OF 3\n",
    "    - What is $v_\\pi(s_1)$?\n",
    "        - (the right answers are the following)\n",
    "            - 2 <br><br>       \n",
    "            \n",
    "#### Question 3\n",
    "- QUESTION 3 OF 3 \n",
    "    - Consider the following statements:\n",
    "        - (1) $v_\\pi(s_6) = -1 + v_\\pi(s_5)$ \n",
    "        - (2) $v_\\pi(s_7) = -3 + v_\\pi(s_8)$ \n",
    "        - (3) $v_\\pi(s_1) = -1 + v_\\pi(s_2)$\n",
    "        - (4) $v_\\pi(s_4) = -3 + v_\\pi(s_7)$ \n",
    "        - (5) $v_\\pi(s_8) = -3 + v_\\pi(s_5)$\n",
    "    - Select the statements (listed above) that are true\n",
    "        - (the right answers are the following)\n",
    "            - (2), and (3) <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.8) Optimality\n",
    "- So far in this lesson, we've looked at a particular policy $\\pi$, and calculated its corresponding value function. In the quiz, you calculated the value function corresponding to a different policy which we denoted by Pi-Prime, $\\pi'$. And if you look at each of these value functions, you may notice a pattern or trend. Take the time now to compare them and pause the video if you like.\n",
    "\n",
    "<img src=\"img/6_3_8_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "-  So in reality, there are probably a great number of patterns in these numbers, but the most relevant trend to us now is that when we look at any state in particular and compare the two value functions, the value function for a Pi-Prime is always bigger than or equal to the value function for policy Pi. \n",
    "\n",
    "<img src=\"img/6_3_8_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- For instance, 2 is greater than negative 6, 3 is greater than negative 5, 4 is greater than negative 4, and 1 is equal to 1. So this says, for any state in the environment, it's better to follow policy Pi-prime, right? Because no matter where the agent starts in the grid world, they expected discounted return is larger, and remember that the goal of the agent is to maximize return. So, a greater expected return makes for a better policy. \n",
    "    - This motivates an important definition. \n",
    "    \n",
    "- By definition, we say that a policy Pi-Prime is better than or equal to a policy Pi if it's state-value function is greater than or equal to that of policy Pi for all states. \n",
    "\n",
    "<img src=\"img/6_3_8_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- And so there are a couple of important things to note about this definition. The first is that if you take any two policies, it's not necessarily the case that you're going to be able to decide which is better. In other words, it's possible that they can't be compared. But said, there will always be at least one policy that's better than or equal to all other policies. We call this policy an optimal policy, and it's guaranteed to exist but it may not be unique.\n",
    "\n",
    "<img src=\"img/6_3_8_4.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- And it's important to note that an optimal policy is what the agent is searching for. It's the solution to the MDP and the best strategy to accomplish it's goal. <br><br>\n",
    "\n",
    "\n",
    "- Finally, all optimal policies have the same value function which we denote by Vstar, $v_*$. \n",
    "\n",
    "<img src=\"img/6_3_8_5.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- You might be wondering why it's not written, V sub Pi star $v_{pi *}$. The answer is by convention, and probably because it just looks nicer this way. On that note, it turns out that the policy from the quiz is actually an optimal policy. This is because if you compare it to the value function for any other possible policy, it's value function is always at least as big.\n",
    "\n",
    "<img src=\"img/6_3_8_6.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- but it's not the only optimal policy. For instance, here's another policy that has the same value function.\n",
    "\n",
    "<img src=\"img/6_3_8_7.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- and you might be wondering at this point how I found these optimal policies. Well, this example was simple enough that it was possible to make an educated guess just by staring at the dynamics, but this will often not be the case as many of the MDPs will look at will be far more complicated. So **how might we determine an optimal policy for a much more complicated MDP? To answer this question, we'll need to define another type of value function.** <br>\n",
    "- my note: notice the previous question that motivated us to introduce the Action-Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.9) Action-Value Functions\n",
    "\n",
    "- So far we've been working with the state value function for a policy. For each state s, it yields the expected discounted return If the agent starts in state as and then uses the policy to choose its actions for all time steps. You've seen a few examples and know how to calculate the state value function corresponding to a policy. \n",
    "\n",
    "<img src=\"img/6_3_9_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- In this concept, we'll define a new type of value function known as the action-value function. \n",
    "    - This value function is denoted with a lowercase q instead of v. \n",
    "    - While the state values ($v_\\pi(s)$) are a function of the environment state, the action values ($q_\\pi(s,a)$) are a function of the environment state and the agent's action. \n",
    "    - For each state s and action a, the action value function yields the expected discounted return If the agent starts in state s then chooses action a and then uses a policy to choose its actions for all future time steps. \n",
    "    \n",
    "<img src=\"img/6_3_9_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>    \n",
    "    \n",
    "- Just like with the state value function, it will help your intuition If you calculate this yourself. <br><br>\n",
    "\n",
    "- In the case of the state value function, we kept track of the value of each state with the number on the grid. \n",
    "- We'll do something similar with the action-value function, where we now need up to four values for each state, each corresponding to a different action. \n",
    "\n",
    "<img src=\"img/6_3_9_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>  \n",
    "\n",
    "- These four numbers correspond to the same state but the one on top corresponds to action up; the one on the right, corresponds to moving right before following the policy and so on. You'll see this soon. \n",
    "\n",
    "- So with the exception of the terminal state, I've broken up the figure to leave a space for keeping track of the value corresponding to each possible state and action. \n",
    "\n",
    "<img src=\"img/6_3_9_4.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>  \n",
    "\n",
    "- let's see if we can calculate some action values. We'll begin with the state here. Let's calculate the value corresponding to this state (she highlighted the top-left state) and action down. So the agent starts in this state, takes action down and receives a reward of negative one. \n",
    "- Then for every time step in the future, it just follows the policy until it reaches the terminal state. We can then add up all the rewards that it encountered along the way and when we do that, we get zero. So this zero corresponds to the action value for this state where the agent started and action down. \n",
    "\n",
    "<img src=\"img/6_3_9_5.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br> \n",
    "\n",
    "- Let's try calculating another actual value, this time corresponding to this state (she highlighted the left-middle state) and action up. So the agent starts in the state and takes action up, and it received a reward of negative one. Then it just follows a policy for all future time steps. We add up the reward it collected along the way and this yields a cumulative reward of one. So this one corresponds to the action value for this state and action up. \n",
    "\n",
    "<img src=\"img/6_3_9_6.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br> \n",
    "\n",
    "- So you can continue and do this for every state-action pair that makes sense. And when you do this, you get this action-value function. I highly encourage you to calculate and check these values yourself. \n",
    "\n",
    "<img src=\"img/6_3_9_7.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- Before moving on, it's important to revisit some information that you learned earlier. Remember that we have the notation v_star, $v_*$, to refer to the optimal state value function. Similarly, we'll refer to the optimal action value function as q_star, $q_*$. <br>\n",
    "Q\\ what is the definition of the optimal action value function??!\n",
    "\n",
    "<img src=\"img/6_3_9_7.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "---\n",
    "Note: In this course, we will use \"return\" and \"discounted return\" interchangably. For an arbitrary time step tt, both refer to $G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$, where $\\gamma \\in [0,1]$. In particular, when we refer to \"return\", it is not necessarily the case that $\\gamma = 1$, and when we refer to \"discounted return\", it is not necessarily true that $\\gamma < 1$. (This also holds for the readings in the recommended textbook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.10) Quiz: Action-Value Functions\n",
    "\n",
    "<img src=\"img/6_3_10_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- **True or False?**: For a deterministic policy $\\pi$, <br> \n",
    " $v_\\pi(s) = q_\\pi(s, \\pi(s))$ <br>\n",
    " holds for all $s \\in S$ <br>\n",
    " (Feel free to use the state-value and action-value functions (for an example deterministic policy) above to answer this question.)\n",
    "    - (the right answers are the following)\n",
    "        - True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.11) Optimal Policies\n",
    "- Q on the video\\, \n",
    "    - i don't know what exactly is the \"optimal action value function\" $q_*$ ? from what i see, all states have several action values! so where is this optimality ? \n",
    "        - this may be explained later, so if she don't i have to search online, and ask on the ai group\n",
    "        - as she said at the end of the video, getting the optimal value function is what we will explore in the remainder in this course.<br><br>\n",
    "    \n",
    "- Several concepts ago, I've mentioned that we needed to define the action value function before talking about how the agent could search for an optimal policy, and we will see most of the detail for a later lesson. The main idea is this. \n",
    "    - The agent interacts with the environment. And from that interaction, it estimates the optimal action value function. Then, the agent uses that value function to get the optimal policy. \n",
    "    \n",
    "<img src=\"img/6_3_11_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>    \n",
    "    \n",
    "- So this might all seem quite strange, but it will become much clearer in the next lesson when you implement this process yourself. So for now, please bear with me and let's further ignore the question of how the agent uses its experience to estimate the value function. In particular, let's assume it already knows the optimal action value function, but it doesn't know the corresponding optimal policy. **So how does it get the optimal policy? This is what we'll explore in this video.** <br><br>\n",
    "\n",
    "\n",
    "- So we already have the optimal action value function and you've seen some of the optimal policies already, but I've removed those hints here, so let's try to reconstruct an optimal policy from the value function. \n",
    "\n",
    "<img src=\"img/6_3_11_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- It's possible to show that for each state, we just need to pick the action that yields the highest expected return. So beginning with the state in the top left corner, the policy will go right instead of down since 2 is larger than zero. Moving right, we see two values of 1 and one value of 3. 3 is the largest value into the policy, we'll go right here. And we can continue in this way, always picking the action with the highest value. So 4 is greater than 2, 5 is higher than 1 or 3. Next, 4 is the largest (my note, she highlighted the center square). We'll skip over the state with 3 values of 1 because it's not quite clear what to do here. But then 2 is larger than 0, and 5 is larger than 1. So now back to the state with three values of 1. It turns out that to construct the optimal policy, we have our choice here. The agent could go up down or right and all three choices would yield optimal policies. So let's just say the policy decides to go right. And just like that, we've arrived at an optimal policy \n",
    "\n",
    "<img src=\"img/6_3_11_3.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "- it's worth summarizing what we've noticed here. If the agent has the optimal action value function, it can quickly obtain an optimal policy, **which is the solution to the MDP that we're looking for.** **This brings us to the question of how the agent could find the optimal value function. This is in fact what we'll explore for the remainder of this course.**    \n",
    "\n",
    "<img src=\"img/6_3_11_4.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.12) Quiz: Optimal Policies\n",
    "- I don't understand the following \n",
    "    - \"To see why this should be the case, note that it must hold that $v_*(s) =\\max_{a\\in\\mathcal{A}(s)} $.\" \n",
    "    - In the event that there is some state $s\\in\\mathcal{S}$ for which multiple actions $a\\in\\mathcal{A}(s)$ maximize the optimal action-value function, you can construct an optimal policy by placing any amount of probability on any of the (maximizing) actions. You need only ensure that the actions that do not maximize the action-value function (for a particular state) are given 0% probability under the policy.\n",
    "\n",
    "#### my explanations\n",
    "\n",
    "- i want to differentiate between \n",
    "    - $\\pi_*(s) =\\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a) $ for all $s\\in\\mathcal{S}$.\n",
    "    - $v_*(s) =\\max_{a\\in\\mathcal{A}(s)} q_*(s,a) $\n",
    "    \n",
    "- but first, i want you to know the the difference between $\\max $ and $ \\arg\\max$\n",
    "- then we will see what is the domain and range for a multivariate function (function of more than one variable), and how to calc its $\\max$ and $\\arg\\max$, is it in 3d space, or in 2d by fixing one arg it varying the other, we will see. <br><br>\n",
    "\n",
    "- so, the difference between $\\max $ and $ \\arg\\max$ from stackoverflow :\n",
    "<img src=\"img/6_3_12_max.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- now, in the previous two lines (that we want to compare them),  $\\max$ and $\\arg\\max$ are applied to the optimal state-value function, $q_*(s,a)$ , so for that function :\n",
    "    - domain (horizontal plane) : s, and a. (think of them as a horizontal plane that has two orthogonal axes : s and a)\n",
    "    - range (vertical axis) : $q_*(s,a)$ <br><br>\n",
    "    \n",
    "- now, when we say  $\\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a) $ for all $s\\in\\mathcal{S}$\n",
    "    - the parameter under the word \"$max$\" is \"$a$\" (ignore \"$\\in\\mathcal{A}$\" for now, it is just an aditional info). this means that we are going to get the argument \"$a$\" (i.e its value on the $a$ axis), when q(s,a) is maximum, and we will do that for each (for all) s (leave in S as it is an addiotional informarion). this corresponds to $a_1$, $a_2$, or $a_3$ in the table below<br><br>\n",
    "    \n",
    "- on the other hand, when we say $v_*(s) =\\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$\n",
    "    - the same as the previous demonstration, but now, instead of getting a1, a2, or a3, we get the valaue of $q_*(s,a)$ (the number inside the table below)\n",
    "    \n",
    "    \n",
    "---    \n",
    "#### now the quiz : optimal Policies\n",
    "\n",
    "If the state space $\\mathcal{S}$ and action space $\\mathcal{A}$ are finite, we can represent the optimal action-value function $q_*$ in a table, where we have one entry for each possible environment state $s \\in \\mathcal{S}$ and action $a\\in\\mathcal{A}$.\n",
    "\n",
    "The value for a particular state-action pair $s,a$ is the expected return if the agent starts in state $s$, takes action $a$, and then henceforth follows the optimal policy $\\pi_*$.\n",
    "\n",
    "We have populated some values for a hypothetical Markov decision process (MDP) (where $\\mathcal{S}=\\{ s_1, s_2, s_3 \\}$ and $\\mathcal{A}=\\{a_1, a_2, a_3\\}$) below.\n",
    "\n",
    "<img src=\"img/6_3_12_1.png\" alt=\"Drawing\" style=\"width: 200px;\"/> <br>\n",
    "\n",
    "You learned in the previous concept that once the agent has determined the optimal action-value function $q_*$, it can quickly obtain an optimal policy $\\pi_*$ by setting $\\pi_*(s) = \\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$.\n",
    "\n",
    "To see why this should be the case, note that it must hold that $v_*(s) = \\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$.\n",
    "\n",
    "In the event that there is some state $s\\in\\mathcal{S}$ for which multiple actions $a\\in\\mathcal{A}(s)$ maximize the optimal action-value function, you can construct an optimal policy by placing any amount of probability on any of the (maximizing) actions. You need only ensure that the actions that do not maximize the action-value function (for a particular state) are given 0% probability under the policy.\n",
    "\n",
    "Towards constructing the optimal policy, we can begin by selecting the entries that maximize the action-value function, for each row (or state).\n",
    "\n",
    "<img src=\"img/6_3_12_2.png\" alt=\"Drawing\" style=\"width: 200px;\"/> <br>\n",
    "\n",
    "\n",
    "- Thus, the optimal policy $\\pi_*$ for the corresponding MDP must satisfy:\n",
    "    - $\\pi_*(s_1) = a_2$ .....  (or, equivalently, $\\pi_*(a_2| s_1) = 1$), and\n",
    "    - $\\pi_*(s_2) = a_3$ .....  (or, equivalently, $\\pi_*(a_3| s_2) = 1$).\n",
    "    \n",
    "This is because $a_2 = \\arg\\max_{a\\in\\mathcal{A}(s_1)}q_*(s_1,a)$, and $a_3 = \\arg\\max_{a\\in\\mathcal{A}(s_2)}q_*(s_2,a)$.\n",
    "\n",
    "In other words, under the optimal policy, the agent must choose action $a_2$ when in state $s_1$, and it will choose action $a_3$ when in state $s_2$.\n",
    "\n",
    "As for state $s_3$, note that $a_1$, $a_2$ $\\in \\arg\\max_{a\\in\\mathcal{A}(s_3)}q_*(s_3,a)$. Thus, the agent can choose either action $a_1$ or $a_2$ under the optimal policy, but it can never choose action $a_3$. \n",
    "- That is, the optimal policy $\\pi_*$ must satisfy:\n",
    "    - $\\pi_*(a_1| s_3) = p$,\n",
    "    - $\\pi_*(a_2| s_3) = q$, and\n",
    "    - $\\pi_*(a_3| s_3) = 0$,\n",
    "    \n",
    "where $p,q\\geq 0$, and $p + q = 1$.\n",
    "\n",
    "#### Question\n",
    "Consider a different MDP, with a different corresponding optimal action-value function. Please use this action-value function to answer the following question.\n",
    "\n",
    "<img src=\"img/6_3_12_3.png\" alt=\"Drawing\" style=\"width: 200px;\"/> <br>\n",
    "\n",
    "- **QUIZ QUESTION**\n",
    "    - Which of the following describes a potential optimal policy that corresponds to the optimal action-value function?\n",
    "        - (the right answers are the following)\n",
    "            - The agent always selects action $a_3$ in state $s_1$.\n",
    "            - The agent is free to select either action $a_1$ or action $a_2$ in state $s_2$.\n",
    "            - The agent must select action $a_1$ in state $s_3$.\n",
    "        - (wrong answers)\n",
    "            - The agent selects action $a_1$ in state $s_1$\n",
    "            - The agent must select action $a_3$ in state $s_2$.\n",
    "            - The agent is free to select either action $a_2$ or $a_3$ in state $s_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.13) Summary\n",
    "- my note : note that the \"Bellman expectation equation\" is a recursive equation. (i stress on that b/c for a moment i thought that it only calculates the state-value for two terms only)\n",
    "    - actually it does, if u start from the target (terminal state) and stepped back to the starting states\n",
    "---\n",
    "#### Summary\n",
    "\n",
    "<img src=\"img/6_3_13_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "#### Policies\n",
    "- A **deterministic policy** is a mapping $\\pi: \\mathcal{S}\\to\\mathcal{A}$. For each state $s\\in\\mathcal{S}$, it yields the action $a\\in\\mathcal{A}$ that the agent will choose while in state $s$.\n",
    "- A **stochastic policy** is a mapping $\\pi: \\mathcal{S}\\times\\mathcal{A}\\to [0,1]$. For each state $s\\in\\mathcal{S}$ and action $a\\in\\mathcal{A}$, it yields the probability $\\pi(a|s)$ that the agent chooses action $a$ while in state $s$.\n",
    "\n",
    "#### State-Value Functions\n",
    "- The **state-value function** for a policy $\\pi$ is denoted $v_\\pi$. For each state $s \\in\\mathcal{S}$, it yields the expected return if the agent starts in state $s$ and then uses the policy to choose its actions for all time steps. That is, $v_\\pi(s) \\doteq \\text{} \\mathbb{E}_\\pi[G_t|S_t=s]$. We refer to $v_\\pi(s)$ as the **value of state** $s$ **under policy** $\\pi$.\n",
    "- The notation $\\mathbb{E}_\\pi[\\cdot]$ is borrowed from the suggested textbook, where $\\mathbb{E}_\\pi[\\cdot]$ is defined as the expected value of a random variable, given that the agent follows policy $\\pi$.\n",
    "\n",
    "#### Bellman Equations\n",
    "- The **Bellman expectation equation for** $v_\\pi$ is: $v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$.\n",
    "    - **my note**: i want u to know the difference between the state-value function and the first bellman equation :\n",
    "        - the deifinition of the state-value function is illustrated above by few lines\n",
    "        - first bellman equation is a mathematical recursive way to calculate the state-value function. this equation defines the relationships between a given state (or state-action pair) to its successors (التالى) .\n",
    "\n",
    "#### Optimality\n",
    "- A policy $\\pi'$  is defined to be better than or equal to a policy $\\pi$ if and only if $v_{\\pi'}(s) \\geq v_\\pi(s)$ for all s\\in\\mathcal{S}.\n",
    "- An **optimal policy** $\\pi_*$ satisfies $\\pi_* \\geq \\pi$ for all policies $\\pi$. An optimal policy is guaranteed to exist but may not be unique.\n",
    "    - **my Q\\** where is the proof of that last claim ?\n",
    "- All optimal policies have the same state-value function $v_*$, called the **optimal state-value function**.\n",
    "\n",
    "#### Action-Value Functions\n",
    "- The **action-value** function for a policy $\\pi$ is denoted $q_\\pi$. For each state $s \\in\\mathcal{S}$ and action $a \\in\\mathcal{A}$, it yields the expected return if the agent starts in state $s$, takes action $a$, and then follows the policy for all future time steps. That is, $q_\\pi(s,a) \\doteq \\mathbb{E}_\\pi[G_t|S_t=s, A_t=a]$. We refer to $q_\\pi(s,a)$ as the **value of taking action** $a$ **in state** $s$ **under a policy** $\\pi$ (or alternatively as the **value of the state-action pair** $s, a$).\n",
    "- All optimal policies have the same action-value function $q_*$, called the **optimal action-value function**.\n",
    "- **my Q\\** i still do not know how the definition of the action-value fuction will help in getting the optimal policy. of course i saw the way they done it to get the optimal policy, but i mean, what is the point of (the definition of the action value function) taking an action then follow the policy ? why not, for example, the agent takes two actions and then follow the policy, would that help in any thing? this is the concept that i am missing and i hope to get an answer to it.(i may search for it or i may post it on the ai group, but i need first to make sure that this is not answered (or not made clear) later in this course.) \n",
    "\n",
    "#### Optimal Policies\n",
    "- Once the agent determines the optimal action-value function $q_*$, it can quickly obtain an optimal policy $\\pi_*$ by setting $\\pi_*(s) = \\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$.\n",
    "    - remember, the return of the last function \"$\\arg\\max_{a\\in\\mathcal{A}(s)}$\" in the (above) function is : optimal actions that the agent can take at each state (this is the optimal policy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamic programming setting is a useful first step towards tackling the reinforcement learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (27 concepts)\n",
    "- 1- Introduction\n",
    "- 2- OpenAI Gym: FrozenLakeEnv\n",
    "- 3- Your Workspace\n",
    "- 4- Another Gridworld Example\n",
    "- 5- An Iterative method, Part 1\n",
    "- 6- An Iterative method, Part 2\n",
    "- 7- Quiz: An Iterative method\n",
    "- 8- Iterative Policy Evaluation\n",
    "- 9- Implementation\n",
    "- 10- Mini Project: DP (Parts 0 and 1)\n",
    "- 11- Action Values\n",
    "- 12- Implementation\n",
    "- 13- Mini Project: DP (Parts 2)\n",
    "- 14- Policy Improvement\n",
    "- 15- Implementation \n",
    "- 16- Mini Project: DP (Parts 3)\n",
    "- 17- Policy Iteration\n",
    "- 18- Implementation\n",
    "- 19- Mini Project: DP (Parts 4)\n",
    "- 20- Truncated Policy Iteration\n",
    "- 21- Implementation\n",
    "- 22- Mini Project: DP (Parts 5)\n",
    "- 23- Value Iteration\n",
    "- 24- Implementation\n",
    "- 25- Mini Project: DP (Parts 6)\n",
    "- 26- Check Your Understadning\n",
    "- 27- Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.1) Introduction\n",
    "\n",
    "- For this lesson, we'll confine our attention to a problem that's slightly easier than the reinforcement learning problem. Instead of working in a setting where the agent has to learn from interaction, we'll assume that the agent already knows everything about the environment. So the agent knows how the environment decides the next state, and it knows how the environment decides reward. The goal will remain the same. Given this information, the agent would like to find the optimal policy. \n",
    "- Solving the simpler problem first will prove incredibly useful for building intuition before we tackle the full reinforcement learning problem. With this in mind, I'll catch you in the next video.\n",
    "\n",
    "---\n",
    "In the **dynamic programming** setting, the agent has full knowledge of the Markov decision process (MDP) that characterizes the environment. (This is much easier than the **reinforcement learning** setting, where the agent initially knows nothing about how the environment decides state and reward and must learn entirely from interaction how to select actions.)\n",
    "\n",
    "This lesson covers material in **Chapter 4** (especially 4.1-4.4) of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### an added cell by me to make an improtant illustration.\n",
    "~~~ start quote from wiki\n",
    "Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.\n",
    "\n",
    "If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems.[1] In the optimization literature this relationship is called the Bellman equation.\n",
    "~~~ end quote from wiki\n",
    "\n",
    "#### do not confuse Richard Bellman with richard feynman (the latter is a known physicist)\n",
    "\n",
    "| Richard Bellman | richard feynman  | \n",
    "| --- | --- | \n",
    "| <img src=\"img/richard_bellman.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> | <img src=\"img/richard_feynman.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>) |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.2) OpenAI Gym: FrozenLakeEnv\n",
    "In this lesson, you will write your own Python implementations of all of the algorithms that we discuss. While your algorithms will be designed to work with any OpenAI Gym environment, you will test your code with the FrozenLake environment.\n",
    "\n",
    "In the FrozenLake environment, the agent navigates a 4x4 gridworld. You can read more about the environment in its corresponding [GitHub file](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py), by reading the commented block in the `FrozenLakeEnv` class. For clarity, we have also pasted the description of the environment below:\n",
    "\n",
    "~~~ **start .py file**\n",
    "```python\n",
    "  \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "    \"\"\"\"\n",
    "```\n",
    "~~~ **end .py file**\n",
    "\n",
    "#### The Dynamic Programming Setting\n",
    "Environments in OpenAI Gym are designed with the reinforcement learning setting in mind. For this reason, OpenAI Gym does not allow easy access to the underlying one-step dynamics of the Markov decision process (MDP).\n",
    "\n",
    "Towards using the FrozenLake environment for the dynamic programming setting, we had to first download the [file](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) containing the `FrozenLakeEnv` class. Then, we added a single line of code to share the one-step dynamics of the MDP with the agent.\n",
    "\n",
    "~~~ **start .py file**\n",
    "```python\n",
    "# obtain one-step dynamics for dynamic programming setting\n",
    "self.P = P\n",
    "```\n",
    "~~~ **end .py file**\n",
    "\n",
    "Q\\ what does both sides of the previous assignment mean ? i.e what is self.P and what is the RHS P ?\n",
    "\n",
    "The new `FrozenLakeEnv` class was then saved in a Python file **frozenlake.py**, which we will use (instead of the original OpenAI Gym file) to create an instance of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.3) Your Workspace\n",
    "- You will write all of your implementations within the classroom, using an interface identical to the one shown below. Your Workspace contains five files:\n",
    "    - **frozenlake.py** - contains the `FrozenLakeEnv` class\n",
    "    - **Dynamic_Programming.ipynb** - the mini project notebook where you will write all of your implementations (*this is the **only** file that you will modify!*)\n",
    "    - Dynamic_Programming_Solution.ipynb - the instructor solutions corresponding to the mini project notebook\n",
    "    - **check_test.py** - contains unit tests that you will use to verify that your implementations are correct\n",
    "    - **plot_utils.py** - contains a plotting function for visualizing state-value functions\n",
    "    \n",
    "Note that **Dynamic_Programming.ipynb** is broken into parts, which are designed to be completed at different parts of the lesson. For instance, you will complete Parts 0 and 1 in the concept titled Mini Project: DP (Parts 0 and 1). Then, you should wait to complete Part 2 until you reach the Mini Project: DP (Part 2) concept. DO NOT COMPLETE THE ENTIRE NOTEBOOK ALL AT ONCE! :)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.4) Another Gridworld Example\n",
    "\n",
    "- Let's begin with a very small world and an agent who lives in it.\n",
    "\n",
    "<img src=\"img/6_4_4_1.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- The world is primarily composed of nice patches of grass, but one of the four locations in the world has a large mountain. \n",
    "- We can think of each of these four possible locations in the world as states in the environment. \n",
    "- At each point in time, let's say the agent can only move up, down, left or right and can only take actions that lead it to not fall off the grid. \n",
    "- Here, the arrows show the possible movements that we allow the agent. \n",
    "- let's also say the goal of the agent is to get to the bottom right hand corner of the world as quickly as possible. \n",
    "    - We'll think of this as an episodic task where an episode finishes when the agent reaches the goal, so we won't have to worry about transitions away from the school state. \n",
    "    - Furthermore, say that the agent receives a reward of a negative one for most transitions, but if an action leads it to encounter a mountain, it receives a reward of negative three and if it reaches the goal state, it gets a reward of five. <br><br>\n",
    "        \n",
    "- In the dynamic programming setting, the agent knows this rewards structure and it knows how transitions happen between states. So the agent already knows everything about how the environment operates. \n",
    "- So now what? How can the agent use this information to find the optimal policy?\n",
    "\n",
    "---\n",
    "In this simple gridworld example, you may find it easy to determine the optimal policy by visual inspection. Of course, solving Markov decision processes (MDPs) corresponding to real world problems will prove far more challenging! :)\n",
    "\n",
    "To avoid over-complicating the theory, we'll use this simple example to illustrate the same algorithms that are used to solve much more complicated MDPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.5) An Iterative method, Part 1\n",
    "- my questions on this video:\n",
    "    - Q\\ how this system of equation is solved while the value of $v_{\\pi}(s_1)$ is always recursive ? \n",
    "        - I think you can do it easily by substituting, for example : substitute the first equation into the second equation. (another way of substitution is illustrated in next concept \"Part 2\")\n",
    "    - Q\\ what is the proof that the `iterative policy evaluation` will always converge to the true value function? \n",
    "---\n",
    "- Let's build off the grid world example and investigate how we might determine the value function corresponding to a particular policy. To begin, we'll enumerate the states. So, state S1 is the state in the top left corner, then S2, S3, and S4. \n",
    "\n",
    "<img src=\"img/6_4_5_1.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- Say, we're trying to <u>evaluate</u> the <u>Stochastic</u> Policy where the agent selects actions uniformly from the set of possible actions. For instance, in state S1, the agent either moves right or down and it essentially decides by just flipping a coin. That is, it moves right with 50 percent probability and otherwise, moves down. In state S2, it moves left half of the time and moves down the other half of the time. The same goes for state S3, where the agent moves up with 50 percent probability and otherwise moves right. \n",
    "\n",
    "<img src=\"img/6_4_5_2.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- So, how about we determine the state value function corresponding to this policy? Let's begin with recording what we know about state <u>S1</u>. \n",
    "    - So, half the time, the agent moves right. In the event that the agent does move right, the expected return it collects can be calculated as negative one plus the value of the state S2 that it lands on. And the other half of the time, the agent moves down and the resultant expected return is just negative three plus the value of the next state S3. So then, the value of state S1 can be calculated as the average of these two values. Since the agent chooses each of the potential actions with equal probability. \n",
    "    \n",
    "<img src=\"img/6_4_5_3.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- you might recognize this equation as just the Bellman equation evaluated at the state S1. \n",
    "    - Here we see that it let's us express the value of state S1 and the context of the values of all of its <u>possible successor states</u>. <br><br>\n",
    "    \n",
    "- Continuing with state <u>S2</u>, \n",
    "    - if the agent moves left, the expected return is negative one plus the value of the state S1 and if the agent moves down, the expected return is five plus the value of the terminal state S4. And to get the value of state S2, we need to just take the average of these two values. And here we've arrived at the Bellman equation again but now, for state S2. \n",
    "    \n",
    "<img src=\"img/6_4_5_4.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>    \n",
    "    \n",
    "- We can continue the trend for state <u>S3</u> \n",
    "    - where we take into account that the agent can move up or right and we take the average of the two values. \n",
    "    \n",
    "<img src=\"img/6_4_5_5.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>    \n",
    "    \n",
    "- Finally, the value of the state S4 will always be zero since it's a terminal state. After all, in the case that the agent starts at the state, the episode ends immediately and no reward is received. \n",
    "\n",
    "<img src=\"img/6_4_5_6.PNG\" alt=\"Drawing\" style=\"width: 150px;\"/> <br> \n",
    "\n",
    "- In this way, we see that we have one equation for each state and we can directly solve this system of equations to get the value of each state. And when you solve the system, you get these values where the values of state S1 and of the terminal state are both zero and each of the other two states has a value of two. \n",
    "\n",
    "<img src=\"img/6_4_5_7.PNG\" alt=\"Drawing\" style=\"width: 150px;\"/> <br> \n",
    "\n",
    "- For each state, we now have the expected return that's likely to follow if the agent starts in that state and then follows the equal probable random policy. \n",
    "- (**my note**: notice now why she will justify the need of the iterative method (i think this is the same concept as using gradient descent vs normal equation) :\n",
    "    - The only problem here is that typically the state space is much larger so directly solving the system of equations is more difficult. In this case, using an iterative method for solving the system generally works better. \n",
    "    \n",
    "    \n",
    "- So instead of solving the system directly, what we can do is start off with a guess for the value of each state. It doesn't have to be any good and what's typically done is to set the value of each state to zero. \n",
    "\n",
    "<img src=\"img/6_4_5_8.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- Then we start by focusing our attention to one state. Say state <u>S1</u>. \n",
    "    - And we'd like to improve our guess for the value of the state. To do this, we'll use the same Bellman equation from before but we'll adapt it so it works as an update rule. \n",
    "    \n",
    "<img src=\"img/6_4_5_9.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>    \n",
    "    \n",
    "- Here, the capital V denotes our current guess for the value function. We'll use this update rule to obtain a new guess for the value of state S1. <br><br>\n",
    "\n",
    "- <u>The idea is that we'll update a guess with a guess</u> where the current guesses for the values of states S2 and S3 are used to obtain a new guess for the value of state S1. \n",
    "    - So we plug in the estimates for the values of states S2 and S3 and when we do that, we get negative two which is our new guess for the value of state S1 and will record that new value. \n",
    "    \n",
    "<img src=\"img/6_4_5_10.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>    \n",
    "    \n",
    "- Then, we'll do the same for the second state where we update the guess for the value of state S2 using the value of state S1. When we plug in the value, we get one as our new estimate for the value of state S2 and we'll keep track of that. \n",
    "\n",
    "<img src=\"img/6_4_5_11.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- Then onto state S3, we update the value using our current estimate for the value of state S1 (**my note**: it is the new value of S1, not the old value, i mean, we always use the new value, i.e we do not do batch update). Our new estimate is one which we can record. \n",
    "\n",
    "<img src=\"img/6_4_5_12.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- **We won't need to update the value of state S4 since it's a terminal state so it'll always have a value of zero.** \n",
    "\n",
    "- But we can loop back to the first state and continue to proceed in this way with updating our guess based on the current guess for the values of the other states. In this case, the value is updated to negative one which we can record. \n",
    "\n",
    "<img src=\"img/6_4_5_13.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- Then we move on to state S2, S3 and so on over and over and it turns out that this iterative algorithm yields an estimate that converges to the true value function. **This is the idea behind an algorithm known as Iterative Policy Evaluation and we'll go into much more detail soon.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.6) An Iterative method, Part 2\n",
    "- (my note before the reading )\n",
    "    - I need you to keep close attention to the result of multiplying two summations (b/c I am so vulnerable when dealing with summation notaion because i was not introduced to it in the faculty. I may also try to do a correspondance between summations and for-loops so that i may understand summation with the aid of my understanding of for-loops)\n",
    "\n",
    "---\n",
    "~~ the Reading ~~\n",
    "#### An Iterative Method\n",
    "In this concept, we will examine some ideas from the last video in more detail.\n",
    "\n",
    "<img src=\"img/6_4_6_1.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "#### Notes on the Bellman Expectation Equation\n",
    "In the previous video, we derived one equation for each environment state. For instance, for state $s_1$, we saw that:\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3))$.\n",
    "\n",
    "- We mentioned that this equation follows directly from the Bellman expectation equation for $v_\\pi$.\n",
    "    - $v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s] = \\sum_{a \\in \\mathcal{A}(s)}\\pi(a|s)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma v_\\pi(s'))$ <br>\n",
    "    (**The Bellman expectation equation for** $v_\\pi$)\n",
    "    \n",
    "- (**my notes**: about the previous equation)\n",
    "    - sum . sum , is it a dot product !! so we do not apply the sum at each function alone\n",
    "        - this way, only one for-loop will do this line of (two summations)\n",
    "    - is pi a vector ?? can be considered a vector ?? (if it is a vector, then if the sum is applied to it, it must be reduced to a single value befor going to the next sum !!)\n",
    "    - the $\\pi(a|s)$ is a probability regarding the policy, so it is something that the agent has learned to get the optimial solution. on the other hand, $p(s',r|s,a)$ is something about the environment, it is something that the agent is trying to learn about the environment (i.e how the environment works). \n",
    "\n",
    "In order to see this, we can begin by looking at what the Bellman expectation equation tells us about the value of state $s_1$ (where we just need to plug in $s_1$ for state $s$).\n",
    "\n",
    "$v_\\pi(s_1) = \\sum_{a \\in \\mathcal{A}(s_1)}\\pi(a|s_1)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s_1,a)(r + \\gamma v_\\pi(s'))$\n",
    "\n",
    "- Then, it's possible to derive the equation for state $s_1$ by using the following:\n",
    "    - $\\mathcal{A}(s_1)=\\{ \\text{down}, \\text{right} \\}$ \n",
    "        - (*When in state $s_1$, the agent only has two potential actions: down or right.*) <br><br>\n",
    "        \n",
    "    - $\\pi({down}|s_1) = \\pi(\\text{right}|s_1) = \\frac{1}{2}$ \n",
    "        - (*We are currently examining the policy where the agent goes down with 50% probability and right with 50% probability when in state $s_1$.*)<br><br>\n",
    "        \n",
    "    - $p(s_3,-3|s_1,\\text{down}) = 1$  $\\;\\;\\;\\;$    (and $p(s',r|s_1,\\text{down}) = 0$ if $s'\\neq s_3$ or $r\\neq -3$) \n",
    "        - (*If the agent chooses to go down in state $s_1$, then with 100% probability, the next state is $s_3$, and the agent receives a reward of -3.*)<br><br>\n",
    "        \n",
    "    - $p(s_2,-1|s_1,\\text{right}) = 1$  $\\;\\;\\;\\;$     (and $p(s',r|s_1,\\text{right}) = 0$ if $s'\\neq s_2$ or $r\\neq -1$) \n",
    "        - (If the agent chooses to go right in state $s_1$, then with 100% probability, the next state is $s_2$, and the agent receives a reward of -1.)<br><br>\n",
    "        \n",
    "    - $\\gamma = 1$ \n",
    "        - (*We chose to set the discount rate to 1 in this gridworld example.*)\n",
    "\n",
    "If this is not entirely clear to you, please take the time now to plug in the values to derive the equation from the video. Then, you are encouraged to repeat the same process for the other states.\n",
    "\n",
    "#### Notes on Solving the System of Equations\n",
    "In the video, we mentioned that you can directly solve the system of equations:\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3))$\n",
    "\n",
    "$v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$\n",
    "\n",
    "$v_\\pi(s_3) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$\n",
    "\n",
    "$v_\\pi(s_4) = 0$\n",
    "\n",
    "Since the equations for $v_\\pi(s_2)$ and $v_\\pi(s_3)$ are identical, we must have that $v_\\pi(s_2) = v_\\pi(s_3)$\n",
    "\n",
    "Thus, the equations for $v_\\pi(s_1)$ and $v_\\pi(s_2)$ can be changed to:\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_2)) = -2 + v_\\pi(s_2)$\n",
    "\n",
    "$v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + 0) = 2 + \\frac{1}{2}v_\\pi(s_1)$\n",
    "\n",
    "Combining the two latest equations yields\n",
    "\n",
    "$v_\\pi(s_1) = -2 + 2 + \\frac{1}{2}v_\\pi(s_1) = \\frac{1}{2}v_\\pi(s_1)$,\n",
    "\n",
    "which implies $v_\\pi(s_1)=0$. Furthermore, $v_\\pi(s_3) = v_\\pi(s_2) = 2 + \\frac{1}{2}v_\\pi(s_1) = 2 + 0 = 2$.\n",
    "\n",
    "Thus, the state-value function is given by:\n",
    "\n",
    "$v_\\pi(s_1) = 0v$ \n",
    "\n",
    "$v_\\pi(s_2) = 2$\n",
    "\n",
    "$v_\\pi(s_3) = 2$\n",
    "\n",
    "$v_\\pi(s_4) = 0$\n",
    "\n",
    "**Note.** This example serves to illustrate the fact that it is ***possible*** to <u>*directly*</u> solve the system of equations given by the Bellman expectation equation for $v_\\pi$. However, in practice, and especially for much larger Markov decision processes (MDPs), we will instead use an <u>*iterative*</u> solution approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.7) Quiz: An Iterative method\n",
    "\n",
    "#### Quiz: An Iterative Method\n",
    "So far in this lesson, we have discussed how an agent might obtain the state-value function $v_\\pi$ corresponding to a policy $\\pi$.\n",
    "\n",
    "- In the dynamic programming setting, the agent has full knowledge of the Markov decision process (MDP). In this case, it's possible to use the one-step dynamics $p(s',r|s,a)$ of the MDP to obtain a system of equations corresponding to the Bellman expectation equation for $v_\\pi$ \n",
    "    - **my note** : the one step dynamic was shown in the previous concept to be :\n",
    "        - $p(s_3,-3|s_1,\\text{down}) = 1$  $\\;\\;\\;\\;$    (and $p(s',r|s_1,\\text{down}) = 0$ if $s'\\neq s_3$ or $r\\neq -3$) \n",
    "        - $p(s_2,-1|s_1,\\text{right}) = 1$  $\\;\\;\\;\\;$     (and $p(s',r|s_1,\\text{right}) = 0$ if $s'\\neq s_2$ or $r\\neq -1$) \n",
    "\n",
    "In the gridworld example, the system of equations corresponding to the equiprobable random policy was given by:\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3))$\n",
    "\n",
    "$v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$\n",
    "\n",
    "$v_\\pi(s_3) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$\n",
    "\n",
    "$v_\\pi(s_4) = 0$\n",
    "\n",
    "In order to obtain the state-value function, we need only solve the system of equations.\n",
    "\n",
    "While it is always possible to directly solve the system, we will instead use an iterative solution approach.\n",
    "\n",
    "#### An Iterative Method\n",
    "The iterative method begins with an initial guess for the value of each state. In particular, we began by assuming that the value of each state was zero.\n",
    "\n",
    "Then, we looped over the state space and amended the estimate for the state-value function through applying successive update equations.\n",
    "\n",
    "Recall that $V$ denotes the most recent guess for the state-value function, and the update equations are:\n",
    "\n",
    "$V(s_1) \\leftarrow \\frac{1}{2}(-1 + V(s_2)) + \\frac{1}{2}(-3 + V(s_3))$\n",
    "\n",
    "$V(s_2) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5)$\n",
    "\n",
    "$V(s_3) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5)$\n",
    "\n",
    "#### Quiz Question\n",
    "Say that the most recent guess for the state-value function is given in the figure below.\n",
    "\n",
    "<img src=\"img/6_4_7_1.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "Currently, the estimate for $v_\\pi(s_2)$ is given by $V(s_2) = 1$.\n",
    "\n",
    "Say that the next step in the algorithm is to update $V(s_2)$.\n",
    "\n",
    "What is the new value for $V(s_2)$ after applying the update step once?\n",
    "\n",
    "- my solution : <br>\n",
    "$V(s_2) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5)$ <br>\n",
    "$V(s_2) \\leftarrow \\frac{1}{2}(-1 + -1) + \\frac{1}{2}(5)$ <br>\n",
    "$V(s_2) \\leftarrow -1 + 2.5$ <br>\n",
    "$V(s_2) \\leftarrow 1.5$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.8) Iterative Policy Evaluation\n",
    "- **note before the video**:\n",
    "    - I assume that for larger problems, the iterative process is faster than solving the sys of eqs b/c the later (I assume) envolves inverse of big matrices. So I want to try and and plot the time needed for both methods to get to the answer while I increase the number of inputs. things to look for is : when the iterative method will be faster than the solving-eq method (at what number of inputs (critical number of input) )?\n",
    "        - recall that I got this intuition from the Coursera ML course when Dr.Andrew contrasted the Gradient Descent (which is an iterative method) vs the Normal equation (which is one shot solving-eqs method)\n",
    "---\n",
    "- earlier in this lesson you were introduced to an iterative method for determining the value function corresponding to a policy \n",
    "- in this video we'll discuss the algorithm in more detail  <br><br>\n",
    "\n",
    "- the algorithm is called iterative policy evaluation and <u>it assumes that the agent already has full and perfect knowledge of the MDP that characterizes the environment</u>  <br><br>\n",
    "\n",
    "- remember that this algorithm (in picture below) was motivated by the bellmen expectation equation for the state value function and this equation is really a system of equations since we have one equation for \n",
    "- each environment state each equation relates the value of a state to the values of its successor States. \n",
    "\n",
    "<img src=\"img/6_4_8_1.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- theoretically since we have a system of equations with one equation for each unknown quantity we could solve the system but there's an easier way where instead of solving the system exactly we'll construct an iterative algorithm where each step gets us closer and closer to solving this system of equations \n",
    "- specifically our iterative algorithm will adapt this bellman equation so it can be used as an update rule \n",
    "\n",
    "<img src=\"img/6_4_8_2.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- here the capital V denotes the current guess for the policies value function note that I've only changed two things about the bellman equation which I've underlined in yellow here  <br><br>\n",
    "\n",
    "- so the algorithm begins with an initial guess for the value function of the policy what's typically done is to set the initial guess for each state to 0 okay and so there's no way that's actually the true value function but we just need to start somewhere \n",
    "- then we'll loop over the states and use the update rule to improve our guess for the state value function \n",
    "\n",
    "<img src=\"img/6_4_8_3.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- what's nice is that as long as a few technical conditions are satisfied this algorithm is guaranteed to converge to the value function for the policy \n",
    "- now we can only attain true convergence in the limit of running this algorithm an infinite number of times and that's not feasible in practice so we'll have to stop short of true convergence \n",
    "- and the question is how can we tell when we've gotten close enough \n",
    "    - well in practice when you implement the algorithm you'll notice that the first few times the update step is applied there are big changes to the value function but then eventually at a later time point you'll notice that updates are hardly noticeable and once we get to that point we're applying the update rule doesn't change the estimate of the value function much that's a strong indication that our algorithm has gotten reasonably close to converging to the true value function \n",
    "    - and so inspired by this fact we can design a stopping criterion that terminates the algorithm whenever we've done a complete pass over all the states and none of the values has changed much \n",
    "        - to do this we'll amend the pseudocode where \n",
    "            - the first step is for you to set a very small positive number theta \n",
    "            - next as before you initialize the first guess for the value function to be all zeros \n",
    "            - then you enter the iterative loop where each step you apply the development update and check to see how much the values for each of the states has changed if the maximum change over all states is less than that small number theta that you set then you stop the algorithm and say that your estimate for the value function is good enough\n",
    "            \n",
    "<img src=\"img/6_4_8_4.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- and that's it but now if that update step (she highlighted the bellman assignment line) feels suspicious to you I don't blame you and your instincts to one a proof are correct it's not at all intuitive that this algorithm should work the way it does but here's the main idea behind what that update step is doing\n",
    "\n",
    "<img src=\"img/6_4_8_5.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- remember that we loop over states and apply the update to one state at a time and from each state the agent could choose any of a number of possible actions that bring it to any of a number of potential next States and the bellman equation helps us relate the value of this parent state, $s$ to the values of all of its possible successor states $s'$. <br><br>\n",
    "\n",
    "- now what we do is look at the estimated value function for the parent state (she underlined $V(s)$) along with the values of the successor States (she underlined $V(s')$) and if we plug in those values to the bellman equation and it's not satisfied, what we'll do is we'll change the value of the parent state so that the bellman equation is satisfied, then we'll do the same for the second state and so on, and what's important to note is that if at some point we go to apply this update rule and there's no change in any of the values of any of the states then that means we have value function that perfectly satisfies the bellman equation and the only value function that does that is the true value function of the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.9) Implementation\n",
    "#### Implementation: Iterative Policy Evaluation\n",
    "\n",
    "The pseudocode for iterative policy evaluation can be found below.\n",
    "\n",
    "- **my note**: in the most inner for-loop , the line $\\Delta \\leftarrow max(\\Delta, |v-V(s)|)$ , I want to demonstrate why there is a max function between two ipnuts:\n",
    "    - in $|v-V(s)|$ we store the error of the current state\n",
    "    - but in $\\Delta$ we store the maximum error of all states in one loop. this is because we want the maximum error of all state to be less the $\\theta$ (this what breaks the outer loop, not single error of one state being less that $\\theta$, but rather, all error in all states are less than $\\theta$ ) <br><br>\n",
    "    \n",
    "- Note that policy evaluation is guaranteed to converge to the state-value function $v_\\pi$ corresponding to a policy $\\pi$, as long as $v_\\pi(s)$ is finite for each state $s\\in\\mathcal{S}$. For a finite Markov decision process (MDP), this is guaranteed as long as either:\n",
    "    - $\\gamma < 1$, or\n",
    "    - if the agent starts in any state $s\\in\\mathcal{S}$, it is guaranteed to eventually reach a terminal state if it follows policy $\\pi$.\n",
    "    \n",
    "Please use the next concept to complete Part 0: Explore FrozenLakeEnv and Part 1: Iterative Policy Evaluation of Dynamic_Programming.ipynb. Remember to save your work!    \n",
    "\n",
    "#### (Optional) Additional Note on the Convergence Conditions\n",
    "- To see intuitively *why* the conditions for convergence make sense, consider the case that neither of the conditions are satisfied, so:\n",
    "    - $\\gamma = 1$, and\n",
    "    - there is some state $s\\in\\mathcal{S}$ where if the agent starts in that state, it will never encounter a terminal state if it follows policy $\\pi$.\n",
    "\n",
    "- In this case,\n",
    "    - reward is not discounted, and\n",
    "    - an episode may never finish.\n",
    "\n",
    "Then, it is possible that iterative policy evaluation will not converge, and this is because the state-value function may not be well-defined! To see this, note that in this case, calculating a state value could involve adding up an infinite number of (expected) rewards, where the sum may not [converge](https://en.wikipedia.org/wiki/Convergent_series).\n",
    "\n",
    "- In case it would help to see a concrete example, consider an MDP with:\n",
    "    - two states $s_1$ and $s_2$, where $s_2$ is a terminal state\n",
    "    - one action $a$ (*Note: An MDP with only one action can also be referred to as a [Markov Reward Process (MRP)](https://en.wikipedia.org/wiki/Markov_reward_model).*)\n",
    "    - $p(s_1,1|s_1, a) = 1$\n",
    "    \n",
    "In this case, say the agent's policy $\\pi$ is to \"select\" the only action that's available, so $\\pi(s_1) = a$. Say $\\gamma = 1$. According to the one-step dynamics, if the agent starts in state $s_1$, it will stay in that state forever and never encounter the terminal state $s_2$.\n",
    "\n",
    "In this case, $v_\\pi(s_1)$ **is not well-defined**. To see this, remember that $v_\\pi(s_1)$ is the (expected) return after visiting state $s_1$, and we have that\n",
    "\n",
    "$v_\\pi(s_1) = 1 + 1 + 1 + 1 + ...$\n",
    "\n",
    "which [diverges](https://en.wikipedia.org/wiki/Divergent_series) to infinity. (Take the time now to convince yourself that if either of the two convergence conditions were satisfied in this example, then $v_\\pi(s_1)$ would be well-defined. As a very optional next step, if you want to verify this mathematically, you may find it useful to review [geometric series](https://en.wikipedia.org/wiki/Geometric_series) and the [negative binomial distribution](https://en.wikipedia.org/wiki/Negative_binomial_distribution).)\n",
    "\n",
    "(**my note**: i think the video of the boat that decided to loop around regenerated coins instead of going to the finish line, is a problem where the state value will not converge.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.10) Mini Project: DP (Parts 0 and 1)\n",
    "\n",
    "#### Part 0: Explore FrozenLakeEnv\n",
    "Use the code cell below to create an instance of the FrozenLake environment.\n",
    "\n",
    "~~~ **start python code**\n",
    "```python\n",
    "!pip install -q matplotlib==2.2.2\n",
    "from frozenlake import FrozenLakeEnv\n",
    "\n",
    "env = FrozenLakeEnv()\n",
    "```\n",
    "~~~ **end python code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent moves through a $4 \\times 4$ gridworld, with states numbered as follows:\n",
    "```\n",
    "[[ 0  1  2  3]\n",
    " [ 4  5  6  7]\n",
    " [ 8  9 10 11]\n",
    " [12 13 14 15]]\n",
    "```\n",
    "and the agent has 4 potential actions:\n",
    "```\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "```\n",
    "\n",
    "Thus, $\\mathcal{S}^+ = \\{0, 1, \\ldots, 15\\}$, and $\\mathcal{A} = \\{0, 1, 2, 3\\}$.  Verify this by running the code cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~ **start python code**\n",
    "```python\n",
    "# print the state space and action space\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "# print the total number of states and actions\n",
    "print(env.nS)\n",
    "print(env.nA)\n",
    "```\n",
    "~~~ **end python code**\n",
    "\n",
    "output : <br>\n",
    "Discrete(16) <br>\n",
    "Discrete(4)  <br>\n",
    "16  <br>\n",
    "4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic programming assumes that the agent has full knowledge of the MDP.  We have already amended the `frozenlake.py` file to make the one-step dynamics accessible to the agent.  \n",
    "\n",
    "Execute the code cell below to return the one-step dynamics corresponding to a particular state and action.  In particular, `env.P[1][0]` returns the the probability of each possible reward and next state, if the agent is in state 1 of the gridworld and decides to go left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~ **start python code**\n",
    "```python\n",
    "env.P[1][0]\n",
    "```\n",
    "~~~ **end python code**\n",
    "\n",
    "output : <br>\n",
    "[(0.3333333333333333, 1, 0.0, False), <br>\n",
    " (0.3333333333333333, 0, 0.0, False), <br>\n",
    " (0.3333333333333333, 5, 0.0, True)]\n",
    " \n",
    "---\n",
    "- My explanation for the previous results. if the agent is in state 1, and decided to go left, then it has \n",
    "    - $\\frac{1}{3}$ probability of statying in the same state with no reward , and since this state is not terminal, we are not done yet (i.e False)\n",
    "    - $\\frac{1}{3}$ probability of going left to the state 0 with no reward , and since this state is not terminal state, we are not done yet (i.e False)\n",
    "    - $\\frac{1}{3}$ probability of going down to state 5 with no reward , and since this state is a terminal state, we are done (i.e True)\n",
    "        - i think there is no reward here b/c although it is a terminal state, this is not the target\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry takes the form \n",
    "```\n",
    "prob, next_state, reward, done\n",
    "```\n",
    "where: \n",
    "- `prob` details the conditional probability of the corresponding (`next_state`, `reward`) pair, and\n",
    "- `done` is `True` if the `next_state` is a terminal state, and otherwise `False`.\n",
    "\n",
    "Thus, we can interpret `env.P[1][0]` as follows:\n",
    "$$\n",
    "\\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=1,A_t=0) = \\begin{cases}\n",
    "               \\frac{1}{3} \\text{ if } s'=1, r=0\\\\\n",
    "               \\frac{1}{3} \\text{ if } s'=0, r=0\\\\\n",
    "               \\frac{1}{3} \\text{ if } s'=5, r=0\\\\\n",
    "               0 \\text{ else}\n",
    "            \\end{cases}\n",
    "$$\n",
    "\n",
    "To understand the value of `env.P[1][0]`, note that when you create a FrozenLake environment, it takes as an (optional) argument `is_slippery`, which defaults to `True`.  \n",
    "\n",
    "To see this, change the first line in the notebook from `env = FrozenLakeEnv()` to `env = FrozenLakeEnv(is_slippery=False)`.  Then, when you check `env.P[1][0]`, it should look like what you expect (i.e., `env.P[1][0] = [(1.0, 0, 0.0, False)]`).\n",
    "\n",
    "The default value for the `is_slippery` argument is `True`, and so `env = FrozenLakeEnv()` is equivalent to `env = FrozenLakeEnv(is_slippery=True)`.  In the event that `is_slippery=True`, you see that this can result in the agent moving in a direction that it did not intend (where the idea is that the ground is *slippery*, and so the agent can slide to a location other than the one it wanted).\n",
    "\n",
    "Feel free to change the code cell above to explore how the environment behaves in response to other (state, action) pairs.  \n",
    "\n",
    "Before proceeding to the next part, make sure that you set `is_slippery=True`, so that your implementations below will work with the slippery environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Iterative Policy Evaluation\n",
    "\n",
    "In this section, you will write your own implementation of iterative policy evaluation.\n",
    "\n",
    "Your algorithm should accept four arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `theta`: This is a very small positive number that is used to decide if the estimate has sufficiently converged to the true value function (default value: `1e-8`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s` under the input policy.\n",
    "\n",
    "<img src=\"img/6_4_10_1.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_4_9_1.PNG\" alt=\"Drawing\" style=\"width: 477px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS): # for each state\n",
    "            Vs = 0\n",
    "            for a, action_prob in enumerate(policy[s]): # for each available action and its corresponding probability (note I think that the available action is set by the policy). also look here for a refresher about enumerate in python -> https://www.geeksforgeeks.org/enumerate-in-python/\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, np.abs(V[s]-Vs))\n",
    "            V[s] = Vs\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### my illustration for the previous code\n",
    "- the target of the previous code is to use the policy (table) to get the value fuction for each state. (note that the policy is given)\n",
    "- normally, i would expect to have only two for loops to go through the 2D policy table and use that in filling the 1D array of the Value-functions, however, since this example is with slippery mode (so for each action in each state there are multiple probabilities fot several next state (of course this is given by the environment), so that's why we needed a third loop.) <br><br>\n",
    "\n",
    "- take, for example, the state s1\n",
    "    - state s1 can take one of four actions, and for each action there are several probabilities of the next state .\n",
    "    - i.e ...  s1 $ \\begin{cases} 0 (LEFT) \\begin{cases} (\\frac{1}{3}, 1, 0.0, False)  \\\\ (\\frac{1}{3}, 0, 0.0, False) \\\\ (\\frac{1}{3}, 5, 0.0, True) \\end{cases} \\\\ 1 (DOWN) \\\\ 2 (RIGHT) \\\\ 3 (UP) \\end{cases} $\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the equiprobable random policy $\\pi$, where $\\pi(a|s) = \\frac{1}{|\\mathcal{A}(s)|}$ for all $s\\in\\mathcal{S}$ and $a\\in\\mathcal{A}(s)$.  \n",
    "\n",
    "Use the code cell below to specify this policy in the variable `random_policy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_4_10_2.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"img/6_4_10_3.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to evaluate (using the function i just made above) the equiprobable random policy and visualize the output.  The state-value function has been reshaped to match the shape of the gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import plot_values\n",
    "\n",
    "# evaluate the policy \n",
    "V = policy_evaluation(env, random_policy)\n",
    "\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output : \n",
    "    \n",
    "<img src=\"img/6_4_10_4.png\" alt=\"Drawing\" style=\"width: 338px;\"/>    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.11) Action Values\n",
    "In a previous concept, you wrote your own implementation of iterative policy evaluation to estimate the state-value function $v_\\pi$ for a policy $\\pi$. In this concept, you will use the simple gridworld from the videos to practice converting a state-value function $v_\\pi$ to an action-value function $q_\\pi$.\n",
    "\n",
    "Consider the small gridworld that we used to illustrate iterative policy evaluation. The **state-value function** for the equiprobable random policy is visualized below.\n",
    "\n",
    "<img src=\"img/6_4_11_1.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "Take the time now to verify that the below image corresponds to the **action-value function** for the same policy.\n",
    "\n",
    "<img src=\"img/6_4_11_2.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "As an example, consider $q_\\pi(s_1, \\text{right})$. This action value can be calculated as\n",
    "\n",
    "$q_\\pi(s_1, \\text{right}) = -1 + v_\\pi(s_2) = -1 + 2 = 1$,\n",
    "\n",
    "where we just use the fact that we can express the value of the state-action pair $s_1$, $\\text{right}$ as the sum of two quantities: (1) the immediate reward after moving right and landing on state $s_2$, and (2) the cumulative reward obtained if the agent begins in state $s_2$ and follows the policy.\n",
    "\n",
    "- **my note** : note that i needn't have the policy known to get the action value function from the state value function, i only apply the method described above and will get the action value function for each state as a sum of two quantities. on the other hand, i must know the policy to be able to get the state value function for each state.\n",
    "\n",
    "Please now use the state-value function $v_\\pi$ to calculate $q_\\pi(s_1, \\text{down})$, $q_\\pi(s_2, \\text{left})$, $q_\\pi(s_2, \\text{down})$, $q_\\pi(s_3, \\text{up})$, and $q_\\pi(s_3, \\text{right})$.\n",
    "\n",
    "#### For More Complex Environments\n",
    "- In this simple gridworld example, the environment is **deterministic**. In other words, after the agent selects an action, the next state and reward are 100% guaranteed and non-random. For deterministic environments, $p(s',r|s,a) \\in \\{ 0,1 \\}$ for all $s', r, s, a$.\n",
    "    - In this case, when the agent is in state $s$ and takes action $a$, the next state $s'$ and reward $r$ can be predicted with certainty, and we must have $q_\\pi(s,a) = r + \\gamma v_\\pi(s')$ <br><br>\n",
    "\n",
    "- In general, the environment need not be deterministic, and instead may be **stochastic**. This is the default behavior of the `FrozenLake` environment from the mini project; in this case, once the agent selects an action, the next state and reward cannot be predicted with certainty and instead are random draws from a [(conditional) probability distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution) $p(s',r|s,a)$.\n",
    "    - In this case, when the agent is in state $s$ and takes action $a$, the probability of each possible next state $s'$ and reward $r$ is given by $p(s',r|s,a)$. In this case, we must have $q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$, where we take the [expected value](https://en.wikipedia.org/wiki/Expected_value) of the sum $r + \\gamma v_\\pi(s')$.\n",
    "\n",
    "Over the next couple concepts, you'll use this equation to write a function that yields an action-value function $q_\\pi$ corresponding to a policy $\\pi$ for the `FrozenLake` environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.12) Implementation\n",
    "#### Implementation: Estimation of Action Values\n",
    "\n",
    "In the next concept, you will write an algorithm that accepts an estimate $V$ of the state-value function $v_\\pi$, along with the one-step dynamics of the MDP $p(s',r|s,a)$, and returns an estimate $Q$ the action-value function $q_\\pi$.\n",
    "\n",
    "In order to do this, you will need to use the equation discussed in the previous concept, which uses the one-step dynamics $p(s',r|s,a)$ of the Markov decision process (MDP) to obtain $q_\\pi$ from $v_\\pi$. Namely,\n",
    "\n",
    "$q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$\n",
    "\n",
    "holds for all $s\\in\\mathcal{S}$ and $a\\in\\mathcal{A}(s)$.\n",
    "\n",
    "You can find the associated pseudocode below.\n",
    "\n",
    "<img src=\"img/6_4_12_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "Please use the next concept to complete **Part 2: Obtain** $q_\\pi$ **from** $v_\\pi$ of `Dynamic_Programming.ipynb`. Remember to save your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.13) Mini Project: DP (Parts 2)\n",
    "#### Part 2: Obtain $q_\\pi$ from $v_\\pi$\n",
    "\n",
    "In this section, you will write a function that takes the state-value function estimate as input, along with some state $s\\in\\mathcal{S}$.  It returns the **row in the action-value function** corresponding to the input state $s\\in\\mathcal{S}$.  That is, your function should accept as input both $v_\\pi$ and $s$, and return $q_\\pi(s,a)$ for all $a\\in\\mathcal{A}(s)$.\n",
    "\n",
    "Your algorithm should accept four arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "- `s`: This is an integer corresponding to a state in the environment.  It should be a value between `0` and `(env.nS)-1`, inclusive.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `q`: This is a 1D numpy array with `q.shape[0]` equal to the number of actions (`env.nA`).  `q[a]` contains the (estimated) value of state `s` and action `a`.\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_from_v(env, V, s, gamma=1):\n",
    "    q = np.zeros(env.nA)\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    for a in range(env.nA): # my Q\\ what about if the action is not allowed (i.e if it will take the agent out of the maze), i think this is handeled by the environment, in which it will give such action a probability of 0\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            q[a] += prob * (reward + gamma * V[next_state])\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to print the action-value function corresponding to the above state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.nS, env.nA])\n",
    "for s in range(env.nS):\n",
    "    Q[s] = q_from_v(env, V, s)\n",
    "print(\"Action-Value Function:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_4_13_1.png\" alt=\"Drawing\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.14) Policy Improvement\n",
    "\n",
    "- I hope you enjoyed implementing iterative policy evaluation in the first part of the mini project. Feel free to use your algorithm to evaluate a policy for any finite MDP of your choosing. You need not confine yourself to the Frozen Lake environment. Just remember that policy evaluation requires the agent to have full knowledge of the MDP. Then if you like, you can use the second part of the mini project to obtain the corresponding action value function. But for now, let's continue our search for an optimal policy. <br><br>\n",
    "\n",
    "- Policy evaluation gets us partially there (my note: i think, by \"there\" she means to the destination of solving the RL problem). After all, in order to figure out the best policy, it helps to be able to evaluate candidate policies. \n",
    "\n",
    "<img src=\"img/6_4_14_1.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "- In this video, you'll learn about policy improvement, an algorithm that uses the value function for a policy in order to propose a new policy that's at least as good as the current one. \n",
    "\n",
    "<img src=\"img/6_4_14_2.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "- But why would we want to do that? Well, immediately we can see how policy evaluation and policy improvement could go quite nicely together. \n",
    "\n",
    "<img src=\"img/6_4_14_3.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "- Policy evaluation takes a policy and yields the value function. Then we can take that same value function and use policy improvement to get a potentially new and improved policy. Then it's possible to take that new policy, plug it in to do policy evaluation again, then policy improvement over and over until we converge to an optimal policy. \n",
    "\n",
    "\n",
    "\n",
    "- Let's try to understand this better in the context of the grid world from earlier in the lesson. \n",
    "    - Remember that this was an episodic task where the only terminal state was the bottom right hand corner. \n",
    "\n",
    "<img src=\"img/6_4_14_4.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "- _\n",
    "    - We assume that the agent already knows everything about the environment. So it knows how transitions happen and how reward is decided. And in particular, it does not need to interact with the environment to get this information, but it still needs to determine the optimal policy.\n",
    "\n",
    "<img src=\"img/6_4_14_5.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "- _    \n",
    "    - So say the agent starts off with an initial guess for the optimal policy. In a previous video, we saw that it made sense to start with the equiprobable random policy, where in each state the agent randomly picks from a set of available actions. \n",
    "    - So based on this idea, the agent calculates the corresponding value function through iterative policy evaluation. We did this in an earlier video, where we saw that this was the value function. \n",
    "    \n",
    "<img src=\"img/6_4_14_6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>    \n",
    "    \n",
    "- _    \n",
    "    - So now the question becomes, how might we design this policy improvement step to find a policy that's at least as good as the current one? We'll break policy improvement into two steps. \n",
    " \n",
    "<img src=\"img/6_4_14_7.png\" alt=\"Drawing\" style=\"width: 700px;\"/> \n",
    "\n",
    "- _\n",
    "    - The first step is to convert the state value function to an actual value function. You've already seen how to implement this. And when we only follow that same procedure on this small grid world, we get this action-value function. \n",
    "    \n",
    "<img src=\"img/6_4_14_8.png\" alt=\"Drawing\" style=\"width: 700px;\"/> \n",
    "\n",
    "- _    \n",
    "    - So now (i think this is the second step) we need only focus on how to use this action-value function to obtain a policy that's better than the equal probable random policy. \n",
    "    - So here's the idea. For each state, we'll just pick the action that maximizes the action-value function. So beginning with the state in the top left corner, one is greater than negative one, so the policy will be to go right, instead of down. Then (she highlighted the top right corner state) five is greater than negative one, so the policy goes down here. And again, (she highlighted the bottom left corner state) five is greater than negative one, so the policy goes right. \n",
    "    - You might be wondering at this point what would happen if we encountered a state with multiple choices of actions that maximize the action-value function. In this case you have options. \n",
    "        - You could arbitrarily pick one or \n",
    "        - construct a stochastic policy that assigns non-zero probability to any or all of them. We'll go more into this soon.\n",
    "    - But let's dig deeper into exactly why this (i think by \"this\" she means \"choosing the action that maximizes the action-value function\") should work. \n",
    "        - Remember that the value function corresponding to a policy just stores the expected return if the agent follows the policy for all time steps. \n",
    "        - And when we calculate the action-value function, we're looking at what would happen if the agent at some time step t chose an action a and then henceforth followed the policy. \n",
    "        \n",
    "<img src=\"img/6_4_14_9.png\" alt=\"Drawing\" style=\"width: 700px;\"/>       \n",
    "       \n",
    "- _\n",
    "    - _\n",
    "        - And our method for constructing the next policy pi_prime, $\\pi'$, looks at this action-value function and for each state determines that first action $a$ that's best for maximizing return. In this way, it follows that whatever expected return was associated to following the old policy pi for all time steps, the agent has higher expected return If it initially follows policy pi_prime and then henceforth follows policy $\\pi$. \n",
    "        \n",
    "<img src=\"img/6_4_14_10.png\" alt=\"Drawing\" style=\"width: 700px;\"/>         \n",
    "        \n",
    "- _\n",
    "    - _\n",
    "        - That said, it's possible to prove not only is policy pi_prime, $\\pi'$, better to follow for the first time step, it's also better to follow for all time steps. In other words, it's possible to prove that policy pi_prime, $\\pi'$, is better than or equal to policy $\\pi$. \n",
    "        \n",
    "<img src=\"img/6_4_14_11.png\" alt=\"Drawing\" style=\"width: 700px;\"/> \n",
    "\n",
    "- _\n",
    "    - _        \n",
    "       - And remember in order to prove this, we need to show that the value function for policy pi_prime is greater than or equal to the value function for policy pi for all states. \n",
    "       \n",
    "<img src=\"img/6_4_14_12.png\" alt=\"Drawing\" style=\"width: 350px;\"/>       \n",
    "       \n",
    "- _\n",
    "    - _       \n",
    "       - If you'd like to see how to do this in more detail, you're encouraged to check out the optional reading in the textbook. For now we can plug this into our algorithm for policy improvement where as you can see (in the pic belwo), the idea is that you'll first calculate the action-value function from the state value function. Then, in order to construct a better policy for each state, we just need to pick one action that maximizes the action-value function. You'll have the chance to implement this algorithm yourself soon.\n",
    "       \n",
    "<img src=\"img/6_4_14_13.png\" alt=\"Drawing\" style=\"width: 700px;\"/>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.15) Implementation\n",
    "#### Implementation: Policy Improvement\n",
    "\n",
    "In the last lesson, you learned that given an estimate QQ of the action-value function q_\\piq \n",
    "π\n",
    "​\t  corresponding to a policy \\piπ, it is possible to construct an improved (or equivalent) policy \\pi'π \n",
    "′\n",
    " , where \\pi'\\geq\\piπ \n",
    "′\n",
    " ≥π.\n",
    "\n",
    "For each state s\\in\\mathcal{S}s∈S, you need only select the action that maximizes the action-value function estimate. In other words,\n",
    "\n",
    "\\pi'(s) = \\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)π \n",
    "′\n",
    " (s)=argmax \n",
    "a∈A(s)\n",
    "​\t Q(s,a) for all s\\in\\mathcal{S}s∈S.\n",
    "\n",
    "The full pseudocode for policy improvement can be found below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.16) Mini Project: DP (Parts 3)\n",
    "هاى هاى هاى اعمل اللى فى النوتة تحت و امسح الكلام دا بعدا كدا\n",
    "- note: since `policy` is initialized with zeros, the only items which are modified in `policy` will have a probability higher than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.17) Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.18) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.19) Mini Project: DP (Parts 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.20) Truncated Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.21) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.22) Mini Project: DP (Parts 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.23) Value Iteration\n",
    "- I did not quite understand the derivations that was demonstrated in this video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.24) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.25) Mini Project: DP (Parts 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.26) Check Your Understadning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.27) Summary\n",
    "برأيى الآن أرجع و أقرأ كل ال <br> \n",
    "summary <br>\n",
    "هات"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Monte Carlo Methods\n",
    "- see this post by eng. Mohamed Hammad : https://www.facebook.com/mohamed.hamedhammad/posts/2738975202841679"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.2) OpenAI Gym: BlackjackEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.3) MC Prediction: State Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.4) Implementation\n",
    "- I do not understand the `if-statement` in the algorithm! I mean, is there something wrong with \"$N(S_t) \\leftarrow N(S_t)+1$\"? I mean, how do we get the total number of the rewards that follows a state (while he only adds one when that state appears for the first time !) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.5) Mini Project: MC (Parts 0 and 1)\n",
    "- I do not fully understand the code and what it does, i think i need to make animation to visualize the `code` & `data types changes` while the code runs . (i think i must do this systematically using scripts in **After Effects**)\n",
    "- also i do not understand the plots that is generated ! ot what should i infer from it ?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.6) MC Prediction: Action Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.7) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.8) Mini Project: MC (Parts 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.9) Generalized Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.10) MC Control: Incremental Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.11) Quiz: Incremental Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.12) MC Control: Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.13) MC Control: Policy Improvement\n",
    "- At $3:22$, for the Epsilon-Greedy Policy , I need to understand how these two cases of probabilities add up (i need to use numercial example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.14) Quiz: Epsilon-Greedy Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.15) Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.16) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.17) Mini Project: MC (Parts 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.18) MC Control: Constant-alpha, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.19) MC Control: Constant-alpha, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.20) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.21) Mini Project: MC (Parts 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.22) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Temporal-Difference Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.1) Introducution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.2) OpenAI Gym: CliffWalkingEnv\n",
    "- I it a good general note for me to know the difference between coordinates in \n",
    "    - x-y pairs (in math) and \n",
    "    - row-column pairs (in programming)\n",
    "- the thing is : when i say a point at (1,2) in math , it is a point in the array in (2,1) , because when i change the row , i acually move in a vertical direction (y-direction) , and when i change the column, i actually move in the horizontal direction (x-direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.3) TD Prediciton: TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.4) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.5) Mini-Project TD (Parts 0 and 1)\n",
    "- Q\\ how do we prevent the agent from taking an action that goes out of bound of the array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.6) TD Prediction: Action Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.7) TD Control: Sarsa(0)\n",
    "- i still do not know what does \"predciont\" and \"control\" in Monte Carlo and in TD !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.8) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.9) Mini-ProjectL TD (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.10) TD Control: Sarsamax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.11) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.12) Mini-Project TD (Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.13) TD Control: Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.14) Implementaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.15) Mini-Project TD (Part 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.16) Analyzing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.17) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Lesson 7 : Solve OpenAI Gym's Taxi-v2 Task </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-7.1) Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-7.2) Instructions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=red> (6-7.3) Mini-Project </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8 : RL in Continuous Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.1) Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.2) Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.3) Discrete vs. Continuous Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.4) Quiz: Space Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.5) Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (6-8.6) Exercise: Discretization </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.7) Tile Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (6-8.8) Exercise: Tile Coding </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.9) Coarse Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.10) Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.11) Linear Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.12) Kernel Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.13) Non-Linear Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.14) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 9 : Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.1) Intro to Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.2) Neural Nets as Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.3) Monte Carlo Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.4) Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.5) Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.6) Deep Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.7) Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.8) Fixed Q Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.9) Deep Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.10) DQN Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.11) Implementing Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (6-9.12) TensorFlow Implementation </font>\n",
    "<font color=red> Warning: I did not download this. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.13) Wrap Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 10 : Policy-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.1) Policy-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.2) Why Policy-Based Methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.3) Policy Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.4) Stochastic Policy Search\n",
    "- First, I wanted to know the difference between stochastic and random, so using google :\n",
    "    - A variable is random. A process is stochastic. Apart from this difference, the two words are synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.5) Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.6) Monte Carlo Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.7) Constrained Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.8) Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 11 : Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.1) Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.2) A Better Score Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.3) Two Function Approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.4) The Actor and The Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.5) Advantage Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.6) Actor-Critic with Advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.7) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links to tensor flow by jay alammar https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a tweet by Mat Leonard, in which someone,in a blog, talks about why we may leave matplotlib in favor of Altair for visualization, the blog starts by saying \"Sadly, in Python, we do not have a ggplot2.\" (the visualization library in R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side effects in python !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999964\n"
     ]
    }
   ],
   "source": [
    "print(8.5-8.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see this link for explanation -> https://www.quora.com/In-Python-why-does-8-5-8-4-give-0-099999999999999964"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 403,
   "position": {
    "height": "40px",
    "left": "921px",
    "right": "20px",
    "top": "79px",
    "width": "335px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
