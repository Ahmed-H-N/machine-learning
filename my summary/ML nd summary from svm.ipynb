{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-5.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-5.5) Perceptron Algorithm\n",
    "- in Wx+b=0 or =1,2 etc (which is equivalent of making b:=b-1 or b-2 etc)\n",
    "    - <div dir=rtl> برأيى إن سبب إن بينتج خطوط متوازية من تغيير ال b لأن الميل ثابت ، حيث تذكر إن الميل بيأثر فيه المتجه W</div>\n",
    "\n",
    "### (3-5.7) Margin Error\n",
    "> لحد الآن أنا شايف إنه هو نفس الخط ، لكن لما ضرب ال دبليو و ال بى فى اتنين، فبالرغم إنه نفس الخط لكن الماردن اتغير\n",
    "    اللى لسة عاةز أشوفه ، أمال استفدنا احنا ايه لما هو نفس الخط ؟! <br>\n",
    "    اااه اعتقد لسة احنا مش بنقول هنستفاد إيه ، احنا بس بنعمل صيغة و بنوضح إزاى ممكن المارجن ندخله فى معادلة الايرور ، فأكيد هنشوف دا بعد شوية إزاى هنستخدمه ، أكيد انا منتظر إن الخط يميل وكدا ويتحرك و يتغير صح ؟ هشوف\n",
    "- المهم تلاحظ من الفيديو دا إزاى بيزيد أو يقل المارجن بإننا بنضرب معاملات المعادلة كلها فى ثابت\n",
    "\n",
    "### (3-5.8) (Optional) Margin Error Calculation \n",
    " عاوز أفهم آخر جملة قبل الرسمة 4 <br>\n",
    "بعد الرسمة 4 ، ليه اشترط إن مدام المقام هو قيمة مطلقة ، فنقدر نعمل اللى عمله... طب لو كان المقام قيمة متجهة ،، مكنش هينفع؟ ليه؟\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-6.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-6.9) AdaBoost in sklearn\n",
    "أنا مش فاهم دلوقت هل فى خطأ فى الشرح لما قال إن <br> \n",
    "model has been fitted to a tree ??!!\n",
    "\n",
    "فبالتالى أنا عاوز أعرف هل أنا بعمل <br> \n",
    "fitting to the model first then use tha AdaBoost ? or it is demonstrated wrongly in that section? <br><br>\n",
    "\n",
    "<div dir=rtl>\n",
    "عاوز أجرب كذا base_estimator و أشوف بيعملوا إيه ،، و كمان هل ممكن أعمل ال base_estimator خوارزميات مختلفة ؟ ولا لازم نفس الخوارزيمة هى اللى بيتم استخدامها كذا مرة ؟\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.1) Announcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.2) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.3) Classification Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.4) Classification Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.5) Linear Boundries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.6) Higher Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.7) Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.8) Perceptron as Logical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.9) Why \"Neural Networks\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.10) Perceptron Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.11) Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.11) Non-Linear Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.12) Error Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.13) Log-loss Error Function \n",
    "- Nice reasoning for why we can't use \"count of error points\" as the error metric (b/c it is discrete and discrete functions are not continuous, and we really need a continuous error function (and differentiable) so that a small change in the precision will be captured as transition (change) in the output of the funtion) (but in discrete error func, we can change the function of the line , but the error is still the same). <br>\n",
    "- Note: don't confuse the error function (loss function) with the function of the line (the hypothesis)\n",
    "- <font color=red>**Q\\**</font> what is the \"loss\" in \"log-loss error\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.14) Discrete vs Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.15) Softmax\n",
    "- i need to know: how softmax turns into sigmoid when n=2 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.16) One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.17) Maximum likelihood\n",
    "- my note: the reason that made the first model give low overall probability is that : the misclassified points have low probability of belonging to their right region. ex, the misclassified red point , has P(blue) =0.9 , and hence have P(red)0.1, but notice that we use the latter in calculating the whole probability of the model, i.e we use the P(red) fot the red point, and P(blue) if the point is actuallu blue. <br>\n",
    "- I might ask, what is the meaning of the red point having high P(blue)=0.9? and i will tell that this is the wrong prediction made by the model . the model assumes that this point has higher probability of being blue because it lies far in the blue region.\n",
    "- i still need a formal definition of P(all) and maximum liklihood , are they the samething or do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.18) Maximizing Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.19) Cross-Entropy 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.20) Cross-Entropy 2\n",
    "In the quiz, I stayed half an hour searching for the error, and it was that i  made this line as <br>\n",
    "`for i in Y` <br>\n",
    "when it should be <br>\n",
    "`for i in range(len(Y))` <br>\n",
    "because I want `i` to be an increasing index, not each element in the `Y` list <br>\n",
    "وكدا كدا الحل دايما مش بيعمل فور لوب كدا، بل دا تفكيرك عشان انت متعود على السى ، لكن هنا اللستات بتتشقلب مع بعض عادى بدون لوب <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**why the udacity solution made the np.float_ to the two lists?** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.828313737302301\n",
      "[0. 1. 0. 0.]\n",
      "[0.6 0.4 0.9 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y=[1,0,1,1] \n",
    "P=[0.4,0.6,0.1,0.5]\n",
    "\n",
    "Y_new = np.float_(Y)\n",
    "P_new = np.float_(P)\n",
    "\n",
    "CE_entropy = -np.sum(Y_new * np.log(P_new) + (1 - Y_new) * np.log(1 - P_new)) # note that np.log is the natural log (ln)\n",
    "print (CE_entropy)\n",
    "\n",
    "# 1- Y , ERROR , can't do subtraction operation between tupes : int and  list\n",
    "print (1-Y_new)\n",
    "print (1-P_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y is python list , and you cant do operation such as `1-Y` on a python list. <br>\n",
    "Y_new is a numpy ndarray (n dimensional array), which can be operated on as `1-Y_new` and the result is also ndarray. <br>\n",
    "so np.float_(Y) casts the list to an ndarray  of float elements <br>\n",
    "I might ask, why he made it `float`, not `int`? well. obviously float is the general case, and since we almost always will have fractions (like we do in the P list) so we have to cast the lists to float lists <br>\n",
    "you might look at the ndarray and its function in the numpy documentaion : https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to know why numpy use underscore after the types , here is a link, and biscally numpy does that so that these types do not clash with the same type define in python (without underscore) -> https://stackoverflow.com/questions/6205020/numpy-types-with-underscore-int-float-etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.21) Multi-class Cross Entropy\n",
    "- i need to answer the question at the end of the video : how do we say that the cross entropy formula is the same for the multi class m>2 and for m=2 while we saw that it was different form at m=2.\n",
    "- first i want to point out that the previous case it think it was m=2 class, but in the example of the three animals, it is 3*animals i.e it is 6 classes because each animal defines two classes (exists or does not exist)\n",
    "- but the problem is that the summation is from j=0 to m, which (from looking at the doors) implies that m=3 not 6 !! \n",
    "- so here comes a second question, if we have 3 animals and two doors, how will we separate the number of doors and the number of animals in the formula, i think there is a lot of ambiguity in the example given in this topic of multi-class cross Entropy\n",
    "- third question, whay is the meaning of \"Cross\" in the Cross entropy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.22) Logistic Regression\n",
    "- Note: at the end of the video, $x^i$ denotes each point in our input data samples, because remember :\n",
    "    - $Wx+b$ is the equation of the line (this defines a line to be drawn, a seperator)\n",
    "    - $Wx^i+b$ this is not an equation of a line, but rather, it is an equation where we substitute $x$ by a point: $x^i$, so making this substitution will give as a single value (a number, not a line nor a separator)\n",
    "    - also rememebr that $x^i$ is a pair of values : $(x_1,x_2)$, i.e $x^i$ is a 2x1 vecotor (or 1x2 i dont remembr) , (or nx1 for higher dimensions.  also it could be 1xn, i will revise and see)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.22) Gradient Descent\n",
    "- <font color=red>**Q\\**</font> in calculating $\\frac{\\partial }{\\partial w_j}E$ , why did he considered $y$ to be constant w.r.t $w_j$ (i.e $\\frac{\\partial }{\\partial w_j}y=y$)? I ask this because as far as i know , $y=Wx+b$, so $y$ is **not** constant w.r.t $w_j$, right ? <br>\n",
    "- <font color=blue> **No my dear friend** </font> , I knew later that $y$ is the label ,(e.g in a binary class classification, y is 0 or 1), so it is a consntat , but this raises another question, if $\\hat{y}=Wx^i+b$ , so <font color=red>**Q\\**</font> what is the variable assgined to (or used to name) the hypothesis : (var? $= Wx+b$) ? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.23) Logistic Regresion Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.24) Pre-Lab: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.25) Notebook: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.26) Perceptron vs Gradient Descent\n",
    "it is importnat to note the values that can be taken by $\\hat{y}$ I both GD and Percptron algo.. note in which it is it continuous and in which it is discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.27) Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center> Example of code writing equations here, not in inline mode, so that all letters appear well (inline makes them very small)</center>**  <br>\n",
    "\\begin{equation*}\n",
    "P(E)   = {n \\choose k} p^k (1-p)^{ n-k}\n",
    "\\end{equation*}\n",
    "\n",
    "**<center> or make a boxed equation (i can use it for final results or for hint to recall previous equations say for derivative of $ln$, etc)</center>**  <br>\n",
    "\\begin{equation*}\n",
    "    \\boxed{P(E)   = {n \\choose k} p^k (1-p)^{ n-k}}\n",
    "\\end{equation*}\n",
    "\n",
    "**<center> what i want to do is : before any mathematical derivation, i list the mathimatical rules that i am gonna use, so that i remember it easily . i may preceed these equation by the ward \"remember:\"</center>**  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : Cloud Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.1) Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.2) Create an AWS Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.3) Get Access to GPU Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.4) Launch an Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.5) Login to the Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : Deep Neural Networks\n",
    "- References :\n",
    "    - A post shared by ibrahim sobh: a recipe for training nn : \n",
    "        - http://karpathy.github.io/2019/04/25/recipe/\n",
    "    - 37 reasons why your neural network is not working (other than overfitting) , i think he talks about the implementation itself\n",
    "        - medium link https://medium.com/m/global-identity?redirectUrl=https%3A%2F%2Fblog.slavv.com%2F37-reasons-why-your-neural-network-is-not-working-4020854bd607%3Ffbclid%3DIwAR0FtIi4E6FSyn2anmsCcmnrzCHt9OLSlciXm4O9YT4WGVohiQnFO2X9M0Q\n",
    "        - other link if the above did not work https://weekly-geekly.github.io/articles/334944/index.html\n",
    "    - deep mind tweet: For neural networks to be deployed in the real world, we need guarantees that our network satisfy desired specifications. We introduce a method to verify complex non-linear specifications. :https://twitter.com/DeepMindAI/status/1125503318749581312\n",
    "        - link of the paper : https://arxiv.org/pdf/1902.09592.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.1) Non-linear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.2) Continuous Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.3) Non-linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.4) Neural Networks Architecture\n",
    "- <font color=red>**Q\\**</font> when combining two nns, why take the sigmoid of the addition of the two points? i know that he said that we want the scale to be between 0 and 1, but we could've used averaging and it would achieve the same goal!! ... i may try both and see the resutling boundries (i should know how to plot these things!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.5) Feedforward\n",
    "- at video 1 : feedforward\n",
    "    - <font color=red>**Q\\**</font> at $0:41$, how did he know that $w_1$ is bigger than $w_2$ ?! is this implied from the drwaing ? i want to know this !!\n",
    "    - <font color=red>**Q\\**</font> at $3:34$, what is that strange circle between matrices ? is this a multiplication operatot or what ?!\n",
    "    - <font color=red>**Q\\**</font> what does `Dense` mean ?\n",
    "    - <font color=red>**Q\\**</font> what if i made a mistake (or changed my mind) and wanter to change something in the middle of the path of the network (e.g an activation function)?\n",
    "    - <font color=red>**Q\\**</font> it is written \"We can see that the output has dimension 1.\" , where can i see that ???\n",
    "    - <font color=red>**Q\\**</font> in the quiz, \n",
    "        - when he starts with a random seed to the numpy package, what does this line affect afterword?\n",
    "        - when he did \"One-hot encoding the output\", what was the shape of y before and after this step? (this will tell me why did he do this in the first place! i ask this because i see that the output is not categorical, so why did he do that ! )\n",
    "        - i skipped this quiz because there are lots of things that i do not understand. (كلفتونى أوى فى الكيراس!!) i will return and do it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.6) Backpropagation\n",
    "- **Recommendation from eng.mohamed hammad** look at : \n",
    "    - https://github.com/DebPanigrahi/Machine-Learning/blob/master/back_prop.pdf\n",
    "    - https://www.facebook.com/mohamed.hamedhammad/posts/2503343279738207\n",
    "    - https://www.facebook.com/mohamed.hamedhammad/posts/2538529736219561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.7) Keras\n",
    "- <font color=red>**Q\\**</font> why he writes `dtype=np.float32` in defining X and y ??\n",
    "- <font color=red>**Q\\**</font> why `X.shape[1]` what does it mean, and why not `X.shape[0]`?? i need to get the code here and see each output of these things and i shall demonstrate each line myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.8) Pre-Lab: Student Admission in Keras</font>\n",
    "- what does dense do ?\n",
    "- why did he put 32 as the first input to the neuron ?\n",
    "- how do we choose the number of hidden layers and the number of neurons in each layer ?\n",
    "- i want to see the effect of adding/changing each neuron/layer/activation function. <br><br>\n",
    "\n",
    "- <font color=blue> **answer** </font> the last two questions are part of the DNN hyperparamater tuning, do the changes in lab \"(4-4.8) Mini project: Training an MLP on MNIST\n",
    "\" to get the intuition, and see this link to know how to tune these parameters -> https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.9) Lab: Student Admission in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.10) Training Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.11) Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.12) Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.13) Regularization 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.14) Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-315) Local Minima\n",
    "- <font color=red>**Q\\**</font> what is the solution ?!!\n",
    "    - <font color=blue> the answer is at (4-3.20) Random Restart and (4-3.21) Momentum</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.16) Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.17) Other Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.18) Batch vs Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.19) Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.20) Random Restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.21) Momentum\n",
    "- <font color=red>**Q\\**</font> I don't understand why the momentum idea behaves in such manner! i mean, what is so special about the global minimum that makes the algorithm converge to it ? what if the global minimum was not so deep (surrounded by low humps that the algorithm may also go over it !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.22) Optimizers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.23) Error Functions Around the World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.24) Neural Network Regression\n",
    "- at $1:30$, he says \"the way we turn a linear function into a ReLU is by turning off the parts of the negative values (or underneath the x-axis into zero)\"\n",
    "    - I think by underneath x-axis he means (to the left of the y-axis, i.e the negative part of the x-axis)\n",
    "- <font color=red>**Q\\**</font> why we bother and use ReLU ? why not just use a linear function ? (what is the advantgae of ReLU over the linear function)    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.25) Neural Networks Playground\n",
    "How did Jay alammar made these beautiful interaction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.26) Mini Project Intro </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-3.27) Pre-Lab: IMDB Data in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-3.28) Lab: IMDB Data in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.29) Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read this tweet from ian goodfellow about OctConv which is a simple replacement for the traditional convolution operation that gets better accuracy with fewer FLOPs <br>\n",
    "https://twitter.com/goodfellow_ian/status/1117929612200120320?s=19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.1) Introducing Alexis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.2) Applications of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.3) How Computers Interprets Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.4) MLPs for Image Classification\n",
    "Q\\ what is the input shape in `model.add(Flatten(input_shape=X_train.shape[1:]))` ? i mean whay is [1:] does ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.5) Categorical Cross-Entropy\n",
    "- Q\\ `accuracy = 100*score[1]` what is returnd into the score var, so that we take its 2nd element  ?\n",
    "    - for the line `score = model.evaluate(X_test, y_test, verbose=0)` the documentation says: evaluate *\"Returns (1)the loss value & (2)metrics values for the model\"* \n",
    "    - this makes sense, but i still want to know what is the *\"the loss value\" is it the error value ? how does it compare with the metric value ? why would i need both numbers?*\n",
    "        - answer, in an upcoming video, i knew that : Overfitting is detected by comparing the validation loss to the training loss. If the training loss is much lower than the validation loss, then the model might be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.6) Model Validation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.7) When do MLPs (not) work well?\n",
    "- unitl perior of here, all video were about MLPs (Deep nn).\n",
    "- here the video started to introduce CNN and compare it with DNN (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-4.8) Mini project: Training an MLP on MNIST </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.9) Local Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.10) Convolution Layers (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.11) Convolution Layers (Part 2) \n",
    "- Q\\ why does she use ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.12) Stride and Padding  \n",
    "(stride = |noun| a long, decisive step. |verb|walk with long, decisive steps in a specified direction.  ) <br>\n",
    "(Padding = حشوة أو حشو)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.13) Convolutional Layers in Keras\n",
    "- Q\\ in the reading it says \"You are **strongly encouraged** to add a ReLU activation function to **every** convolutional layer in your networks.\" why is that ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.14) Quiz: Dimensionality\n",
    "- I will demonstrate the equation of the width of the conv layer **in the case of `padding = 'valid'`**\n",
    "\\begin{equation*}\n",
    "width = ceil(float( \\frac{W_{in} - F + 1}{float(S)}))\n",
    "\\end{equation*}\n",
    "- for the nomenator `W_in - F + 1` , i was able to think about it like this : \n",
    "    - `W_in - (F - 1)` which says: if i have a width W_in , and a filter with with width F, how many steps are required for the filter to slide over the W_in (assuming that stride (طول الخطوة) is 1)? the answer is `W_{in} - (F - 1)` , and for the `-1` you can understand it by thinking of some examples :\n",
    "        - if W_in =6 , and F=3  , it will require the filter to do 4 steps to reach the end of the image (try to do it manually to see it)\n",
    "        - also think of W_in=6 and F=2, it will take you 5 steps\n",
    "    - now the previous assumed that the stride is 1, if the stride (طول الخطوة) was S, simply devide on it to get the number of steps using this new stride. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.15) Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.16) Max Pooling Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.17) CNNs for Image Classification\n",
    "- at $1:16$, why she said \"where the spatial dimension is a **power of 2** or else a number that is **divisible by a large power of 2**\" . why exactly is these two constrains ?:\n",
    "    - **power of 2**\n",
    "    - **divisible by a large power of 2** (also why **large** power of 2 , not any power ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.18) CNNs in Keras: Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-4.19) Mini project: CNNs in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.20) Image Augmentation in Keras\n",
    "- Q\\ when the reading says : \"where x_train.shape[0] corresponds to the number of unique samples in the training dataset x_train.\" , i think by unique means \"not including the augmentation\" right ?\n",
    "- i don't understand the reading that says \"By setting steps_per_epoch to this value, we ensure that the model sees x_train.shape[0] augmented images in each epoch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-4.21)  Mini project: Image Augmentation in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.22) Groundbreaking CNN Architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.23) Visualizing CNNs (Part 1)\n",
    "- Q\\ at $1:06$, i do not whay does \"take a filter from a cnn, then construct images that maximize the activation of that filter \" mean ? \n",
    "    - answer found in this link , under the title \"Retrieving images that maximally activate a neuron\" -> http://cs231n.github.io/understanding-cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.24) Visualizing CNNs (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.25) Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.26) Transfer Learning in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Deep Learning for Cancer Detection with Sebastian Thrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.2) Skin Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.3) Survival Probability of Skin Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.4) Medical Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.5) The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.6) Image Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.7) Quiz: Data Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.8) Solution: Data Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.9) Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.10) Quiz: Random vs Pre-initialized Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.11) Solution: Random vs Pre-initialized Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.12) Validation the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.13) Quiz: Sensitivity and Specifity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.14) Solution: Sensitivity and Specifity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.15) More on Sensitivity and Specifity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.16) Quiz: Diagnosing Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.17) Solution: Diagnosing Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.18) Refresh on ROC curves\n",
    "عجبنى جدا الأنيميشن اللى فى فى آخر فيديو فى الصفحة ، برأيى هعمل زيه و أكتب (فى الريبو بتاع الإنتويشن) الاستفادة من الأنيميشن دا إيه أو الرسمة يعنى دى ناخد منها (نبنى عليها) قرارات إزاى ،،، و هشير الفيديو دا فى لنكد إن وفى تويتر و فى جروب الفيس بتاع الذكاء الاصطناعى"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.19) Quiz: ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.20) Solution: ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.21) Comparing our Results with Doctors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.22) Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.23) What is the network looking at?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.24) Refresh on Confusing Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.25) Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.26) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.27) Useful Resoucres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.28) Mini Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-5.29) Mini Project: Dermatologist AI </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.30) Share your Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Deep Learning Assesment\n",
    "QUESTION 2 OF 5: <br>\n",
    "my asnwer is calculating : $3*2 + 3*1$\n",
    "\n",
    "QUESTION 3 OF 5: <br>\n",
    "I need to understand why!\n",
    "\n",
    "QUESTION 4 OF 5: <br>\n",
    "what is the kernel ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Unsupervised Learning \n",
    "### introduction and sub-summary\n",
    "in this unit we have 9 lessons: <br>\n",
    "- lesson 9: we learn two concepts\n",
    "    - Random Projection : \n",
    "        - same as PCA but the line is chosen randomly.\n",
    "        - it is faster than PCA\n",
    "        - we can specify epslon or n_compeonets\n",
    "    - ICA : (Independit component analysis)\n",
    "        - statistal method applied when we think that features are statistically independet\n",
    "        - common use cases \n",
    "            - in seprating voice in the cocktail party problem\n",
    "            - in seperating signals in EEG\n",
    "            - some finance application but it is rarely useful to use ICA in finance since financial systems are very complex.\n",
    "        - it has a mathematical derivation, but the most importnat thing that you have to know is the assumptions that is considered to make this algo work, they are :\n",
    "            - a\n",
    "            - a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Clustreing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-.6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.11) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.13) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.14) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.15) \n",
    "note that the K-means algorithm is a hill climbing algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.17) \n",
    "important rule of thumps : the more classes u have, the more local minimum will exists, the more you need to run K-means algorithm to reach to the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : Clustering Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.2) k-means Clustering of Movie Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.3) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : Hierarchical Density-based Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.1) K-means considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.2) Overview of other clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.3) Hierarchical clustering: single-link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.4) Examining single-link clustering\n",
    "- i was wondering how the coloring of clusters is done in this algotithm (the final result) (b/c the dendrogram goes all the way to 1 cluster!)? \n",
    "    - and i think the answer is that the algortithm stops depending on a threshold that is set to tell us that two cluster should not be merged. I think this is mapped to the dendrogram when we see a long vertical distance before two clusters are merged (as Jay said in the video about the long vertical distance in the dendrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.5) complete-link, average-link, Ward\n",
    "- I don't understand how subtracting $A_1^2, A_2^2, B_1^2, B_2^2$ reduces variance ! what is the variance anyway in the context of clustering? (i want to visualize clusters with high variance and with low variance to compare the two.)\n",
    "- Q\\ $A_1 = A_2$ and $B_1 = B_2$ right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.6) Hierarchical clustering implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.7) [Lab] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.8) [Lab Solution] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.9) HC examples and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.10) [Quiz] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.11) DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.12) DBSCAN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.13) [Lab] DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.14) [Lab Solution] DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.15) DBSCAN examples & applications\n",
    "- it is very important to me to know that DBSCAN is used as anomaly detection (detecting outliers in a dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.16) [Quiz] DBSCAN\n",
    "- Question 2 of 2 : imporntant to note that DBSCAN is not a hierarchical clustering method, but is a density-based clustering method that is robust against noisy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Gaussian Mixture models and Cluster Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.2) Gaussian Mixture Model (GMM) Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.3) Gaussian Distribution in One Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.4) GMM Clustering in One Dimension\n",
    "Q\\ how did he know that the physics test was hard by looking at its distribution of scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.5) Gaussian Disrtibution in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.6) GMM in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.7) Quiz: Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.8) Overview of The Expectation Maximization (EM) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.9) Expectation Maximization Part 1\n",
    "- I want to study the things in statistics and probability that was mentioned in this video\n",
    "- i want to define soft clustering, is it assigning probabilities to points of membership to the available clusters? i will search for the proper definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.10) Expectation Maximization Part 2\n",
    "- in the weighted average calculation, i was surprised that we did not devide by the number of elements, but we devided by the sum of the probabilities (since all probabilities are less than 1, so this sum is sure less than the number of points).\n",
    "    - 1- i want to know why\n",
    "    - 2- i want to compare this with if i had devided by the number of points, and i want to visualize both and get an intuition about the difference between the right method and my wrong assumption.\n",
    "- i want to know what is the log-likelihood, an what is mixing coefficient (note: they are in statistics)\n",
    "- Q\\ what is the \"1n\" that is in the video that precedes the equation of evaluating the log-likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.11) Visual Example of EM Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.12) Quiz: Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.13) GMM Implementation\n",
    "- Q\\ i want to something about the content of the array that contains the predicted labels, what if a point is inside two clusters (i.e when the two clusters overlap as the case in the previous video), to which cluster will it be assigned ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note:** on the difference between probability and membership\n",
    "- The term \"membership\" got me thinking if there is a difference between the two terms : membership and probability. \n",
    "    - first i assumed that i will say \"probabiliy of X\" when X is an event in the future that may have some kind of probability to happen, while i will say \"the membership of X\" when i talk/analyse something that already exists, such as saying that the movie (which is something already exists) is a member of (or belonging to) the genre action (by 80%) and romance (by 20%).\n",
    "    - i also assumed that probability and membership have the same mathematical tools, but we say probabilty of membership depending on the thing that we study\n",
    "- my assumptions were not so bad, as i found out that there are two fields that are related alot : probability and fuzzy logic (the latter has the membership function). and with little search i found that there are differences. look at the following references : <br>\n",
    "https://goodmath.scientopia.org/2011/02/02/fuzzy-logic-vs-probability/ <br>\n",
    "https://www.researchgate.net/post/Difference_between_fuzzy_logic_and_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.14) GMM Examples & Applications\n",
    "- I am wondering if i can mix up GMM with CNN and come up with useful things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.15) Cluster Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.16) Cluster Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.17) External Validation indices\n",
    "- i want to compare the `external validation idnex` to calculating the `accuracy` of of the clustering (the ratio of the points that was correctly clustered). \n",
    "    - Q\\ why we needed the `external validation idnex` when we could just use the `accuracy`?\n",
    "        - i think the answer lies in the objective we need when we asses the clustering, it is not just how many points are correctly clustered, but rather there were two objectives that was mentioned in the previous video that assess the clustering, namely: being compact and separated.\n",
    "    -Q\\ what does it mean for `ARI` to be equal to `0` ? what about `-1` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.18) Quiz: Adjusted Rand index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.19) Internal Validation Indices\n",
    "- Q\\ why the formula devides by $max(a_i,b_i)$ , i mean 1- why we devide? 2- why we use the max (what is the benefit of using the maximum?)!? \n",
    "    - is it because this normailize the output of this index to be between `-1` and `1` ? (i still need to see when it becomes `-1` and when it becomes `1` and when it becomes `0` or `-0.5` etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.20) Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.21) GMM & Cluster Validation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.22) GMM & Cluster Validation Lab Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.1) Chris's T-shirt Size (intuition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.2) A Metric for Chris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.3) Height + Weight for Cameron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.4) Sarah's Height + Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.5) Chris 's Shirt Size by Our Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.6) Comparing Features with Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.7) Feature Scaling Formula Quiz 1\n",
    "- For the formula of feature scaling, I, Ahmed, want to demonstrated this formula.\n",
    "- so the formula is as follows:\n",
    "\\begin{equation*}\n",
    "x' = \\frac{x-x_{min}}{x_{max}-x_{min}}\n",
    "\\end{equation*}\n",
    "- take for example the array [10 , 15 , 20]\n",
    "- the numerator works on shifting the array so that the minimum number becomes zero, so when we do $x-x_{min}$ the resulting array is [0 , 5 , 10]\n",
    "- and the denominator works on scaling the result array by deviding on its range, the range would be max of the resulting new array (=10) or it could be calculated from the old array as $x_{max}-x_{min}$,\n",
    "- so the final result becomes [0 ,  0.5 ,  1]\n",
    "- note : \n",
    "    - $x$ represents each element in the old array (the old one),  $x_{min}$ and $x_{max}$ are single elements in the array (the old one)\n",
    "    - $x'$ represents each new element in the final result (i.e each rescaled element)\n",
    "- when I demonstrate this concept to students, i may say that different range of values introduce a problem in our ML algorithm, since our algorithms usually use \"addition\" [i need to give example here], and we already know (in the course of numercal  analysis for example) that when we add large number with small number, the small number contribute to the result in an non-sensible way (we actuall sometimes ignore the small term in the addition when it is added to a large number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.8) Feature Scaling Formula Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.9) Feature Scaling Formula Quiz 3\n",
    "- it is important to note the advantage and disadvante of rescaling to the range of [0 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(5-5.10) Min/Max Rescaling Codeing Quiz</font>\n",
    "- a very important note was given in that quiz which is, the straight forward solution will be prone to divide-by-zero error.\n",
    "    - i should always think of this error whenever i implement anything that has devision operation in it\n",
    "- look here to see several ways to make an operation (such as subtracting a number) on every element on a python list : \n",
    "    - https://stackoverflow.com/questions/4918425/subtract-a-value-from-every-number-in-a-list-in-python\n",
    "    - https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.11) Min/Max Scaler in sklearn\n",
    "- i did not understand the part when she talked about the content of the numpy array! (she said things about each training point and each feature, and i did not understand that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.12) Quiz on Algorithms Requiring Rescaling\n",
    "- important video!! and i want analyse (maybe search online for fast answers) the other ML algorithms i took so far to see which ones are affected by features ranges (so we have to do feature scaling with them) and which ones do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.1) Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.2) Trickier Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.3) One-Dimensional, or Two?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.4) Slightly Less Perfect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.5) Trickiest Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.6) PCA for Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.7) Center of a New Coordinate System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.8) Principal Axis of New Coordinate System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.9) Second Principal Component of New System\n",
    "- Q\\ i did not understand how did he made the $-1$ !!? i do not see how should i look at the new components to conclude the $\\Delta y$ and $\\Delta x$\n",
    "    - answer, assume that the vectors are on the origin point of the old coordinates, and try to find out the values of the new vectors that represent the new coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.10) Practice Finding Centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.11) Pactice Finding New Axes\n",
    "- after finding the chagne for the new x vector, getting the new y vector may be hard at first, but i know two ways to get it :\n",
    "    - recall that to get the slope of perpendicular  - > (اقلب الميل و غير إشارته)\n",
    "    - i can also draw an imaginary triangle of the change in x, and then rotate this triangle by +90 degrees and hence i will get the change in y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.12) Which Data is Ready for PCA\n",
    "- Q\\ for the third data set :\n",
    "    - why he said it is impossible to do regression for this dataset (why he said it is impossible to build a regression that goes vertically?) ?! (note he said \"remember\" so it might be mentioned in the older version of the course, i may see it, but is suggest to search for the answer online to not waste my time searching for small piece of info in tens of vides)\n",
    "    - why the PCA is not as follows : the x' is to the bottom and y' is to the right ?\n",
    "    \n",
    "- note : he said that the circualr dataset will always give a deterministic result of PCA, i need to know why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.13) When Does an Axis Dominate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.14) Measurable vs. Latent Features Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.15) From Four Features to Two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.16) Compression While Preserving Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.17) Composote Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.18) Maximal Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.19) Advantages of Maximal Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.20) Maximal Variance and Information Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.21) Info Loss and Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.22) Neighborhood Composite Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.23) PCA for Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.24) Maximum number of PCs Quiz\n",
    "Q\\ why it is the $min(n_{samples}, n_{features})$ !! i thought it would be always the $n_{features}$ ! i need to understand that and maybe make an example where $n_{samples}<n_{features}$ so that i can see how the $n_{samples}$ will set the max num of PCs   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.25) Review/Definition of PCA\n",
    "- This is an important video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.26) Applying PCA to Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.27) PCA on the Enron Finance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.28) PCA in sklearn\n",
    "Q\\ i do not understand the writing : \"The projection step of PCA can be easiest to understand when you subtract out the mean shift of the new principal components, so the new and old dimensions have the same mean:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.29) When to Use PCA\n",
    "- Q\\ her talking about that PC made me wonder, how could it be to have a 2nd PC stronger than the 1st PC, i need to understand that b/c the demonstration so far implied that the 1st PC is at the dirction of maximum variance, and the 2nd PC is orthogonal to the 1st PC.\n",
    "    - note: in the PCA mini-Project, it is written : \"We mentioned that PCA will order the principal components, with the first PC giving the direction of maximal variance, second PC has second-largest variance, and so on. How much of the variance is explained by the first principal component? The second?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.30) PCA for Facial Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.31) Eigenfaces Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 7 : PCA Mini-Project\n",
    "- i need to answer the four questions at the end of that jupyter project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8 : Random Projection and ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.1) Random Projection\n",
    "- looking at $0.51$ then $2:11$ i conclude that a vertical \"in the table of the dataset\" column from top to bottom (which represents a single feature, but in several samples) is put in a horizontal row in the Matrix .\n",
    "    - similarly, each row in the table (represinting one point / one sample) is a column in the matrix\n",
    "    - in other words, going from table to matrix is as if we done a transpose to the table (خلى كل صف عمود و كل عمود صف)\n",
    "    \n",
    "- in the video, and also in the scikit-learn documentation, they talk about \"conservative estimation\"!! what is this ? for exapmle, at the sklearn site , the following is written \"It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.\" -> https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.2) Quiz: Random Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.3) Random Projection in sklearn\n",
    "- i've noticed that there are three types of \"fit\" functions so far in sklearn:\n",
    "    - fit\n",
    "    - fit_predict\n",
    "    - fit_transform\n",
    "- i want to summariez the difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.4) Independent Componenet Analysis (ICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.5) FastICA Algorithm\n",
    "- Q\\ what does it mean to center the dataset ? and whitening the dataset ?\n",
    "- i did not understand anything regarding the math :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.6) Quiz: ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.7) ICA in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.8) [Lab] Independent Componenet Analysis\n",
    "- it is written \"Map the values to the appropriate range for int16 audio. That range is between -32768 and +32767. A basic mapping can be done by multiplying by 32767.\"\n",
    "    - Q\\ how come that a basic mapping can be done by multiplying by 32767 ??!!!!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.9) [Solution] Independent Componenet Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.10) ICA Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 9 : Unsupervised Learning Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QUESTION 2 OF 3\n",
    "    - Note to not confuse K-nearest neighbors and K-means clustering. K-nearest neighbors is a classification algorithm, which is a subset of supervised learning. K-means is a clustering algorithm, which is a subset of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- References to check during the study\n",
    "    - post by mohamed hammad, in which he gave us three lecture that can make us sense more the way that RL learn : https://www.facebook.com/100001876777351/posts/2627523623986838\n",
    "    - a paper tweet by DeepMindAi, in which they reviews recent techniques in deep RL that narrow the gap in learning speed between humans and agents, & demonstrate an interplay between fast and slow learning w/ parallels in animal/human cognition\n",
    "        - https://twitter.com/deepmindai/status/1123979484485570566?s=19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Introduction to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is a type of machine learning where the machine or software agent learns how to maximize its performance at a task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (6 concepts)\n",
    "- 1- Introduction\n",
    "- 2- Applications\n",
    "- 3- The Setting\n",
    "- 4- OpenAI Gym\n",
    "- 5- Resources\n",
    "- 6- Reference Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.1) Introduction\n",
    "- In which, the author shows that we (humans) learn from interacting with the environment.\n",
    "    - so as we grow up, we learn about \"cause and effect\" (or how the world responds to our actions)\n",
    "        - once we know how the world works, we use our knowledge to accomplish specific goals. <br><br>\n",
    "        \n",
    "- In this section (of RL), we will take a computational approach called \"Reinforcement Learning\" to mimic the way humans learn.<br><br>\n",
    "\n",
    "- Since the world is complicated, we will simplify the world to study well defined rules and dynamics.\n",
    "    - we will then construct algorithms to teach an individual (in that simple world) to learn from interaction.\n",
    "        - we will study many of these algorithms to understand the strengths and limitations of each. <br><br>\n",
    "        \n",
    "- We will start simple and build up the complexity as we go.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.2) Applications\n",
    "- Self-driving cars, ships, and airplanes\n",
    "- Robotics (e.g walking , flying a quadcopter, etc)\n",
    "- Business, trading and finance\n",
    "- Biology\n",
    "- Telecommunications\n",
    "- Board games and online strategy games\n",
    "- And others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.3) The Setting\n",
    "- Agent: the learner, or decision maker.\n",
    "    - the agent continuously proposes and tests hypotheses (language note, \"hypotheses\" is a plural noun of hypothesis).\n",
    "    - the agent will need to deal with the dilemma of exploration vs exploitation.\n",
    "    - the agent will want to maximize its reward on the long term (not just the current moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.4) OpenAI Gym\n",
    "- It is an open source toolkit for developing and comparing RL algorithms.\n",
    "    - we will use it extensively in this section (of RL) <br>\n",
    "    \n",
    "- One of the really cool things about OpenAI Gym is that you can record your performance. So your agent might start off just behaving randomly but as it learns from interaction, you'll be able to see it choose actions in a much more intelligent way.   - What's also really cool is that if you're happy with how smart you've made your agents or how quickly they learn, you can upload your implementations to share your knowledge with the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.5) Resources\n",
    "-  Optional but encouraged reading: The most popular textbook on RL is available for free online.\n",
    "    - Reinforcement Learning - an introduction - 2nd ed- Richard S. Sutton and Andrew G. Barto <br><br>\n",
    "\n",
    "- **Whenever u face an euqation in this section (of RL), after u understand it, make sure to revist it and describe it in your own words** <br><br>\n",
    "\n",
    "- Check out this [GitHub repository](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction) to see Python implementations of most of the figures in the book.    \n",
    "----\n",
    "Before transitioning to the next lesson, you are encouraged to read Chapter 1 (especially 1.1-1.4) of the textbook to get a nice introduction to the field of reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.6) Reference Guide\n",
    "- You are encouraged to download [this sheet](https://github.com/udacity/rl-cheatsheet/blob/master/cheatsheet.pdf), which contains all of the notation and algorithms that we will use in this course. Please only use this sheet as a supplement to your own notes! :)\n",
    "- Another useful notation guide can be found in the pages immediately preceding (يسبق) Chapter 1 of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.+1) my reading\n",
    "- Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may a↵ect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—**trial-and-error** search and **delayed reward**—are the two most important distinguishing features of reinforcement learning. <br><br>\n",
    "\n",
    "- Reinforcement learning, like many topics whose names end with “ing,” such as machine learning and mountaineering, is simultaneously a (1) problem, (2) a class of solution methods that work well on the problem, and (3) the field that studies this problem and its solution methods. It is convenient to use a single name for all three things, but at the same time essential to keep the three conceptually separate. \n",
    "    - **In particular, the distinction between problems and solution methods is very important in reinforcement learning; failing to make this distinction is the source of many confusions.** <br><br>\n",
    "    \n",
    "- We formalize the problem of reinforcement learning using ideas from **dynamical systems theory**, specifically, as the optimal control of incompletely-known Markov decision processes. The details of this formalization must wait until Chapter 3, but the basic idea is simply to capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal. A learning agent must be able to **sense** the state of its environment to some extent and must be able to **take actions** that affect the state. The agent also must **have a goal or goals** relating to the state of the environment. \n",
    "- **Markov decision processes are intended to include just these three aspects—sensation, action, and goal—**in their simplest possible forms without trivializing any of them. \n",
    "- Any method that is well suited to solving such problems we consider to be a reinforcement learning method. <br><br>\n",
    "\n",
    "- look at [page 24] to see how RL is different from Suprevised and unsupervised learning... <br><br>\n",
    "\n",
    "- One of the challenges that arise in reinforcement learning, and not in other kinds of learning, is the trade-off between exploration and exploitation.\n",
    "    - The agent must try a variety of actions and progressively favor those that appear to be best. On a stochastic task, each action must be tried many times to gain a reliable estimate of its expected reward.\n",
    "- Another key feature of reinforcement learning is that it explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment. This is in contrast to many approaches that consider subproblems without addressing how they might fit into a larger picture. Although these approaches have yielded many useful results, their focus on isolated subproblems is a significant limitation. <br><br>\n",
    "\n",
    "- One of the most exciting aspects of modern reinforcement learning is its substantive and fruitful interactions with other engineering and scientific disciplines. Reinforcement learning is part of a decades-long trend within artificial intelligence and machine learning toward greater integration with statistics, optimization, and other mathematical subjects. For example, the ability of some reinforcement learning methods to learn with parameterized approximators addresses the classical “curse of dimensionality” in operations research and control theory. More distinctively, reinforcement learning has also interacted strongly with psychology and neuroscience, with substantial benefits going both ways. Of all the forms of machine learning, reinforcement learning is the closest to the kind of learning that humans and other animals do, and many of the core algorithms of reinforcement learning were originally inspired by biological learning systems. <br><br>\n",
    "\n",
    "- Finally, reinforcement learning is also part of a larger trend in artificial intelligence back toward simple general principles.\n",
    "    - Modern artificial intelligence now includes much research looking for general principles of learning, search, and decision making. It is not clear how far back the pendulum will swing, but reinforcement learning research is certainly part of the swing back toward simpler and fewer general principles of artificial intelligence. <br><br>\n",
    "\n",
    "**1.3 elements of RL** <br>\n",
    "- Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: (1) a policy, (2) a reward signal, (3) a value function, and, optionally, (4) a model of the environment.\n",
    "    - (1) **policy**\n",
    "        - defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.\n",
    "        -  In some cases the policy may be a simple function or lookup table, whereas in others it may involve extensive computation such as a search process. \n",
    "        - The policy is the core of a reinforcement learning agent in the sense that it alone is sufficient to determine behavior. \n",
    "        - In general, policies may be stochastic, specifying probabilities for each action.\n",
    "    - (2) **reward signal**\n",
    "        - defines the goal of a reinforcement learning problem. \n",
    "        - On each time step, the environment sends to the reinforcement learning agent a single number called the reward. \n",
    "        - The agent’s sole objective is to maximize the total reward it receives over the long run. The reward signal thus defines what are the good and bad events for the agent.\n",
    "        - The reward signal is the primary basis for altering the policy; if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future. \n",
    "        - In general, reward signals may be stochastic functions of the state of the environment and the actions taken.\n",
    "    - (3) **value function**\n",
    "        - Whereas the **reward signal** indicates what is good in an immediate sense, a **value function** specifies what is good in the long run. \n",
    "        - Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.\n",
    "        - Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states. \n",
    "        - For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards. Or the reverse could be true.\n",
    "        - Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward. Nevertheless, it is values with which we are most concerned when making and evaluating decisions. Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run.\n",
    "        - <u> Unfortunately, it is much harder to determine values than it is to determine rewards. Rewards are basically given directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime. In fact, the most important component of almost all reinforcement learning algorithms we consider is a method for efficiently estimating values. </u> The central role of value estimation is arguably the most important thing that has been learned about reinforcement learning over the last six decades.\n",
    "    - (4) **model of the environment**\n",
    "        - This is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave. For example, given a state and action, the model might predict the resultant next state and next reward. \n",
    "        - Models are used for **planning**, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced. \n",
    "        - Methods for solving reinforcement learning problems that use *models and planning* are called **model-based** methods, as opposed to simpler **model-free** methods that are explicitly *trial-and-error* learners—viewed as almost the **opposite** of planning. \n",
    "        - In Chapter 8 we explore reinforcement learning systems that simultaneously learn by trial and error, learn a model of the environment, and use the model for planning. Modern reinforcement learning spans the spectrum from low-level, trial-and-error learning to high-level, deliberative planning.\n",
    "        \n",
    "**1.4 Limitaion and Scope** <br>        \n",
    "- We do not address the issues of constructing, changing, or learning the state signal in this book (other than briefly in  ection 17.3). We take this approach not because we consider state representation to be unimportant, but in order to focus fully on the decision-making issues. In other words, our concern in this book is not with designing the state signal, but with deciding what action to take as a function of whatever state signal is available.\n",
    "- Most of the reinforcement learning methods we consider in this book are structured around estimating value functions, but it is not strictly necessary to do this to solve reinforcement learning problems. For example, solution methods such as genetic algorithms, genetic programming, simulated annealing, and other optimization methods never estimate value functions. These methods apply multiple static policies each interacting over an extended period of time with a separate instance of the environment. The policies that obtain the most reward, and random variations of them, are carried over to the next generation of policies, and the process repeats. We call these evolutionary methods because their operation is analogous to the way biological evolution produces organisms with skilled behavior even if they do not learn during their individual lifetimes. If the space of policies is sufficiently small, or can be structured so that good policies are\n",
    "- common or easy to find—or if a lot of time is available for the search—then evolutionary methods can be e↵ective. In addition, evolutionary methods have advantages on problems in which the learning agent cannot sense the complete state of its environment.\n",
    "- Our focus is on reinforcement learning methods that learn while interacting with the environment, which evolutionary methods do not do. Methods able to take advantage of the details of individual behavioral interactions can be much more efficient than evolutionary methods in many cases. Evolutionary methods ignore much of the useful structure of the reinforcement learning problem: they do not use the fact that the policy they are searching for is a function from states to actions; they do not notice which states an individual passes through during its lifetime, or which actions it selects. In some cases this information can be misleading (e.g., when states are misperceived), but more often it should enable more efficient search. Although evolution and learning share many features and naturally work together, we do not consider evolutionary methods by themselves to be especially well suited to reinforcement learning problems and, accordingly, we do not cover them in this book.\n",
    "\n",
    "**1.5 ** An Extended Example: Tic-Tac-Toe<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : The RL Framework: The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to mathematically formulate tasks as Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (19 concepts)\n",
    "- 1- Introduction\n",
    "- 2- The Setting, Revisited\n",
    "- 3- Episodic vs. Continuing Tasks\n",
    "- 4- Quiz: Test Your Intuition\n",
    "- 5- Quiz: Episodic or Continuing?\n",
    "- 6- The Reward Hypothesis\n",
    "- 7- Goals and Rewards, Part 1\n",
    "- 8- Goals and Rewards, Part 2\n",
    "- 9- Quiz: Goals and Rewards\n",
    "- 10- Cumulative Reward\n",
    "- 11- Discounted Return\n",
    "- 12- Quiz: Pole Balancing\n",
    "- 13- MDPs, Part 1\n",
    "- 14- MDPs, Part 2\n",
    "- 15- Quiz: One Step Dynamics, Part 1\n",
    "- 16- Quiz: One Step Dynamics, Part 2\n",
    "- 17- MDPs, Part 3\n",
    "- 18- Finite MDPs\n",
    "- 19- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.1) Introduction\n",
    "- the author : \n",
    "    - In this lesson, we'll end with a rigorous definition for the reinforcement learning problem.\n",
    "        - Specifically, you'll learn how to take a real world problem and formulate it so it can be solved through reinforcement learning.\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.2) The Setting, Revisited\n",
    "- The RL framework is characterized by an agent learned to interact with its environment. (fig.1) \n",
    "    - We assume that time evolves and discrete timesteps.\n",
    "    - At the initial timestep,the agent observes the environment. (fig.2)\n",
    "    - You can think of this observation as a situation that the environment presents to the agent.\n",
    "    - Then, it (the agent) must select an appropriate action in response. (fig.3)\n",
    "    - Then at the next timestep in response to the agents action, the environment presents a new situation to the agent. (fig.4).  At the same time the environment gives the agent a reward which provides some indication of whether the agent has responded appropriately to the environment. (fig.5)\n",
    "    - Then the process continues where at each timestep the environment sends the agent an observation and reward (fig.6). And in response, the agent must choose an action. (fig.7) <br><br>\n",
    "    \n",
    "- In general, we don't need to assume that the environment shows the agent everything he needs to make well-informed decisions. But it greatly simplifies the underlying mathematics if we do. So in this course, we'll make the assumption that the agent is able to fully observe what ever state the environment is in. And instead of referring to the agent as receiving an observation, we will hence say that it receives the environment state. (fig.8 where \"observation\" is changed to \"state\")\n",
    "But let's make this description a bit clearer with\n",
    "some added notation where we again start from the very beginning at timestep zero.\n",
    "The agent first receives the environment state which we denote by S0,\n",
    "where zero stands for a timestep zero of course.\n",
    "Then, based on that observation the agent chooses an action,\n",
    "A0, at the next timestep,\n",
    "in this case, it timestep one and that's\n",
    "a direct consequence of the agent's choice of action, A0.\n",
    "And the environments previous state, S0,\n",
    "the environment transitions to a new state, S1,\n",
    "and gives some reward,\n",
    "R1, to the agent.\n",
    "The agent then chooses an action, A1.\n",
    "At timestep two, the process continues where the environment passes the reward in state.\n",
    "Then the agent responds with an action and so on.\n",
    "Whereas the agent interacts with the environment,\n",
    "this interaction is manifest as a sequence of states, actions, and rewards.\n",
    "That said, the reward will always be the most relevant quantity to the agent.\n",
    "To be specific, any agent has the goal to maximize\n",
    "expected cumulative reward or the some of the rewards attained over all timesteps.\n",
    "In other words, it seeks to find the strategy for choosing\n",
    "actions with the cumulative reward is likely to be quite high.\n",
    "And the agent can only accomplish this by interacting with the environment.\n",
    "This is because at every timestep,\n",
    "the environment decides how much reward the agent receives.\n",
    "In other words, the agent must play by the rules of the environment.\n",
    "But through interaction, the agent can learn\n",
    "those rules and choose appropriate actions to accomplish its goal.\n",
    "And this is essentially what we'll try to accomplish in this course.\n",
    "But it's important to emphasize that all of this is\n",
    "just a mathematical model for a real world problem.\n",
    "So if you have a problem in mind that you\n",
    "think can be solved with reinforcement learning,\n",
    "you will have to specify the states, actions,\n",
    "and rewards, and you'll have to decide the rules of the environment in this course.\n",
    "You'll see a lot of examples for how to accomplish this.\n",
    "\n",
    "\n",
    "| Stretch/Untouched | ProbDistribution | dsdsd |\n",
    "| --- | --- | --- |\n",
    "| ![title](img/1.png) | ![title](img/1.png) | ![title](img/1.png) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.3) Episodic vs. Continuing Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.4) Quiz: Test Your Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.5) Quiz: Episodic or Continuing?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.6) The Reward Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.7) Goals and Rewards, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.8) Goals and Rewards, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.9) Quiz: Goals and Rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.10) Cumulative Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.11) Discounted Return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.12) Quiz: Pole Balancing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.13) MDPs, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.14) MDPs, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.15) Quiz: One Step Dynamics, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.16) Quiz: One Step Dynamics, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.17) MDPs, Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.18) Finite MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.19) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : The RL Framework: The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.2) Policies\n",
    "- At $02:36$ :\n",
    "    - I want to make it clear by differentiate between these probabalities (Stocastic Policy) and the probabilities that are on the graph. I will make an example\n",
    "        - the stocastic probabilities \"$\\pi (search|high) = 0.9 $\" and \"$\\pi (wait|high) = 0.1 $\" means that if the state is high, you have 0.9 <u>**probability to do the \"action\":search and 0.1 probability to do the (another) \"action\":wait**</u> (note that they add up to one)\n",
    "        - on the other hand, the probabilities on the graph \"say the .7 and .3 below & below-left-corner respectively\" these are the <u>**probabilities of going to a particular next state given that you took the action \"a\"**</u> (here it is search) . (note that those also add up to 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.3) Quiz: Interpret the Policy\n",
    "- From the looking at the quiz i noticed something and I also have a question:\n",
    "    - consider the notation for deterministic policy $π:S \\rightarrow A$  and stochastic policy $π:S \\times A \\rightarrow [0,1]$\n",
    "        -  note that the first formula mapps from  $S$ to $A$ , while the second one maps from $S \\times A$ to $[0,1]$ (I stress on this last mapping because I mistakenly thought that it maps from S to A via an operator x !!)\n",
    "- now here comes a question, what does $S \\times A$ mean ?     \n",
    "    - is it a cartesian product ? https://en.wikipedia.org/wiki/Cartesian_product\n",
    "        - even if it is like this, why do we do cartesian product?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.4) Gridworld Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.5) State-Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.6) Bellman Equations\n",
    "- there are lots of things in the reading that i did not understand :( and sometimes i got confused.\n",
    "- and there is a possible error at \"n the event that the agent's policy $\\pi$ is deterministic, the agent selects action $\\pi(s)$\"  i think it should be \" action a(s)\" right?\n",
    "- بس من الكويز اللى بعده أنا شفت حاجة خلتنى أشك إن <br>\n",
    "$\\pi(s)$ is an action ?!!  <br>\n",
    "ولا أنا كدا لخبطت كله فى بعضه؟\n",
    " \n",
    "- Q\\ in \"Bellman Expectation Equation\", the term $R_{t+1}$, does it refere to immediate reward or next reward ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### (6-3.7) Quiz: State-Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.8) Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.9) Action-Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.10) Quiz: Action-Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.11) Optimal Policies\n",
    "- i don't know hat exactly is the \"optimal action value function\" $q_*$ ? from what i see, all states have several action values! so where is this optimality ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.12) Quiz: Optimal Policies\n",
    "- I don't understand the following \n",
    "    - \"To see why this should be the case, note that it must hold that $v_*(s) =\\max_{a\\in\\mathcal{A}(s)} $.\" \n",
    "    - In the event that there is some state $s\\in\\mathcal{S}$ for which multiple actions $a\\in\\mathcal{A}(s)$ maximize the optimal action-value function, you can construct an optimal policy by placing any amount of probability on any of the (maximizing) actions. You need only ensure that the actions that do not maximize the action-value function (for a particular state) are given 0% probability under the policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.13) Summary\n",
    "- note that the \"Bellman expectation equation\" is a recursive equation. (i stress on that b/c for a moment i thought that it only calculates the state-value for two terms only)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.2) OpenAI Gym: FrozenLakeEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.3) Your Workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.4) Another Gridworld Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.5) An Iterative method, Part 1\n",
    "- Q\\ how this system of equation is solved while the value of $v_{\\pi}(s_1)$ is always recursive ? \n",
    "    - I think you can do it easily by substituting, for example : substitute the first equation into the second equation. (another way of substitution is illustrated in next concept \"Part 2\")\n",
    "- Q\\ what is the proof that the `iterative policy evaluation` will always converge to the true value function? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.6) An Iterative method, Part 2\n",
    "- I need you to keep close attention to the result of multiplying two summations (b/c I am so vulnerable when dealing with summation notaion because i was not introduced to it in the faculty. I may also try to do a correspondance between summations and for-loops so that i may understand summation with the aid of my understanding of for-loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.7) Quiz: An Iterative method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.8) Iterative Policy Evaluation\n",
    "- I assume that for larger problems, the iterative process is faster than solving the sys of eqs b/c the later (I assume) envolves inverse of big matrices. So I want to try and and plot the time needed for both methods to get to the answer while I increase the number of inputs. things to look for is : when the iterative method will be faster than the solving-eq method (at what number of inputs (critical number of input) )?\n",
    "    - recall that I got this intuition from the Coursera ML course when Dr.Andrew contrasted the Gradient Descent (which is an iterative method) vs the Normal equation (which is one shot solving-eqs method)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.9) Implementation\n",
    "- note: in the most inner for-loop , the line $\\Delta \\leftarrow max(\\Delta, |v-V(s)|)$ , I want to demonstrate why there is a max function between two ipnuts:\n",
    "    - in $|v-V(s)|$ we store the error of the current state\n",
    "    - but in $\\Delta$ we store the maximum error of all states in one loop. this is because we want the maximum error of all state to be less the $\\theta$ (this what breaks the outer loop, not single error of one state being less that $\\theta$, but rather, all error in all states are less than $\\theta$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.10) Mini Project: DP (Parts 0 and 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.11) Action Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.12) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.13) Mini Project: DP (Parts 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.14) Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.15) Implementatoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.16) Mini Project: DP (Parts 3)\n",
    "- note: since `policy` is initialized with zeros, the only items which are modified in `policy` will have a probability higher than zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.17) Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.18) Implementaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.19) Mini Project: DP (Parts 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.20) Truncated Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.21) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.22) Mini Project: DP (Parts 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.23) Value Iteration\n",
    "- I did not quite understand the derivations that was demonstrated in this video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.24) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.25) Mini Project: DP (Parts 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.26) Check Your Understadning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.27) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Monte Carlo Methods\n",
    "- see this post by eng. Mohamed Hammad : https://www.facebook.com/mohamed.hamedhammad/posts/2738975202841679"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.1) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.2) OpenAI Gym: BlackjackEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.3) MC Prediction: State Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.4) Implementation\n",
    "- I do not understand the `if-statement` in the algorithm! I mean, is there something wrong with \"$N(S_t) \\leftarrow N(S_t)+1$\"? I mean, how do we get the total number of the rewards that follows a state (while he only adds one when that state appears for the first time !) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.5) Mini Project: MC (Parts 0 and 1)\n",
    "- I do not fully understand the code and what it does, i think i need to make animation to visualize the `code` & `data types changes` while the code runs . (i think i must do this systematically using scripts in **After Effects**)\n",
    "- also i do not understand the plots that is generated ! ot what should i infer from it ?!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.6) MC Prediction: Action Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.7) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.8) Mini Project: MC (Parts 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.9) Generalized Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.10) MC Control: Incremental Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.11) Quiz: Incremental Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.12) MC Control: Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.13) MC Control: Policy Improvement\n",
    "- At $3:22$, for the Epsilon-Greedy Policy , I need to understand how these two cases of probabilities add up (i need to use numercial example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.14) Quiz: Epsilon-Greedy Policies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.15) Exploration vs Exploitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.16) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.17) Mini Project: MC (Parts 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.18) MC Control: Constant-alpha, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.19) MC Control: Constant-alpha, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.20) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.21) Mini Project: MC (Parts 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.22) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Temporal-Difference Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.1) Introducution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.2) OpenAI Gym: CliffWalkingEnv\n",
    "- I it a good general note for me to know the difference between coordinates in \n",
    "    - x-y pairs (in math) and \n",
    "    - row-column pairs (in programming)\n",
    "- the thing is : when i say a point at (1,2) in math , it is a point in the array in (2,1) , because when i change the row , i acually move in a vertical direction (y-direction) , and when i change the column, i actually move in the horizontal direction (x-direction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.3) TD Prediciton: TD(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.4) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.5) Mini-Project TD (Parts 0 and 1)\n",
    "- Q\\ how do we prevent the agent from taking an action that goes out of bound of the array?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.6) TD Prediction: Action Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.7) TD Control: Sarsa(0)\n",
    "- i still do not know what does \"predciont\" and \"control\" in Monte Carlo and in TD !!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.8) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.9) Mini-ProjectL TD (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.10) TD Control: Sarsamax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.11) Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.12) Mini-Project TD (Part 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.13) TD Control: Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.14) Implementaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.15) Mini-Project TD (Part 4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.16) Analyzing Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.17) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Lesson 7 : Solve OpenAI Gym's Taxi-v2 Task </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-7.1) Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-7.2) Instructions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=red> (6-7.3) Mini-Project </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8 : RL in Continuous Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.1) Deep Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.2) Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.3) Discrete vs. Continuous Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.4) Quiz: Space Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.5) Discretization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (6-8.6) Exercise: Discretization </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.7) Tile Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (6-8.8) Exercise: Tile Coding </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.9) Coarse Coding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.10) Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.11) Linear Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.12) Kernel Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.13) Non-Linear Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.14) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 9 : Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.1) Intro to Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.2) Neural Nets as Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.3) Monte Carlo Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.4) Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.5) Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.6) Deep Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.7) Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.8) Fixed Q Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.9) Deep Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.10) DQN Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.11) Implementing Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (6-9.12) TensorFlow Implementation </font>\n",
    "<font color=red> Warning: I did not download this. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.13) Wrap Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 10 : Policy-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.1) Policy-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.2) Why Policy-Based Methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.3) Policy Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.4) Stochastic Policy Search\n",
    "- First, I wanted to know the difference between stochastic and random, so using google :\n",
    "    - A variable is random. A process is stochastic. Apart from this difference, the two words are synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.5) Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.6) Monte Carlo Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.7) Constrained Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.8) Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 11 : Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.1) Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.2) A Better Score Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.3) Two Function Approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.4) The Actor and The Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.5) Advantage Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.6) Actor-Critic with Advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.7) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links to tensor flow by jay alammar https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a tweet by Mat Leonard, in which someone,in a blog, talks about why we may leave matplotlib in favor of Altair for visualization, the blog starts by saying \"Sadly, in Python, we do not have a ggplot2.\" (the visualization library in R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side effects in python !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999964\n"
     ]
    }
   ],
   "source": [
    "print(8.5-8.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see this link for explanation -> https://www.quora.com/In-Python-why-does-8-5-8-4-give-0-099999999999999964"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 403,
   "position": {
    "height": "40px",
    "left": "921px",
    "right": "20px",
    "top": "79px",
    "width": "335px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
