{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-5.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-5.5) Perceptron Algorithm\n",
    "- in Wx+b=0 or =1,2 etc (which is equivalent of making b:=b-1 or b-2 etc)\n",
    "    - <div dir=rtl> برأيى إن سبب إن بينتج خطوط متوازية من تغيير ال b لأن الميل ثابت ، حيث تذكر إن الميل بيأثر فيه المتجه W</div>\n",
    "\n",
    "### (3-5.7) Margin Error\n",
    "> لحد الآن أنا شايف إنه هو نفس الخط ، لكن لما ضرب ال دبليو و ال بى فى اتنين، فبالرغم إنه نفس الخط لكن الماردن اتغير\n",
    "    اللى لسة عاةز أشوفه ، أمال استفدنا احنا ايه لما هو نفس الخط ؟! <br>\n",
    "    اااه اعتقد لسة احنا مش بنقول هنستفاد إيه ، احنا بس بنعمل صيغة و بنوضح إزاى ممكن المارجن ندخله فى معادلة الايرور ، فأكيد هنشوف دا بعد شوية إزاى هنستخدمه ، أكيد انا منتظر إن الخط يميل وكدا ويتحرك و يتغير صح ؟ هشوف\n",
    "- المهم تلاحظ من الفيديو دا إزاى بيزيد أو يقل المارجن بإننا بنضرب معاملات المعادلة كلها فى ثابت\n",
    "\n",
    "### (3-5.8) (Optional) Margin Error Calculation \n",
    " عاوز أفهم آخر جملة قبل الرسمة 4 <br>\n",
    "بعد الرسمة 4 ، ليه اشترط إن مدام المقام هو قيمة مطلقة ، فنقدر نعمل اللى عمله... طب لو كان المقام قيمة متجهة ،، مكنش هينفع؟ ليه؟\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-6.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3-6.9) AdaBoost in sklearn\n",
    "أنا مش فاهم دلوقت هل فى خطأ فى الشرح لما قال إن <br> \n",
    "model has been fitted to a tree ??!!\n",
    "\n",
    "فبالتالى أنا عاوز أعرف هل أنا بعمل <br> \n",
    "fitting to the model first then use tha AdaBoost ? or it is demonstrated wrongly in that section? <br><br>\n",
    "\n",
    "<div dir=rtl>\n",
    "عاوز أجرب كذا base_estimator و أشوف بيعملوا إيه ،، و كمان هل ممكن أعمل ال base_estimator خوارزميات مختلفة ؟ ولا لازم نفس الخوارزيمة هى اللى بيتم استخدامها كذا مرة ؟\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.1) Announcement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.2) Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.3) Classification Problem 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.4) Classification Problem 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.5) Linear Boundries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.6) Higher Dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.7) Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.8) Perceptron as Logical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.9) Why \"Neural Networks\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.10) Perceptron Trick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.11) Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.11) Non-Linear Regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.12) Error Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.13) Log-loss Error Function \n",
    "- Nice reasoning for why we can't use \"count of error points\" as the error metric (b/c it is discrete and discrete functions are not continuous, and we really need a continuous error function (and differentiable) so that a small change in the precision will be captured as transition (change) in the output of the funtion) (but in discrete error func, we can change the function of the line , but the error is still the same). <br>\n",
    "- Note: don't confuse the error function (loss function) with the function of the line (the hypothesis)\n",
    "- <font color=red>**Q\\**</font> what is the \"loss\" in \"log-loss error\" ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.14) Discrete vs Continuous"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.15) Softmax\n",
    "- i need to know: how softmax turns into sigmoid when n=2 ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.16) One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.17) Maximum likelihood\n",
    "- my note: the reason that made the first model give low overall probability is that : the misclassified points have low probability of belonging to their right region. ex, the misclassified red point , has P(blue) =0.9 , and hence have P(red)0.1, but notice that we use the latter in calculating the whole probability of the model, i.e we use the P(red) fot the red point, and P(blue) if the point is actuallu blue. <br>\n",
    "- I might ask, what is the meaning of the red point having high P(blue)=0.9? and i will tell that this is the wrong prediction made by the model . the model assumes that this point has higher probability of being blue because it lies far in the blue region.\n",
    "- i still need a formal definition of P(all) and maximum liklihood , are they the samething or do they differ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.18) Maximizing Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.19) Cross-Entropy 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.20) Cross-Entropy 2\n",
    "In the quiz, I stayed half an hour searching for the error, and it was that i  made this line as <br>\n",
    "`for i in Y` <br>\n",
    "when it should be <br>\n",
    "`for i in range(len(Y))` <br>\n",
    "because I want `i` to be an increasing index, not each element in the `Y` list <br>\n",
    "وكدا كدا الحل دايما مش بيعمل فور لوب كدا، بل دا تفكيرك عشان انت متعود على السى ، لكن هنا اللستات بتتشقلب مع بعض عادى بدون لوب <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**why the udacity solution made the np.float_ to the two lists?** <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.828313737302301\n",
      "[0. 1. 0. 0.]\n",
      "[0.6 0.4 0.9 0.5]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "Y=[1,0,1,1] \n",
    "P=[0.4,0.6,0.1,0.5]\n",
    "\n",
    "Y_new = np.float_(Y)\n",
    "P_new = np.float_(P)\n",
    "\n",
    "CE_entropy = -np.sum(Y_new * np.log(P_new) + (1 - Y_new) * np.log(1 - P_new)) # note that np.log is the natural log (ln)\n",
    "print (CE_entropy)\n",
    "\n",
    "# 1- Y , ERROR , can't do subtraction operation between tupes : int and  list\n",
    "print (1-Y_new)\n",
    "print (1-P_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y is python list , and you cant do operation such as `1-Y` on a python list. <br>\n",
    "Y_new is a numpy ndarray (n dimensional array), which can be operated on as `1-Y_new` and the result is also ndarray. <br>\n",
    "so np.float_(Y) casts the list to an ndarray  of float elements <br>\n",
    "I might ask, why he made it `float`, not `int`? well. obviously float is the general case, and since we almost always will have fractions (like we do in the P list) so we have to cast the lists to float lists <br>\n",
    "you might look at the ndarray and its function in the numpy documentaion : https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "if you want to know why numpy use underscore after the types , here is a link, and biscally numpy does that so that these types do not clash with the same type define in python (without underscore) -> https://stackoverflow.com/questions/6205020/numpy-types-with-underscore-int-float-etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.21) Multi-class Cross Entropy\n",
    "- i need to answer the question at the end of the video : how do we say that the cross entropy formula is the same for the multi class m>2 and for m=2 while we saw that it was different form at m=2.\n",
    "- first i want to point out that the previous case it think it was m=2 class, but in the example of the three animals, it is 3*animals i.e it is 6 classes because each animal defines two classes (exists or does not exist)\n",
    "- but the problem is that the summation is from j=0 to m, which (from looking at the doors) implies that m=3 not 6 !! \n",
    "- so here comes a second question, if we have 3 animals and two doors, how will we separate the number of doors and the number of animals in the formula, i think there is a lot of ambiguity in the example given in this topic of multi-class cross Entropy\n",
    "- third question, whay is the meaning of \"Cross\" in the Cross entropy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.22) Logistic Regression\n",
    "- Note: at the end of the video, $x^i$ denotes each point in our input data samples, because remember :\n",
    "    - $Wx+b$ is the equation of the line (this defines a line to be drawn, a seperator)\n",
    "    - $Wx^i+b$ this is not an equation of a line, but rather, it is an equation where we substitute $x$ by a point: $x^i$, so making this substitution will give as a single value (a number, not a line nor a separator)\n",
    "    - also rememebr that $x^i$ is a pair of values : $(x_1,x_2)$, i.e $x^i$ is a 2x1 vecotor (or 1x2 i dont remembr) , (or nx1 for higher dimensions.  also it could be 1xn, i will revise and see)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.22) Gradient Descent\n",
    "- <font color=red>**Q\\**</font> in calculating $\\frac{\\partial }{\\partial w_j}E$ , why did he considered $y$ to be constant w.r.t $w_j$ (i.e $\\frac{\\partial }{\\partial w_j}y=y$)? I ask this because as far as i know , $y=Wx+b$, so $y$ is **not** constant w.r.t $w_j$, right ? <br>\n",
    "- <font color=blue> **No my dear friend** </font> , I knew later that $y$ is the label ,(e.g in a binary class classification, y is 0 or 1), so it is a consntat , but this raises another question, if $\\hat{y}=Wx^i+b$ , so <font color=red>**Q\\**</font> what is the variable assgined to (or used to name) the hypothesis : (var? $= Wx+b$) ? <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.23) Logistic Regresion Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.24) Pre-Lab: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.25) Notebook: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.26) Perceptron vs Gradient Descent\n",
    "it is importnat to note the values that can be taken by $\\hat{y}$ I both GD and Percptron algo.. note in which it is it continuous and in which it is discrete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-1.27) Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**<center> Example of code writing equations here, not in inline mode, so that all letters appear well (inline makes them very small)</center>** \n",
    "\n",
    "\\begin{equation*}\n",
    "P(E)   = {n \\choose k} p^k (1-p)^{ n-k}\n",
    "\\end{equation*}\n",
    "\n",
    "**<center> or make a boxed equation (i can use it for final results or for hint to recall previous equations say for derivative of $ln$, etc)</center>** \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\boxed{P(E)   = {n \\choose k} p^k (1-p)^{ n-k}}\n",
    "\\end{equation*}\n",
    "\n",
    "**<center> what i want to do is : before any mathematical derivation, i list the mathimatical rules that i am gonna use, so that i remember it easily . i may preceed these equation by the ward \"remember:\"</center>**  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i want to say that cases are great $ \\begin{cases} one \\\\ two \\\\ three \\end{cases} $ i also can complete here  <br> and can complete down there ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : Cloud Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.1) Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.2) Create an AWS Account"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.3) Get Access to GPU Instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.4) Launch an Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-2.5) Login to the Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : Deep Neural Networks\n",
    "- References :\n",
    "    - A post shared by ibrahim sobh: a recipe for training nn : \n",
    "        - http://karpathy.github.io/2019/04/25/recipe/\n",
    "    - 37 reasons why your neural network is not working (other than overfitting) , i think he talks about the implementation itself\n",
    "        - medium link https://medium.com/m/global-identity?redirectUrl=https%3A%2F%2Fblog.slavv.com%2F37-reasons-why-your-neural-network-is-not-working-4020854bd607%3Ffbclid%3DIwAR0FtIi4E6FSyn2anmsCcmnrzCHt9OLSlciXm4O9YT4WGVohiQnFO2X9M0Q\n",
    "        - other link if the above did not work https://weekly-geekly.github.io/articles/334944/index.html\n",
    "    - deep mind tweet: For neural networks to be deployed in the real world, we need guarantees that our network satisfy desired specifications. We introduce a method to verify complex non-linear specifications. :https://twitter.com/DeepMindAI/status/1125503318749581312\n",
    "        - link of the paper : https://arxiv.org/pdf/1902.09592.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.1) Non-linear Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.2) Continuous Perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.3) Non-linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.4) Neural Networks Architecture\n",
    "- <font color=red>**Q\\**</font> when combining two nns, why take the sigmoid of the addition of the two points? i know that he said that we want the scale to be between 0 and 1, but we could've used averaging and it would achieve the same goal!! ... i may try both and see the resutling boundries (i should know how to plot these things!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.5) Feedforward\n",
    "- at video 1 : feedforward\n",
    "    - <font color=red>**Q\\**</font> at $0:41$, how did he know that $w_1$ is bigger than $w_2$ ?! is this implied from the drwaing ? i want to know this !!\n",
    "    - <font color=red>**Q\\**</font> at $3:34$, what is that strange circle between matrices ? is this a multiplication operatot or what ?!\n",
    "    - <font color=red>**Q\\**</font> what does `Dense` mean ?\n",
    "    - <font color=red>**Q\\**</font> what if i made a mistake (or changed my mind) and wanter to change something in the middle of the path of the network (e.g an activation function)?\n",
    "    - <font color=red>**Q\\**</font> it is written \"We can see that the output has dimension 1.\" , where can i see that ???\n",
    "    - <font color=red>**Q\\**</font> in the quiz, \n",
    "        - when he starts with a random seed to the numpy package, what does this line affect afterword?\n",
    "        - when he did \"One-hot encoding the output\", what was the shape of y before and after this step? (this will tell me why did he do this in the first place! i ask this because i see that the output is not categorical, so why did he do that ! )\n",
    "        - i skipped this quiz because there are lots of things that i do not understand. (كلفتونى أوى فى الكيراس!!) i will return and do it later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.6) Backpropagation\n",
    "- **Recommendation from eng.mohamed hammad** look at : \n",
    "    - https://github.com/DebPanigrahi/Machine-Learning/blob/master/back_prop.pdf\n",
    "    - https://www.facebook.com/mohamed.hamedhammad/posts/2503343279738207\n",
    "    - https://www.facebook.com/mohamed.hamedhammad/posts/2538529736219561"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.7) Keras\n",
    "- <font color=red>**Q\\**</font> why he writes `dtype=np.float32` in defining X and y ??\n",
    "- <font color=red>**Q\\**</font> why `X.shape[1]` what does it mean, and why not `X.shape[0]`?? i need to get the code here and see each output of these things and i shall demonstrate each line myself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.8) Pre-Lab: Student Admission in Keras</font>\n",
    "- what does dense do ?\n",
    "- why did he put 32 as the first input to the neuron ?\n",
    "- how do we choose the number of hidden layers and the number of neurons in each layer ?\n",
    "- i want to see the effect of adding/changing each neuron/layer/activation function. <br><br>\n",
    "\n",
    "- <font color=blue> **answer** </font> the last two questions are part of the DNN hyperparamater tuning, do the changes in lab \"(4-4.8) Mini project: Training an MLP on MNIST\n",
    "\" to get the intuition, and see this link to know how to tune these parameters -> https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.9) Lab: Student Admission in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.10) Training Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.11) Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.12) Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.13) Regularization 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.14) Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-315) Local Minima\n",
    "- <font color=red>**Q\\**</font> what is the solution ?!!\n",
    "    - <font color=blue> the answer is at (4-3.20) Random Restart and (4-3.21) Momentum</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.16) Vanishing Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.17) Other Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.18) Batch vs Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.19) Learning Rate Decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.20) Random Restart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.21) Momentum\n",
    "- <font color=red>**Q\\**</font> I don't understand why the momentum idea behaves in such manner! i mean, what is so special about the global minimum that makes the algorithm converge to it ? what if the global minimum was not so deep (surrounded by low humps that the algorithm may also go over it !)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.22) Optimizers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.23) Error Functions Around the World"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.24) Neural Network Regression\n",
    "- at $1:30$, he says \"the way we turn a linear function into a ReLU is by turning off the parts of the negative values (or underneath the x-axis into zero)\"\n",
    "    - I think by underneath x-axis he means (to the left of the y-axis, i.e the negative part of the x-axis)\n",
    "- <font color=red>**Q\\**</font> why we bother and use ReLU ? why not just use a linear function ? (what is the advantgae of ReLU over the linear function)    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.25) Neural Networks Playground\n",
    "How did Jay alammar made these beautiful interaction!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-3.26) Mini Project Intro </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-3.27) Pre-Lab: IMDB Data in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-3.28) Lab: IMDB Data in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-3.29) Outro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "read this tweet from ian goodfellow about OctConv which is a simple replacement for the traditional convolution operation that gets better accuracy with fewer FLOPs <br>\n",
    "https://twitter.com/goodfellow_ian/status/1117929612200120320?s=19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.1) Introducing Alexis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.2) Applications of CNNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.3) How Computers Interprets Images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.4) MLPs for Image Classification\n",
    "Q\\ what is the input shape in `model.add(Flatten(input_shape=X_train.shape[1:]))` ? i mean whay is [1:] does ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.5) Categorical Cross-Entropy\n",
    "- Q\\ `accuracy = 100*score[1]` what is returnd into the score var, so that we take its 2nd element  ?\n",
    "    - for the line `score = model.evaluate(X_test, y_test, verbose=0)` the documentation says: evaluate *\"Returns (1)the loss value & (2)metrics values for the model\"* \n",
    "    - this makes sense, but i still want to know what is the *\"the loss value\" is it the error value ? how does it compare with the metric value ? why would i need both numbers?*\n",
    "        - answer, in an upcoming video, i knew that : Overfitting is detected by comparing the validation loss to the training loss. If the training loss is much lower than the validation loss, then the model might be overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.6) Model Validation in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.7) When do MLPs (not) work well?\n",
    "- unitl perior of here, all video were about MLPs (Deep nn).\n",
    "- here the video started to introduce CNN and compare it with DNN (MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-4.8) Mini project: Training an MLP on MNIST </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.9) Local Connectivity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.10) Convolution Layers (Part 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.11) Convolution Layers (Part 2) \n",
    "- Q\\ why does she use ReLU activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.12) Stride and Padding  \n",
    "(stride = |noun| a long, decisive step. |verb|walk with long, decisive steps in a specified direction.  ) <br>\n",
    "(Padding = حشوة أو حشو)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.13) Convolutional Layers in Keras\n",
    "- Q\\ in the reading it says \"You are **strongly encouraged** to add a ReLU activation function to **every** convolutional layer in your networks.\" why is that ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.14) Quiz: Dimensionality\n",
    "- I will demonstrate the equation of the width of the conv layer **in the case of `padding = 'valid'`**\n",
    "\\begin{equation*}\n",
    "width = ceil(float( \\frac{W_{in} - F + 1}{float(S)}))\n",
    "\\end{equation*}\n",
    "- for the nomenator `W_in - F + 1` , i was able to think about it like this : \n",
    "    - `W_in - (F - 1)` which says: if i have a width W_in , and a filter with with width F, how many steps are required for the filter to slide over the W_in (assuming that stride (طول الخطوة) is 1)? the answer is `W_{in} - (F - 1)` , and for the `-1` you can understand it by thinking of some examples :\n",
    "        - if W_in =6 , and F=3  , it will require the filter to do 4 steps to reach the end of the image (try to do it manually to see it)\n",
    "        - also think of W_in=6 and F=2, it will take you 5 steps\n",
    "    - now the previous assumed that the stride is 1, if the stride (طول الخطوة) was S, simply devide on it to get the number of steps using this new stride. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.15) Pooling Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.16) Max Pooling Layers in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.17) CNNs for Image Classification\n",
    "- at $1:16$, why she said \"where the spatial dimension is a **power of 2** or else a number that is **divisible by a large power of 2**\" . why exactly is these two constrains ?:\n",
    "    - **power of 2**\n",
    "    - **divisible by a large power of 2** (also why **large** power of 2 , not any power ?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.18) CNNs in Keras: Practical Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-4.19) Mini project: CNNs in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.20) Image Augmentation in Keras\n",
    "- Q\\ when the reading says : \"where x_train.shape[0] corresponds to the number of unique samples in the training dataset x_train.\" , i think by unique means \"not including the augmentation\" right ?\n",
    "- i don't understand the reading that says \"By setting steps_per_epoch to this value, we ensure that the model sees x_train.shape[0] augmented images in each epoch.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (4-4.21)  Mini project: Image Augmentation in Keras </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.22) Groundbreaking CNN Architechture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.23) Visualizing CNNs (Part 1)\n",
    "- Q\\ at $1:06$, i do not whay does \"take a filter from a cnn, then construct images that maximize the activation of that filter \" mean ? \n",
    "    - answer found in this link , under the title \"Retrieving images that maximally activate a neuron\" -> http://cs231n.github.io/understanding-cnn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.24) Visualizing CNNs (Part 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.25) Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-4.26) Transfer Learning in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Deep Learning for Cancer Detection with Sebastian Thrun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.2) Skin Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.3) Survival Probability of Skin Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.4) Medical Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.5) The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.6) Image Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.7) Quiz: Data Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.8) Solution: Data Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.9) Training the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.10) Quiz: Random vs Pre-initialized Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.11) Solution: Random vs Pre-initialized Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.12) Validation the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.13) Quiz: Sensitivity and Specifity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.14) Solution: Sensitivity and Specifity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.15) More on Sensitivity and Specifity "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.16) Quiz: Diagnosing Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.17) Solution: Diagnosing Cancer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.18) Refresh on ROC curves\n",
    "عجبنى جدا الأنيميشن اللى فى فى آخر فيديو فى الصفحة ، برأيى هعمل زيه و أكتب (فى الريبو بتاع الإنتويشن) الاستفادة من الأنيميشن دا إيه أو الرسمة يعنى دى ناخد منها (نبنى عليها) قرارات إزاى ،،، و هشير الفيديو دا فى لنكد إن وفى تويتر و فى جروب الفيس بتاع الذكاء الاصطناعى"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.19) Quiz: ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.20) Solution: ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.21) Comparing our Results with Doctors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.22) Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.23) What is the network looking at?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.24) Refresh on Confusing Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.25) Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.26) Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.27) Useful Resoucres "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.28) Mini Project Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(4-5.29) Mini Project: Dermatologist AI </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4-5.30) Share your Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Deep Learning Assesment\n",
    "QUESTION 2 OF 5: <br>\n",
    "my asnwer is calculating : $3*2 + 3*1$\n",
    "\n",
    "QUESTION 3 OF 5: <br>\n",
    "I need to understand why!\n",
    "\n",
    "QUESTION 4 OF 5: <br>\n",
    "what is the kernel ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Unsupervised Learning \n",
    "### introduction and sub-summary\n",
    "in this unit we have 9 lessons: <br>\n",
    "- lesson 9: we learn two concepts\n",
    "    - Random Projection : \n",
    "        - same as PCA but the line is chosen randomly.\n",
    "        - it is faster than PCA\n",
    "        - we can specify epslon or n_compeonets\n",
    "    - ICA : (Independit component analysis)\n",
    "        - statistal method applied when we think that features are statistically independet\n",
    "        - common use cases \n",
    "            - in seprating voice in the cocktail party problem\n",
    "            - in seperating signals in EEG\n",
    "            - some finance application but it is rarely useful to use ICA in finance since financial systems are very complex.\n",
    "        - it has a mathematical derivation, but the most importnat thing that you have to know is the assumptions that is considered to make this algo work, they are :\n",
    "            - a\n",
    "            - a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Clustreing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-.6) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.9) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.11) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.13) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.14) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.15) \n",
    "note that the K-means algorithm is a hill climbing algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.16) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-1.17) \n",
    "important rule of thumps : the more classes u have, the more local minimum will exists, the more you need to run K-means algorithm to reach to the global minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : Clustering Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.2) k-means Clustering of Movie Ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-2.3) Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : Hierarchical Density-based Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.1) K-means considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.2) Overview of other clustering methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.3) Hierarchical clustering: single-link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.4) Examining single-link clustering\n",
    "- i was wondering how the coloring of clusters is done in this algotithm (the final result) (b/c the dendrogram goes all the way to 1 cluster!)? \n",
    "    - and i think the answer is that the algortithm stops depending on a threshold that is set to tell us that two cluster should not be merged. I think this is mapped to the dendrogram when we see a long vertical distance before two clusters are merged (as Jay said in the video about the long vertical distance in the dendrogram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.5) complete-link, average-link, Ward\n",
    "- I don't understand how subtracting $A_1^2, A_2^2, B_1^2, B_2^2$ reduces variance ! what is the variance anyway in the context of clustering? (i want to visualize clusters with high variance and with low variance to compare the two.)\n",
    "- Q\\ $A_1 = A_2$ and $B_1 = B_2$ right?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.6) Hierarchical clustering implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.7) [Lab] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.8) [Lab Solution] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.9) HC examples and applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.10) [Quiz] Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.11) DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.12) DBSCAN implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.13) [Lab] DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.14) [Lab Solution] DBSCAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.15) DBSCAN examples & applications\n",
    "- it is very important to me to know that DBSCAN is used as anomaly detection (detecting outliers in a dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-3.16) [Quiz] DBSCAN\n",
    "- Question 2 of 2 : imporntant to note that DBSCAN is not a hierarchical clustering method, but is a density-based clustering method that is robust against noisy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Gaussian Mixture models and Cluster Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.1) Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.2) Gaussian Mixture Model (GMM) Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.3) Gaussian Distribution in One Dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.4) GMM Clustering in One Dimension\n",
    "Q\\ how did he know that the physics test was hard by looking at its distribution of scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.5) Gaussian Disrtibution in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.6) GMM in 2D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.7) Quiz: Gaussian Mixtures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.8) Overview of The Expectation Maximization (EM) Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.9) Expectation Maximization Part 1\n",
    "- I want to study the things in statistics and probability that was mentioned in this video\n",
    "- i want to define soft clustering, is it assigning probabilities to points of membership to the available clusters? i will search for the proper definition "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.10) Expectation Maximization Part 2\n",
    "- in the weighted average calculation, i was surprised that we did not devide by the number of elements, but we devided by the sum of the probabilities (since all probabilities are less than 1, so this sum is sure less than the number of points).\n",
    "    - 1- i want to know why\n",
    "    - 2- i want to compare this with if i had devided by the number of points, and i want to visualize both and get an intuition about the difference between the right method and my wrong assumption.\n",
    "- i want to know what is the log-likelihood, an what is mixing coefficient (note: they are in statistics)\n",
    "- Q\\ what is the \"1n\" that is in the video that precedes the equation of evaluating the log-likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.11) Visual Example of EM Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.12) Quiz: Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.13) GMM Implementation\n",
    "- Q\\ i want to something about the content of the array that contains the predicted labels, what if a point is inside two clusters (i.e when the two clusters overlap as the case in the previous video), to which cluster will it be assigned ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**note:** on the difference between probability and membership\n",
    "- The term \"membership\" got me thinking if there is a difference between the two terms : membership and probability. \n",
    "    - first i assumed that i will say \"probabiliy of X\" when X is an event in the future that may have some kind of probability to happen, while i will say \"the membership of X\" when i talk/analyse something that already exists, such as saying that the movie (which is something already exists) is a member of (or belonging to) the genre action (by 80%) and romance (by 20%).\n",
    "    - i also assumed that probability and membership have the same mathematical tools, but we say probabilty of membership depending on the thing that we study\n",
    "- my assumptions were not so bad, as i found out that there are two fields that are related alot : probability and fuzzy logic (the latter has the membership function). and with little search i found that there are differences. look at the following references : <br>\n",
    "https://goodmath.scientopia.org/2011/02/02/fuzzy-logic-vs-probability/ <br>\n",
    "https://www.researchgate.net/post/Difference_between_fuzzy_logic_and_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.14) GMM Examples & Applications\n",
    "- I am wondering if i can mix up GMM with CNN and come up with useful things."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.15) Cluster Analysis Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.16) Cluster Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.17) External Validation indices\n",
    "- i want to compare the `external validation idnex` to calculating the `accuracy` of of the clustering (the ratio of the points that was correctly clustered). \n",
    "    - Q\\ why we needed the `external validation idnex` when we could just use the `accuracy`?\n",
    "        - i think the answer lies in the objective we need when we asses the clustering, it is not just how many points are correctly clustered, but rather there were two objectives that was mentioned in the previous video that assess the clustering, namely: being compact and separated.\n",
    "    -Q\\ what does it mean for `ARI` to be equal to `0` ? what about `-1` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.18) Quiz: Adjusted Rand index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.19) Internal Validation Indices\n",
    "- Q\\ why the formula devides by $max(a_i,b_i)$ , i mean 1- why we devide? 2- why we use the max (what is the benefit of using the maximum?)!? \n",
    "    - is it because this normailize the output of this index to be between `-1` and `1` ? (i still need to see when it becomes `-1` and when it becomes `1` and when it becomes `0` or `-0.5` etc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.20) Silhouette Coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.21) GMM & Cluster Validation Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-4.22) GMM & Cluster Validation Lab Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Feature Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.1) Chris's T-shirt Size (intuition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.2) A Metric for Chris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.3) Height + Weight for Cameron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.4) Sarah's Height + Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.5) Chris 's Shirt Size by Our Metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.6) Comparing Features with Scales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.7) Feature Scaling Formula Quiz 1\n",
    "- For the formula of feature scaling, I, Ahmed, want to demonstrated this formula.\n",
    "- so the formula is as follows:\n",
    "\\begin{equation*}\n",
    "x' = \\frac{x-x_{min}}{x_{max}-x_{min}}\n",
    "\\end{equation*}\n",
    "- take for example the array [10 , 15 , 20]\n",
    "- the numerator works on shifting the array so that the minimum number becomes zero, so when we do $x-x_{min}$ the resulting array is [0 , 5 , 10]\n",
    "- and the denominator works on scaling the result array by deviding on its range, the range would be max of the resulting new array (=10) or it could be calculated from the old array as $x_{max}-x_{min}$,\n",
    "- so the final result becomes [0 ,  0.5 ,  1]\n",
    "- note : \n",
    "    - $x$ represents each element in the old array (the old one),  $x_{min}$ and $x_{max}$ are single elements in the array (the old one)\n",
    "    - $x'$ represents each new element in the final result (i.e each rescaled element)\n",
    "- when I demonstrate this concept to students, i may say that different range of values introduce a problem in our ML algorithm, since our algorithms usually use \"addition\" [i need to give example here], and we already know (in the course of numercal  analysis for example) that when we add large number with small number, the small number contribute to the result in an non-sensible way (we actuall sometimes ignore the small term in the addition when it is added to a large number)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.8) Feature Scaling Formula Quiz 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.9) Feature Scaling Formula Quiz 3\n",
    "- it is important to note the advantage and disadvante of rescaling to the range of [0 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red>(5-5.10) Min/Max Rescaling Codeing Quiz</font>\n",
    "- a very important note was given in that quiz which is, the straight forward solution will be prone to divide-by-zero error.\n",
    "    - i should always think of this error whenever i implement anything that has devision operation in it\n",
    "- look here to see several ways to make an operation (such as subtracting a number) on every element on a python list : \n",
    "    - https://stackoverflow.com/questions/4918425/subtract-a-value-from-every-number-in-a-list-in-python\n",
    "    - https://docs.python.org/3/tutorial/datastructures.html#list-comprehensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.11) Min/Max Scaler in sklearn\n",
    "- i did not understand the part when she talked about the content of the numpy array! (she said things about each training point and each feature, and i did not understand that)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-5.12) Quiz on Algorithms Requiring Rescaling\n",
    "- important video!! and i want analyse (maybe search online for fast answers) the other ML algorithms i took so far to see which ones are affected by features ranges (so we have to do feature scaling with them) and which ones do not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.1) Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.2) Trickier Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.3) One-Dimensional, or Two?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.4) Slightly Less Perfect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.5) Trickiest Data Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.6) PCA for Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.7) Center of a New Coordinate System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.8) Principal Axis of New Coordinate System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.9) Second Principal Component of New System\n",
    "- Q\\ i did not understand how did he made the $-1$ !!? i do not see how should i look at the new components to conclude the $\\Delta y$ and $\\Delta x$\n",
    "    - answer, assume that the vectors are on the origin point of the old coordinates, and try to find out the values of the new vectors that represent the new coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.10) Practice Finding Centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.11) Pactice Finding New Axes\n",
    "- after finding the chagne for the new x vector, getting the new y vector may be hard at first, but i know two ways to get it :\n",
    "    - recall that to get the slope of perpendicular  - > (اقلب الميل و غير إشارته)\n",
    "    - i can also draw an imaginary triangle of the change in x, and then rotate this triangle by +90 degrees and hence i will get the change in y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.12) Which Data is Ready for PCA\n",
    "- Q\\ for the third data set :\n",
    "    - why he said it is impossible to do regression for this dataset (why he said it is impossible to build a regression that goes vertically?) ?! (note he said \"remember\" so it might be mentioned in the older version of the course, i may see it, but is suggest to search for the answer online to not waste my time searching for small piece of info in tens of vides)\n",
    "    - why the PCA is not as follows : the x' is to the bottom and y' is to the right ?\n",
    "    \n",
    "- note : he said that the circualr dataset will always give a deterministic result of PCA, i need to know why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.13) When Does an Axis Dominate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.14) Measurable vs. Latent Features Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.15) From Four Features to Two"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.16) Compression While Preserving Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.17) Composote Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.18) Maximal Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.19) Advantages of Maximal Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.20) Maximal Variance and Information Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.21) Info Loss and Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.22) Neighborhood Composite Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.23) PCA for Feature Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.24) Maximum number of PCs Quiz\n",
    "Q\\ why it is the $min(n_{samples}, n_{features})$ !! i thought it would be always the $n_{features}$ ! i need to understand that and maybe make an example where $n_{samples}<n_{features}$ so that i can see how the $n_{samples}$ will set the max num of PCs   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.25) Review/Definition of PCA\n",
    "- This is an important video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.26) Applying PCA to Real Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.27) PCA on the Enron Finance Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.28) PCA in sklearn\n",
    "Q\\ i do not understand the writing : \"The projection step of PCA can be easiest to understand when you subtract out the mean shift of the new principal components, so the new and old dimensions have the same mean:\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.29) When to Use PCA\n",
    "- Q\\ her talking about that PC made me wonder, how could it be to have a 2nd PC stronger than the 1st PC, i need to understand that b/c the demonstration so far implied that the 1st PC is at the dirction of maximum variance, and the 2nd PC is orthogonal to the 1st PC.\n",
    "    - note: in the PCA mini-Project, it is written : \"We mentioned that PCA will order the principal components, with the first PC giving the direction of maximal variance, second PC has second-largest variance, and so on. How much of the variance is explained by the first principal component? The second?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.30) PCA for Facial Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-6.31) Eigenfaces Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 7 : PCA Mini-Project\n",
    "- i need to answer the four questions at the end of that jupyter project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8 : Random Projection and ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.1) Random Projection\n",
    "- looking at $0.51$ then $2:11$ i conclude that a vertical \"in the table of the dataset\" column from top to bottom (which represents a single feature, but in several samples) is put in a horizontal row in the Matrix .\n",
    "    - similarly, each row in the table (represinting one point / one sample) is a column in the matrix\n",
    "    - in other words, going from table to matrix is as if we done a transpose to the table (خلى كل صف عمود و كل عمود صف)\n",
    "    \n",
    "- in the video, and also in the scikit-learn documentation, they talk about \"conservative estimation\"!! what is this ? for exapmle, at the sklearn site , the following is written \"It should be noted that Johnson-Lindenstrauss lemma can yield very conservative estimated of the required number of components as it makes no assumption on the structure of the dataset.\" -> https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.2) Quiz: Random Projection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.3) Random Projection in sklearn\n",
    "- i've noticed that there are three types of \"fit\" functions so far in sklearn:\n",
    "    - fit\n",
    "    - fit_predict\n",
    "    - fit_transform\n",
    "- i want to summariez the difference between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.4) Independent Componenet Analysis (ICA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.5) FastICA Algorithm\n",
    "- Q\\ what does it mean to center the dataset ? and whitening the dataset ?\n",
    "- i did not understand anything regarding the math :("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.6) Quiz: ICA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.7) ICA in sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.8) [Lab] Independent Componenet Analysis\n",
    "- it is written \"Map the values to the appropriate range for int16 audio. That range is between -32768 and +32767. A basic mapping can be done by multiplying by 32767.\"\n",
    "    - Q\\ how come that a basic mapping can be done by multiplying by 32767 ??!!!!?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.9) [Solution] Independent Componenet Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5-8.10) ICA Applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 9 : Unsupervised Learning Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- QUESTION 2 OF 3\n",
    "    - Note to not confuse K-nearest neighbors and K-means clustering. K-nearest neighbors is a classification algorithm, which is a subset of supervised learning. K-means is a clustering algorithm, which is a subset of unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- References to check during the study\n",
    "    - post by mohamed hammad, in which he gave us three lecture that can make us sense more the way that RL learn : https://www.facebook.com/100001876777351/posts/2627523623986838\n",
    "    - a paper tweet by DeepMindAi, in which they reviews recent techniques in deep RL that narrow the gap in learning speed between humans and agents, & demonstrate an interplay between fast and slow learning w/ parallels in animal/human cognition\n",
    "        - https://twitter.com/deepmindai/status/1123979484485570566?s=19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Introduction to RL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is a type of machine learning where the machine or software agent learns how to maximize its performance at a task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (6 concepts)\n",
    "- 1- Introduction\n",
    "- 2- Applications\n",
    "- 3- The Setting\n",
    "- 4- OpenAI Gym\n",
    "- 5- Resources\n",
    "- 6- Reference Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.1) Introduction\n",
    "- In which, the author shows that we (humans) learn from interacting with the environment.\n",
    "    - so as we grow up, we learn about \"cause and effect\" (or how the world responds to our actions)\n",
    "        - once we know how the world works, we use our knowledge to accomplish specific goals. <br><br>\n",
    "        \n",
    "- In this section (of RL), we will take a computational approach called \"Reinforcement Learning\" to mimic the way humans learn.<br><br>\n",
    "\n",
    "- Since the world is complicated, we will simplify the world to study well defined rules and dynamics.\n",
    "    - we will then construct algorithms to teach an individual (in that simple world) to learn from interaction.\n",
    "        - we will study many of these algorithms to understand the strengths and limitations of each. <br><br>\n",
    "        \n",
    "- We will start simple and build up the complexity as we go.        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.2) Applications\n",
    "- Self-driving cars, ships, and airplanes\n",
    "- Robotics (e.g walking , flying a quadcopter, etc)\n",
    "- Business, trading and finance\n",
    "- Biology\n",
    "- Telecommunications\n",
    "- Board games and online strategy games\n",
    "- And others"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.3) The Setting\n",
    "- Agent: the learner, or decision maker.\n",
    "    - the agent continuously proposes and tests hypotheses (language note, \"hypotheses\" is a plural noun of hypothesis).\n",
    "    - the agent will need to deal with the dilemma of exploration vs exploitation.\n",
    "    - the agent will want to maximize its reward on the long term (not just the current moment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.4) OpenAI Gym\n",
    "- It is an open source toolkit for developing and comparing RL algorithms.\n",
    "    - we will use it extensively in this section (of RL) <br>\n",
    "    \n",
    "- One of the really cool things about OpenAI Gym is that you can record your performance. So your agent might start off just behaving randomly but as it learns from interaction, you'll be able to see it choose actions in a much more intelligent way.   - What's also really cool is that if you're happy with how smart you've made your agents or how quickly they learn, you can upload your implementations to share your knowledge with the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.5) Resources\n",
    "-  Optional but encouraged reading: The most popular textbook on RL is available for free online.\n",
    "    - Reinforcement Learning - an introduction - 2nd ed- Richard S. Sutton and Andrew G. Barto <br><br>\n",
    "\n",
    "- **Whenever u face an euqation in this section (of RL), after u understand it, make sure to revist it and describe it in your own words** <br><br>\n",
    "\n",
    "- Check out this [GitHub repository](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction) to see Python implementations of most of the figures in the book.    \n",
    "----\n",
    "Before transitioning to the next lesson, you are encouraged to read Chapter 1 (especially 1.1-1.4) of the textbook to get a nice introduction to the field of reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.6) Reference Guide\n",
    "- You are encouraged to download [this sheet](https://github.com/udacity/rl-cheatsheet/blob/master/cheatsheet.pdf), which contains all of the notation and algorithms that we will use in this course. Please only use this sheet as a supplement to your own notes! :)\n",
    "- Another useful notation guide can be found in the pages immediately preceding (يسبق) Chapter 1 of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-1.+1) my reading\n",
    "- Reinforcement learning is learning what to do—how to map situations to actions—so as to maximize a numerical reward signal. The learner is not told which actions to take, but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may a↵ect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics—**trial-and-error** search and **delayed reward**—are the two most important distinguishing features of reinforcement learning. <br><br>\n",
    "\n",
    "- Reinforcement learning, like many topics whose names end with “ing,” such as machine learning and mountaineering, is simultaneously a (1) problem, (2) a class of solution methods that work well on the problem, and (3) the field that studies this problem and its solution methods. It is convenient to use a single name for all three things, but at the same time essential to keep the three conceptually separate. \n",
    "    - **In particular, the distinction between problems and solution methods is very important in reinforcement learning; failing to make this distinction is the source of many confusions.** <br><br>\n",
    "    \n",
    "- We formalize the problem of reinforcement learning using ideas from **dynamical systems theory**, specifically, as the optimal control of incompletely-known Markov decision processes. The details of this formalization must wait until Chapter 3, but the basic idea is simply to capture the most important aspects of the real problem facing a learning agent interacting over time with its environment to achieve a goal. A learning agent must be able to **sense** the state of its environment to some extent and must be able to **take actions** that affect the state. The agent also must **have a goal or goals** relating to the state of the environment. \n",
    "- **Markov decision processes are intended to include just these three aspects—sensation, action, and goal—**in their simplest possible forms without trivializing any of them. \n",
    "- Any method that is well suited to solving such problems we consider to be a reinforcement learning method. <br><br>\n",
    "\n",
    "- look at [page 24] to see how RL is different from Suprevised and unsupervised learning... <br><br>\n",
    "\n",
    "- One of the challenges that arise in reinforcement learning, and not in other kinds of learning, is the trade-off between exploration and exploitation.\n",
    "    - The agent must try a variety of actions and progressively favor those that appear to be best. On a stochastic task, each action must be tried many times to gain a reliable estimate of its expected reward.\n",
    "- Another key feature of reinforcement learning is that it explicitly considers the whole problem of a goal-directed agent interacting with an uncertain environment. This is in contrast to many approaches that consider subproblems without addressing how they might fit into a larger picture. Although these approaches have yielded many useful results, their focus on isolated subproblems is a significant limitation. <br><br>\n",
    "\n",
    "- One of the most exciting aspects of modern reinforcement learning is its substantive and fruitful interactions with other engineering and scientific disciplines. Reinforcement learning is part of a decades-long trend within artificial intelligence and machine learning toward greater integration with statistics, optimization, and other mathematical subjects. For example, the ability of some reinforcement learning methods to learn with parameterized approximators addresses the classical “curse of dimensionality” in operations research and control theory. More distinctively, reinforcement learning has also interacted strongly with psychology and neuroscience, with substantial benefits going both ways. Of all the forms of machine learning, reinforcement learning is the closest to the kind of learning that humans and other animals do, and many of the core algorithms of reinforcement learning were originally inspired by biological learning systems. <br><br>\n",
    "\n",
    "- Finally, reinforcement learning is also part of a larger trend in artificial intelligence back toward simple general principles.\n",
    "    - Modern artificial intelligence now includes much research looking for general principles of learning, search, and decision making. It is not clear how far back the pendulum will swing, but reinforcement learning research is certainly part of the swing back toward simpler and fewer general principles of artificial intelligence. <br><br>\n",
    "\n",
    "**1.3 elements of RL** <br>\n",
    "- Beyond the agent and the environment, one can identify four main subelements of a reinforcement learning system: (1) a policy, (2) a reward signal, (3) a value function, and, optionally, (4) a model of the environment.\n",
    "    - (1) **policy**\n",
    "        - defines the learning agent’s way of behaving at a given time. Roughly speaking, a policy is a mapping from perceived states of the environment to actions to be taken when in those states.\n",
    "        -  In some cases the policy may be a simple function or lookup table, whereas in others it may involve extensive computation such as a search process. \n",
    "        - The policy is the core of a reinforcement learning agent in the sense that it alone is sufficient to determine behavior. \n",
    "        - In general, policies may be stochastic, specifying probabilities for each action.\n",
    "    - (2) **reward signal**\n",
    "        - defines the goal of a reinforcement learning problem. \n",
    "        - On each time step, the environment sends to the reinforcement learning agent a single number called the reward. \n",
    "        - The agent’s sole objective is to maximize the total reward it receives over the long run. The reward signal thus defines what are the good and bad events for the agent.\n",
    "        - The reward signal is the primary basis for altering the policy; if an action selected by the policy is followed by low reward, then the policy may be changed to select some other action in that situation in the future. \n",
    "        - In general, reward signals may be stochastic functions of the state of the environment and the actions taken.\n",
    "    - (3) **value function**\n",
    "        - Whereas the **reward signal** indicates what is good in an immediate sense, a **value function** specifies what is good in the long run. \n",
    "        - Roughly speaking, the value of a state is the total amount of reward an agent can expect to accumulate over the future, starting from that state.\n",
    "        - Whereas rewards determine the immediate, intrinsic desirability of environmental states, values indicate the long-term desirability of states after taking into account the states that are likely to follow and the rewards available in those states. \n",
    "        - For example, a state might always yield a low immediate reward but still have a high value because it is regularly followed by other states that yield high rewards. Or the reverse could be true.\n",
    "        - Rewards are in a sense primary, whereas values, as predictions of rewards, are secondary. Without rewards there could be no values, and the only purpose of estimating values is to achieve more reward. Nevertheless, it is values with which we are most concerned when making and evaluating decisions. Action choices are made based on value judgments. We seek actions that bring about states of highest value, not highest reward, because these actions obtain the greatest amount of reward for us over the long run.\n",
    "        - <u> Unfortunately, it is much harder to determine values than it is to determine rewards. Rewards are basically given directly by the environment, but values must be estimated and re-estimated from the sequences of observations an agent makes over its entire lifetime. In fact, the most important component of almost all reinforcement learning algorithms we consider is a method for efficiently estimating values. </u> The central role of value estimation is arguably the most important thing that has been learned about reinforcement learning over the last six decades.\n",
    "    - (4) **model of the environment**\n",
    "        - This is something that mimics the behavior of the environment, or more generally, that allows inferences to be made about how the environment will behave. For example, given a state and action, the model might predict the resultant next state and next reward. \n",
    "        - Models are used for **planning**, by which we mean any way of deciding on a course of action by considering possible future situations before they are actually experienced. \n",
    "        - Methods for solving reinforcement learning problems that use *models and planning* are called **model-based** methods, as opposed to simpler **model-free** methods that are explicitly *trial-and-error* learners—viewed as almost the **opposite** of planning. \n",
    "        - In Chapter 8 we explore reinforcement learning systems that simultaneously learn by trial and error, learn a model of the environment, and use the model for planning. Modern reinforcement learning spans the spectrum from low-level, trial-and-error learning to high-level, deliberative planning.\n",
    "        \n",
    "**1.4 Limitaion and Scope** <br>        \n",
    "- We do not address the issues of constructing, changing, or learning the state signal in this book (other than briefly in  ection 17.3). We take this approach not because we consider state representation to be unimportant, but in order to focus fully on the decision-making issues. In other words, our concern in this book is not with designing the state signal, but with deciding what action to take as a function of whatever state signal is available.\n",
    "- Most of the reinforcement learning methods we consider in this book are structured around estimating value functions, but it is not strictly necessary to do this to solve reinforcement learning problems. For example, solution methods such as genetic algorithms, genetic programming, simulated annealing, and other optimization methods never estimate value functions. These methods apply multiple static policies each interacting over an extended period of time with a separate instance of the environment. The policies that obtain the most reward, and random variations of them, are carried over to the next generation of policies, and the process repeats. We call these evolutionary methods because their operation is analogous to the way biological evolution produces organisms with skilled behavior even if they do not learn during their individual lifetimes. If the space of policies is sufficiently small, or can be structured so that good policies are\n",
    "- common or easy to find—or if a lot of time is available for the search—then evolutionary methods can be e↵ective. In addition, evolutionary methods have advantages on problems in which the learning agent cannot sense the complete state of its environment.\n",
    "- Our focus is on reinforcement learning methods that learn while interacting with the environment, which evolutionary methods do not do. Methods able to take advantage of the details of individual behavioral interactions can be much more efficient than evolutionary methods in many cases. Evolutionary methods ignore much of the useful structure of the reinforcement learning problem: they do not use the fact that the policy they are searching for is a function from states to actions; they do not notice which states an individual passes through during its lifetime, or which actions it selects. In some cases this information can be misleading (e.g., when states are misperceived), but more often it should enable more efficient search. Although evolution and learning share many features and naturally work together, we do not consider evolutionary methods by themselves to be especially well suited to reinforcement learning problems and, accordingly, we do not cover them in this book.\n",
    "\n",
    "**1.5 ** An Extended Example: Tic-Tac-Toe<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 2 : The RL Framework: The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to mathematically formulate tasks as Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (19 concepts)\n",
    "- 1- Introduction\n",
    "- 2- The Setting, Revisited\n",
    "- 3- Episodic vs. Continuing Tasks\n",
    "- 4- Quiz: Test Your Intuition\n",
    "- 5- Quiz: Episodic or Continuing?\n",
    "- 6- The Reward Hypothesis\n",
    "- 7- Goals and Rewards, Part 1\n",
    "- 8- Goals and Rewards, Part 2\n",
    "- 9- Quiz: Goals and Rewards\n",
    "- 10- Cumulative Reward\n",
    "- 11- Discounted Return\n",
    "- 12- Quiz: Pole Balancing\n",
    "- 13- MDPs, Part 1\n",
    "- 14- MDPs, Part 2\n",
    "- 15- Quiz: One Step Dynamics, Part 1\n",
    "- 16- Quiz: One Step Dynamics, Part 2\n",
    "- 17- MDPs, Part 3\n",
    "- 18- Finite MDPs\n",
    "- 19- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.1) Introduction\n",
    "- the author : \n",
    "    - In this lesson, we'll end with a rigorous definition for the reinforcement learning problem.\n",
    "        - Specifically, you'll learn how to take a real world problem and formulate it so it can be solved through reinforcement learning.\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.2) The Setting, Revisited\n",
    "- The RL framework is characterized by an agent learned to interact with its environment. (fig.1) \n",
    "    - We assume that time evolves and discrete timesteps.\n",
    "    - At the initial timestep,the agent observes the environment. (fig.2)\n",
    "    - You can think of this observation as a situation that the environment presents to the agent.\n",
    "    - Then, it (the agent) must select an appropriate action in response. (fig.3)\n",
    "    - Then at the next timestep in response to the agents action, the environment presents a new situation to the agent. (fig.4).  At the same time the environment gives the agent a reward which provides some indication of whether the agent has responded appropriately to the environment. (fig.5)\n",
    "    - Then the process continues where at each timestep the environment sends the agent an observation and reward (fig.6). And in response, the agent must choose an action. (fig.7) <br><br>\n",
    "    \n",
    "- In general, we don't need to assume that the environment shows the agent everything he needs to make well-informed decisions. But it greatly simplifies the underlying mathematics if we do. So in this course, we'll make the assumption that the agent is able to fully observe what ever state the environment is in. And instead of referring to the agent as receiving an observation, we will hence say that it receives the environment state. (fig.8 where \"observation\" is changed to \"state\"). <br><br>\n",
    "\n",
    "- But let's make this description a bit clearer with some added notation where we again start from the very beginning at timestep zero.\n",
    "    - The agent first receives the environment state which we denote by S0, where zero stands for a timestep zero of course. (fig.9)\n",
    "    - Then, based on that observation the agent chooses an action, A0 (fig.10). \n",
    "    - At the next timestep, in this case, at timestep one, and as a direct consequence of the agent's choice of action, A0, and the environments previous state, S0, the environment transitions to a new state, S1, and gives some reward, R1, to the agent. (fig.11)\n",
    "    - The agent then chooses an action, A1. (fig.12 where A0 is changed to A1 inside the square, and A1 is added at the far right)\n",
    "    - At timestep two, the process continues where the environment passes the reward in state, then the agent responds with an action (fig.13) and so on. <br><br>\n",
    "    \n",
    "- Whereas the agent interacts with the environment, this interaction is manifest as a sequence of states, actions, and rewards. That said, the reward will always be the most relevant quantity to the agent. To be specific, any agent has the goal to  aximize expected cumulative reward or the some of the rewards attained over all timesteps. In other words, it seeks to find the  trategy for choosing actions with the cumulative reward is likely to be quite high. And the agent can only accomplish this by  nteracting with the environment. This is because at every timestep, the environment decides how much reward the agent receives. In other words, the agent must play by the rules of the environment. But through interaction, the agent can learn those rules  nd choose appropriate actions to accomplish its goal. And this is essentially what we'll try to accomplish in this course. <br><br>\n",
    "\n",
    "- But it's important to emphasize that all of this is just a mathematical model for a real world problem. **So if you have a  roblem in mind that you think can be solved with reinforcement learning, you will have to specify the states, actions, and rewards, and you'll have to decide the rules of the environment**. \n",
    "- In this course. You'll see a lot of examples for how to accomplish this.\n",
    "\n",
    "\n",
    "| column 1 | column 2  |  column3 |\n",
    "| --- | --- | --- |\n",
    "| ![title](img/6_2_2_1.png) | ![title](img/6_2_2_2.png) | ![title](img/6_2_2_3.png) |\n",
    "| ![title](img/6_2_2_4.png) | ![title](img/6_2_2_5.png) | ![title](img/6_2_2_6.png) |\n",
    "| ![title](img/6_2_2_7.png) | ![title](img/6_2_2_8.png) | ![title](img/6_2_2_9.png) |\n",
    "| ![title](img/6_2_2_10.png) | ![title](img/6_2_2_11.png) | ![title](img/6_2_2_12.png) |\n",
    "| ![title](img/6_2_2_13.png) |  |  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.3) Episodic vs. Continuing Tasks\n",
    "- **Episodic Tasks** are tasks that have a well defined ending point. \n",
    "    - i.e interaction ends at some time step $T$ as follows $S_0, A_0, R_1, S_1, A_1, ..., R_T, S_T$\n",
    "    - in this case, we refer to a complete sequence of interaction from start to finish as an **episode** (the previous line from $S_0$ to $S_T$).\n",
    "        - When the episode ends, the agent looks at the total amount of reward it received to figure out how well it did. \n",
    "        - It's then able to start from scratch as if it has been completely reborn into the same environment but now with the added knowledge of what happened in its past life. \n",
    "        - In this way, as time passes over its many lives, the agent makes better and better decisions and you'll see this for yourself in your coding implementations. \n",
    "        - Once your agents have spent enough time getting to know the environment, they should be able to pick a strategy where the cumulative reward is quite high. In other words, in the context of a game playing agent, it should be able to achieve a higher score.\n",
    "    - examples:\n",
    "        - an agent that plays a game, then the interaction ends win the agent wins or loses (or reaches a draw)\n",
    "        - a simulation to teach a car to drive, then the interaction ends if the car crashes\n",
    "        \n",
    "- **Continuing Tasks** goes on for ever without an end    \n",
    "    - i.e interaction continues without limit $S_0, A_0, R_1, S_1, A_1, ...$\n",
    "    - example:\n",
    "        - An algorithm that buys and sells stocks in response to the financial market would be best modeled as an agent in the continuing tasks. In this case, the agent lives forever. So it has to learn the best way to choose actions while simultaneously interacting with the environment.\n",
    "        - The algorithms for this case are slightly more complex and will be covered a bit later in the course."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.4) Quiz: Test Your Intuition\n",
    "- If we have a chess game, and say That the **reward** is only delivered at the end of the game, and, let’s say, is +1 if you win, and -1 if you lose. <br><br>\n",
    "\n",
    "- This is an episodic task, where an episode finishes when the game ends. The idea is that by playing the game many times, or by interacting with the environment in many episodes, you can learn to play chess better and better. <br><br>\n",
    "\n",
    "- It's important to note that this problem is **exceptionally difficult**, because the feedback is only delivered at the very end of the game. So, if you lose a game (and get a reward of -1 at the end of the episode), it’s unclear when exactly you went wrong: maybe you were so bad at playing that every move was horrible, or maybe instead … you played beautifully for the majority of the game, and then made only a small mistake at the end. <br><br>\n",
    "\n",
    "- When the reward signal is largely uninformative in this way, we say that the task suffers the problem of **sparse rewards**. There’s an entire area of research dedicated to this problem, and you’re encouraged to read more about it, if it interests you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.5) Quiz: Episodic or Continuing?\n",
    "- Remember:\n",
    "    - A **task** is an instance (Q\\ does \"instance\" mean \"an exapmle\" ?) of the reinforcement learning (RL) problem.\n",
    "    - Episodic tasks come to an end whenever the agent reaches a **terminal state**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.6) The Reward Hypothesis\n",
    "- since there are lots of applications of RL, different agents have differencet goals.\n",
    "    - It's truly amazing that all of these very different goals can be addressed with the same theoretical framework.\n",
    "- all agents formulate their goals in terms of maximizing expected cumulative reward. But what could reward mean in the context of something like a robot learning to walk?  \n",
    "    - Maybe we could think of the environment as a type of trainer that watches the robots movements and rewards it for having good walking form. But then the reward that it gives has the potential to be highly subjective and not scientific at all.I  ean, what makes a walk good? And what makes it bad? And how do we address this? or In general, how do we specify reward to describe any of a number of potential goals that our agents could have?\n",
    "    - Well, before we answer this question, let's take one step back. It's important to note that the word \"Reinforcement\" and \"Reinforcement Learning\" is a term originally from behavioral science. It refers to a stimulus that's delivered immediately after behavior to make the behavior more likely to occur in the future. The fact that this name is borrowed is no coincidence. In fact, it's an important defining hypothesis in reinforcement learning that we can always formulate an agents goal along the lines of maximizing **expected** cumulative reward. And we call this hypothesis, the \"Reward Hypothesis\". \n",
    "- the \"Reward Hypothesis\" : All goals can be frames as the maximization of **expected** cumulative reward. <br><br>\n",
    "\n",
    "- If this still seems weird or uncomfortable to you, you are not alone. But allow me to convince you in the next video.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.7) Goals and Rewards, Part 1\n",
    "- So, I'd like to talk to you about some research that I find particularly interesting. And I think it's a great example to illustrate the reward hypothesis that was introduced in the previous video. \n",
    "- Google DeepMind recently addressed the problem of teaching a robot to walk. Among other problem domains, they worked with a physical simulation of a humanoid robot and they managed to apply some nice reinforcement learning to get great results. (fig.1) and (fig.2).\n",
    "- As you learned in an earlier video, in order to frame this as a reinforcement learning problem, we'll have to specify the state's actions and rewards. (fig.3)\n",
    "- We'll dedicate two videos to this example (me: i think they are the current video and the next one.) and we'll begin by detailing the actions. <br><br>\n",
    "\n",
    "- **Actions**:\n",
    "    - are the decisions that need to be made in order for the robot to walk.Now, the humanoid has several joints, and the actions are just the forces that the robot applies to its joints in order to move. Because if the robot has an intelligent method for deciding these forces at every point in time, that will be sufficient to get it walking. (fig.4) <br><br>\n",
    "    \n",
    "- **States**:\n",
    "    - The states are the context provided to the agent for choosing intelligent actions.In this context, the state at any point in time contain :\n",
    "        - the current positions and velocities of all of the joints, along with \n",
    "        - some measurements about the surface that the robot was standing on. These measurements captured how flat or inclined the ground was, if there was a large step along the path and so on.T\n",
    "        - The researchers at Google DeepMind also added contact sensor data, so that it could determine if the robot was still walking or if it had fallen over.\n",
    "    - The idea is that based on the information in the state,the agent has to plan its next action. After all, if there's a step along the path, that will require a different type of movement than if the ground were completely flat. <br><br>\n",
    "\n",
    "- **Rewards**:\n",
    "    - We'll design the reward as a feedback mechanism that tells the agent that it has chosen the appropriate movements.The reward will be our way of telling the agent, \"Good job, for not running into that wall or too bad, you missed that step and fell down.\" \n",
    "    - That's just the main idea and we'll go into depth in the next video.\n",
    "\n",
    "| column 1 | column 2  |  column3 |\n",
    "| --- | --- | --- |\n",
    "| ![title](img/6_2_7_1.png) | ![title](img/6_2_7_2.png) | ![title](img/6_2_7_3.png) |\n",
    "| ![title](img/6_2_7_4.png) |  |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.8) Goals and Rewards, Part 2\n",
    "- So far, we've been trying to frame the idea of a humanoid learning to walk in the context of reinforcement learning. We've detailed the states in actions, and we still need to specify the rewards. <br><br>\n",
    "\n",
    "- **Rewards**\n",
    "    - The reward structure from the DeepMind paper is surprisingly intuitive. The followimg line is pulled from the appendix of the DeepMind paper,and describes how the reward is decided at every time step. Each term communicates to the agent some part of what we'd like it to accomplish.\n",
    "    \n",
    "\\begin{equation*}\n",
    "r = min(v_x, v_{max}) - 0.005(v_y^2+v_z^2) - 0.05y^2-0.02||u||^2+0.02\n",
    "\\end{equation*}\n",
    "\n",
    "- _\n",
    "    - So let's look at each term individually. <br><br>\n",
    "    \n",
    "        - At every time step, the agent receives a reward proportional to its forward velocity.\n",
    "            - the corresponding term : $min(v_x, v_{max})$\n",
    "            - (fig.1)\n",
    "            - So if moves faster, it gets more reward, but up to a limit, here denoted Vmax.  \n",
    "            - my notes:\n",
    "                - i think the robot is moving (or the creators want it to move) in the x direction.\n",
    "                - the function $min(v_x, v_{max})$ will make the robot increase its velocity to reach $v_{max}$, otherwise if $v_x$ is smaller than $v_{max}$, the func will return $v_x$ which will be less reward. Also if the robot increased its speed above $v_{max}$, this will have no effect on the reward since the func will always return $v_{max}$ (the minimumm in that case). <br><br>\n",
    "                \n",
    "        - But the robot is penalized by an amount proportional to the force applied to each joint.\n",
    "            - the corresponding term : $-0.02||u||^2$\n",
    "            - (fig.2)\n",
    "            - So if the agent applies more force to the joints, then more reward is taken away as punishment. \n",
    "            - my notes:\n",
    "                - from the paper, $u$ corresponds to <br><br>\n",
    "                \n",
    "         - Since the researchers also wanted the humanoid to focus on moving forward, the agent is also penalized for moving left, right, or vertically.\n",
    "            - the corresponding term : $- 0.005(v_y^2+v_z^2)$\n",
    "            - (fig.3)\n",
    "            - my note:\n",
    "                - left and right movement are both included in the term $v_y^2$, also by vertically she means up or down (this is included in the term $v_z^2$) <br><br>\n",
    "\n",
    "        - It was also penalized if the humanoid moved its body away from the center of the track.\n",
    "            - the corresponding term : $- 0.05y^2$\n",
    "            - (fig.4)\n",
    "            - So the agent will try to keep the humanoid as close to the center as possible. <br><br>\n",
    "\n",
    "        - At every time step, the agent also receives some positive reward if the humanoid has not yet fallen.\n",
    "            - the corresponding term : $+0.02$\n",
    "            - (fig.5) <br><br>\n",
    "\n",
    "    - They frame the problem as an episodic task where if the human falls, then the episode is terminated. At this point, whatever cumulative reward the agent had at that time point is all it's ever going to get. In this way, the reward signal is designed, so if the robot focused entirely on maximizing this reward, it would also coincidentally learn to walk. To see this, note the following <br><br>\n",
    "    \n",
    "        - first note that if the robot falls, the episode terminates. And that's a missed opportunity to collect more of this positive reward. (fig.6), (my note: by falling, this term \" $+0.02$ \" will stop giving reward)\n",
    "            - And in general, if the robot walks for ten time steps, that's only 10 opportunities to get reward. And if it  tays walking for 100, that's a lot more time to collect more reward. So if we get the reward in this way, the agent will try to keep from falling for as long as possible. <br><br>\n",
    "        \n",
    "        - Next, since the reward is proportional to the forward velocity, this will ensure the robot also feels pressured to walk as quickly as possible in the direction of the walking track. (fig.7) <br><br>\n",
    "        \n",
    "        - but it also makes sense to penalize the agent for applying too much force to the joints. (fig.8) This is because otherwise, we could end up with a situation where the humanoid walks to erratically (language note from google : erratically = in a manner that is not even or regular in pattern or movement; unpredictably.). By penalizing large forces, we can try to keep the movements more smooth and elegant.\n",
    "\n",
    "        - Likewise, we want to keep the agent on the track and moving forward. (fig.9) Otherwise, who knows where it could end up walking off to. <br><br>\n",
    "        \n",
    "    - Of course, the robot can't focus just on walking fast (fig.10), or just on moving forward (fig.11), or only on walking smoothly (fig.12), or just on walking for as long as possible (fig.13). These are four somewhat competing requirements that the agent has to balance for all time steps towards its goal of maximizing expected cumulative reward.\n",
    "    - And Google DeepMind demonstrated that from this very simple reward function, the agent is able to learn how to walk in a very human like fashion. In fact, this rewards function is so simple that it may seem that deciding reward is quite straightforward, but in general, this is not the case. Of course, there are some counterexamples to this. For instance, if you're teaching an agent to play a video game, the reward is just the score on the screen. And if you're teaching an agent to play Backgammon, well, the reward is delivered only at the end of the game, and you could construct it to be positive if the agent wins, and negative, if it loses.\n",
    "- **The fact, that the reward is so simple is precisely what makes this research from DeepMind so fascinating.**\n",
    "    \n",
    "| column 1 | column 2  |  column3 |\n",
    "| --- | --- | --- |\n",
    "| ![title](img/6_2_8_1.png) | ![title](img/6_2_8_2.png) | ![title](img/6_2_8_3.png) |\n",
    "| ![title](img/6_2_8_4.png) | ![title](img/6_2_8_5.png) | ![title](img/6_2_8_6.png) |\n",
    "| ![title](img/6_2_8_7.png) | ![title](img/6_2_8_8.png) | ![title](img/6_2_8_9.png) |\n",
    "| ![title](img/6_2_8_10.png) | ![title](img/6_2_8_11.png) | ![title](img/6_2_8_12.png) |\n",
    "| ![title](img/6_2_8_13.png) |  |  |\n",
    "\n",
    "---\n",
    "If you'd like to learn more about the research that was done at [DeepMind](https://deepmind.com/), please check out [this link](https://deepmind.com/blog/producing-flexible-behaviours-simulated-environments/). The research paper can be accessed [here](https://arxiv.org/pdf/1707.02286.pdf). Also, check out this cool [video](https://www.youtube.com/watch?v=hx_bgoTF7bs&feature=youtu.be)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.9) Quiz: Goals and Rewards\n",
    "- So far, you've seen one example for how to frame an agent's goal as the maximization of expected cumulative reward. In this quiz, you will investigate several more examples. <br><br>\n",
    "\n",
    "- QUESTION 1 OF 3\n",
    "    - Consider an agent who would like to learn to escape a maze. Which reward signals will encourage the agent to escape the maze as quickly as possible? Select all that apply. \n",
    "        - (the right answers are the following)\n",
    "            - The reward is -1 for every time step that the agent spends inside the maze.  Once the agent escapes, the episode terminates.\n",
    "            - The reward is -1 for every time step that the agent spends inside the maze.  Once the agent escapes, it receives a reward of +10, and the episode terminates. \n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - The reward is +1 for every time step that the agent spends inside the maze.  Once the agent escapes, the episode terminates.\n",
    "                - my explanation : this way, the agent will try to stay at the maze indefinitely\n",
    "            - The reward is 0 for every time step that the agent spends inside the maze.  Once the agent escapes, it receives a reward of +1, and the episode terminates.\n",
    "                - my explanation : Although this reward system will let the agent escape the maze, it will not encourage it to escape as quickly as possible (which is required in the question above)<br><br>\n",
    "        \n",
    "- QUESTION 2 OF 3\n",
    "    - Consider an agent who would like to learn to play a board game (like backgammon, chess, or checkers). Which reward signals will encourage the agent to win the game? Select all that apply.        \n",
    "        - (the right answers are the following) \n",
    "            - The agent receives a reward only at the end of the game, and receives a reward of +1 if it wins, -1 if it loses, and 0 if the game is a draw.\n",
    "            - The agent receives a reward only at the end of the game, and receives a reward of +10 if it wins, -10 if it loses, and 0 if the game is a draw.\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - The agent receives a reward of -1 for every time step that it is still playing the game; once the game ends, the episode terminates.\n",
    "                 - my explanation : the agent will try to finish the game as quickly as possible (i think this will usually lead it to try to lose to finish the game fast)\n",
    "            - The agent receives a reward only at the end of the game, and receives a reward of -1 if it wins, +1 if it loses, and 0 if the game is a draw.\n",
    "                - my explanation : the agent goal will be to lose the game ! <br><br>\n",
    "                \n",
    "- QUESTION 3 OF 3\n",
    "    - Consider an agent who would like to learn to balance a plate of food on her head. Which reward signals will encourage the agent to keep the plate balanced for as long as possible? Select all that apply. \n",
    "        - (the right answers are the following) \n",
    "            - The reward is +1 for every time step that the agent keeps the plate balanced on her head.  If the plate falls, the episode terminates.\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - The reward is -1 for every time step that the agent keeps the plate balanced on her head.  If the plate falls, the episode terminates.\n",
    "                - my explanation : the agent will deliberately let go of the plate when the episode starts\n",
    "            - The agent receives a reward only when the plate falls.  If the plate does not break, the agent receives a reward of +1.\n",
    "                - my explanation : the agent's goal will be to let the plate fall in way such that the plate does not break."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.10) Cumulative Reward\n",
    "- Built into the framework is the agent's goal which is to maximize cumulative reward.But what exactly does this mean and how does the agent accomplish this?\n",
    "- Could the agent just maximize the reward and each time step? The short answer to that question is, no. But I think a long answer would be a lot more satisfying. So let's try to understand this with the walking robot example.\n",
    "- Remember that in this case, the goal of the robot was to stay walking forward (first three terms) for as long (last term) and as quickly (first term) as possible while also exerting minimal effort (4th term, that let the robot walk smoothly).\n",
    "    - In this case, if the robot tried to maximize the reward it received at a single time step, that would look like trying to move as quickly as possible with as little effort without falling immediately. That could work well in the short term but it's possible, for instance, that the agents movement gets it moving quickly without falling initially. But that first movement was de-stabilising enough that it doomed the agent to fall in a short time. In this way, if the agent focused on individual time steps, it could learn actions that maximize initial rewards. But then the episode terminates quite quickly. And so the cumulative reward is quite small. And still worse, in this case, the agent will not have learned to walk. In this example then, it's clear that the agent cannot focus on individual time steps and instead, needs to keep all time steps in mind. But this also holds true for reinforcement learning agents in general. **Actions have short and long term consequences and the agent needs to gain some understanding of the complex effects its actions have on the environment.**\n",
    "    - Along these lines in the walking robot example, if the agent always has reward at all time steps in mind, it will learn to choose movement designed for long term stability. So in this way, the robot moves a bit slowly to sacrifice a little bit of reward but it will payoff because it will avoid falling for longer and collect higher cumulative reward. \n",
    "    - But now, what does all of this mean when the agent chooses an action at an arbitrary time step? How exactly does it keep all time steps in mind? Well, if we're looking at some time step, t, it's important to note that the rewards for all previous time steps have already been decided as they're in the past. Only future rewards are inside the agent's control (fig.1). We refer to the sum of rewards from the next time step onward as the **return** and denote it with a **capital G** (fig.2), and at an arbitrary time step, the agent will always choose an action towards the goal of maximizing the *return*. But it's actually more accurate to say that the agent seeks to maximize **expected return**. This is because it's generally the case that the agent can't predict with complete certainty what the future reward is likely to be. So it has to rely on a prediction or an estimate.\n",
    "        - We'll massage this a bit when we talk about **discounted return**. But this is the main idea.\n",
    "        \n",
    "| column 1 | column 2  | \n",
    "| --- | --- | \n",
    "| ![title](img/6_2_10_1.png) | ![title](img/6_2_10_2.png) |        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.11) Discounted Return\n",
    "- We've discussed how an agent might choose actions with the goal of maximizing expected return but we need to dig a bit deeper. For instance, consider our puppy agent, how does he predict how much reward he could get at any point in the future?Puppies can live for decades. Can he really be expected to have just as much of an idea of how much reward he'll get now as he does five years from now? Does it make more sense to consider that it's not entirely clear what the future holds especially if the puppy is still learning, proposing, and testing hypotheses and changing his strategy? It's unlikely that he'll know one thousand times steps in advance what his reward potential is likely to be. In general, the puppy is likely to have a much better idea of what's likely to happen in the near future than he does for a distant time points. Along these lines then, should present reward carry the same weight as future reward? Maybe it makes more sense to value rewards that come sooner more highly, since those rewards are more predictable. This situation motivates the idea of discounting and discounted return. <br><br>\n",
    "\n",
    "- Remember that the goal of the agent is always to maximize cumulative reward. And towards this end, at an arbitrary time step $t$, it can choose the action that maximizes the return. And currently, each time step from $t+1$ onward has an equal say in how the agent should make decisions. What if instead, we wanted time steps that occurred earlier in time to have a much greater say. Well, then instead of maximizing this sum, the idea is that we'll maximize a different sum with rewards that are farther along in time are multiplied by smaller values. We refer to this sum as discounted return. By discounted, we mean that we'll change the goal to care more about immediate rewards rather than rewards that are received further in the future. (fig.1) <br><br>\n",
    "\n",
    "- But how do we choose what values to use here? Well, in practice, we'll define what's called a discount rate, which is always denoted by the Greek letter gamma, and is always a number between zero and one. Then, as for the values, the first term is multiplied by gamma. The second term is multiplied by gamma square. Then, gamma to the third power and so on (fig.2). In this way, we have a nice decay where rewards that occur earlier in time are always multiplied by a larger number. <br><br>\n",
    "\n",
    "\n",
    "- It's **important to note that this gamma is not something that's learned by the agent**. It's something that you set to refine the goal that you have for the agent. So how exactly might you set the value of gamma?  <br>\n",
    "Let's begin by looking at what happens when we set gamma to one. So we plug in one everywhere we see gamma and we see it yields the original, completely un-discounted return from the previous videos. And what about when gamma is set to zero? In this case, every term in this sum disappears with the exception of the most immediate reward (fig.3). \n",
    "- In this way, we see that the larger you make gamma, the more the agent cares about the distant future. And as gamma gets smaller and smaller, we get increasingly extreme discounting, where in the most extreme case, the agent only cares about the most immediate reward.\n",
    "- It's **important to note that discounting is particularly relevant to continuing tasks.** Remember that a continuing task is one where the agent environment interaction goes on without end. In this case, if the agent wants to maximize cumulative reward while it's a pretty difficult task if the feature is limitless. So we use discounting to avoid having to look too far into the limitless future.\n",
    "- But it's **important to note that with or without discounting, the goal is always the same. It's always to maximize cumulative reward.**\n",
    "- The discount rate comes in when the agent chooses actions at an arbitrary time step. It uses the discount rate as part of its program for picking actions. And that program is more interested in securing rewards that come sooner and are more likely than the rewards that come later and are less likely.<br>\n",
    "\n",
    "- You'll learn more about how exactly the agent should select actions in the next lesson. (me: 6-2 RL frame work: the solution) <br>\n",
    "For now, we'll focus on fully specifying the reinforcement learning problem. <br>\n",
    "\n",
    "| column 1 | column 2  |  column3 |\n",
    "| --- | --- | --- |\n",
    "| ![title](img/6_2_11_1.png) | ![title](img/6_2_11_2.png) | ![title](img/6_2_11_3.png) |\n",
    "\n",
    "---\n",
    "Note: In this course, we will use \"return\" and \"discounted return\" interchangably. For an arbitrary time step tt, both refer to $G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$, where $\\gamma \\in [0,1]$. In particular, when we refer to \"return\", it is not necessarily the case that $\\gamma = 1$, and when we refer to \"discounted return\", it is not necessarily true that $\\gamma < 1$. (This also holds for the readings in the recommended textbook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.12) Quiz: Pole Balancing\n",
    "In this classic reinforcement learning task, a cart is positioned on a frictionless track, and a pole is attached to the top of the cart. The objective is to keep the pole from falling over by moving the cart either left or right, and without falling off the track. <br><br>\n",
    "\n",
    "In the OpenAI Gym implementation, the agent applies a force of +1 or -1 to the cart at every time step. It is formulated as an episodic task, where the episode ends when (1) the pole falls more than 20.9 degrees from vertical, (2) the cart moves more than 2.4 units from the center of the track, or (3) when more than 200 time steps have elapsed. The agent receives a reward of +1 for every time step, including the final step of the episode. You can read more about this environment in OpenAI's github. This task also appears in Example 3.4 of the textbook. <br><br>\n",
    "\n",
    "- QUESTION 1 OF 3\n",
    "    - Recall that the agent receives a reward of +1 for every time step, including the final step of the episode. Which discount rates would encourage the agent to keep the pole balanced for as long as possible? (Select all that apply.)\n",
    "        - (the right answers are the following)\n",
    "            - The discount rate is 1\n",
    "            - The discount rate is 0.9\n",
    "            - The discount rate is 0.5\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - (no wrong answers) <br><br>\n",
    "            \n",
    "- QUESTION 2 OF 3\n",
    "    - Say that the reward signal is amended (عُدلت) to only give reward to the agent at the end of an episode. So, the reward is 0 for every time step, with the exception of the final time step. When the episode terminates, the agent receives a reward of -1. Which discount rates would encourage the agent to keep the pole balanced for as long as possible? (Select all that apply.)\n",
    "        - (the right answers are the following)\n",
    "            - The discount rate is 0.9\n",
    "            - The discount rate is 0.5 \n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - The discount rate is 1 \n",
    "                - udacity explanation (then my comment) : Without discounting, the agent will always receive a reward of -1 (no matter what actions it chooses during the episode), and so the reward signal will not provide any useful feedback to the agent. With discounting, the agent will try to keep the pole balanced for as long as possible, as this will result in a return that is relatively less negative. \n",
    "                - my comment on udacity explanation: the it will be less negative because the last reward will be mutlplied by the discount rate. so if the agent fall at the $3^{rd}$ time step, the reward (-1) will be mutliplied by $\\gamma^2$ but if the agent fall ath the $100^{th}$ time step, the reward (-1) will be multiplied by $\\gamma^{101}$ which of course is better than at 3rd time step, so the agent will understand that it has to not fall as possible, so it will learn.\n",
    "                \n",
    "- QUESTION 3 OF 3\n",
    "    - Say that the reward signal is amended to only give reward to the agent at the end of an episode. So, the reward is 0 for every time step, with the exception of the final time step. When the episode terminates, the agent receives a reward of +1. Which discount rates would encourage the agent to keep the pole balanced for as long as possible? (Select all that apply.) \n",
    "        - (the right answers are the following)\n",
    "            - (None of these discount rates would help the agent, and there is a problem with the reward signal.)\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - The discount rate is 1\n",
    "            - The discount rate is 0.9\n",
    "            - The discount rate is 0.5\n",
    "                - udacity explanation (then my comment) : If the discount rate is 1, the agent will always receive a reward of +1 (no matter what actions it chooses during the episode), and so the reward signal will not provide any useful feedback to the agent. If the discount rate is 0.5 or 0.9, the agent will try to terminate the episode as soon as possible (by either dropping the pole quickly or moving off the edge of the track). Thus, you are correct - we must redesign the reward signal!\n",
    "                - my comment on udacity explanation : the reason that a $\\gamma=1$ will always give the same reward is that the rule says that at the end of the episode, the agent takes a reward =1 , so if the robot fall at the 1st, 2nd, 3rd or any other time step, it will always take that reward =1, so the robot will learn nothing. on the otherhand, a $\\gamma=0.9 \\; or \\;  0.5$ will result in a discount for the rewards (so the robot has to fall as soon as possible, so that the last reward (that eqauls 1) is not discounted too much). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.13) MDPs, Part 1\n",
    "Over the next several videos, you'll learn all about how to rigorously define a reinforcement learning problem as a Markov Decision Process (MDP). <br>\n",
    "Towards this goal, we'll begin with an example!\n",
    "\n",
    "---\n",
    "- So far, you've just started a conversation to set the stage for what we'd like to accomplish. We'll use the remainder of this lesson to specify a rigorous definition for the reinforcement learning problem. For context, we'll work with the example of a recycling robot from the Sutton textbook. <br><br>\n",
    "\n",
    "- So consider a robot that's designed for picking up empty soda cans. The robot is equipped with arms to grab the cans and runs on a rechargeable battery. There's a docking station set up in one corner of the room and the robot has to sit at the station if it needs to recharge its battery. Say, you're trying to program this robot to collect empty soda cans without human intervention. In particular, you want the robot to be able to decide for itself when it needs to recharge its battery. And whenever it doesn't need to recharge, you want it to focus on collecting as many soda cans as possible. **So let's see if we can frame this as a reinforcement learning problem.** <br><br>\n",
    "\n",
    "- We'll begin with the actions.\n",
    "- **Actions** :\n",
    "    - We'll say the robot is capable of executing three potential actions. It can search the room for cans, it can head to the docking station to recharge its battery, or it can stay put in the hopes that someone brings it a can. We refer to the set of possible actions as the action space, and it's common to denote it with a script A.\n",
    "    <img src=\"img/6_2_13_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br><br>\n",
    "    \n",
    "- Next, we consider the states.\n",
    "- **States** : \n",
    "    - Remember, the states are just the context provided to the agent for making intelligent actions. So the state, in this case, could be the charge left on the robot's battery. For simplicity, we'll assume that the battery has one of two states. One corresponding to a high amount of charge left, and the other corresponding to a low amount of charge. We refer to the set of possible states as the state space and it's common to denote with a script S. So intuition tells us that if the robot has a high amount of charge left on its battery, we'd like it to know to actively search for the room for cans. Searching the room should use up a lot of energy but this doesn't matter so much because the battery has a lot of charge anyway. But if the state is low, searching for cans has pretty high risk because the battery could get depleted mid-search and then the robot would be stranded and that wouldn't be so good because we don't want to have to come to its rescue. So if the battery is low, maybe we'd like the robot to know to wait for a can or to go to recharge its battery. \n",
    "<img src=\"img/6_2_13_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/><br><br>\n",
    "\n",
    "- In the next few concepts, we'll set up the problem with the ultimate goal of having the robots control equipment learn this behavior.\n",
    "- (me: i am still waiting to see how is the reward formulated ? if it is not mentioned here, i may look at the book)\n",
    "---\n",
    "**Notes :**\n",
    "- In general, the state space $\\mathcal{S}$ is the set of **all nonterminal states**.\n",
    "\n",
    "- In continuing tasks (like the recycling task detailed in the video), this is equivalent to the set of **all states**.\n",
    "\n",
    "- In episodic tasks, we use $\\mathcal{S}^+$ to refer to the set of **all states, including terminal states.**\n",
    "    - the follwing pic is a drawing of me illustrating $\\mathcal{S}$ and $\\mathcal{S}^+$ \n",
    "<img src=\"img/6_2_13_notes.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- The action space $\\mathcal{A}$ is the set of possible actions available to the agent.\n",
    "\n",
    "- In the event that there are some states where only a subset of the actions are available, we can also use \\mathcal{A}(s)A(s) to refer to the set of actions available in state s\\in\\mathcal{S}s∈S."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.14) MDPs, Part 2\n",
    "- So we're working with an example of a recycling robot and we've already detailed the states and actions.\n",
    "- In this example, remember that the state corresponds to the charge left on the robot's battery. And there are two potential states, high and low. <br><br>\n",
    "\n",
    "- As a first step, consider the case of the charge on the battery is <u>high</u>. \n",
    "    - Then, the robot could choose to search, wait, or recharge. But actually, recharging doesn't make much sense if the battery is already high, so we'll say that the only options are to search or wait. All right, so if the agent chooses to search, then at the next time step, the state could be high or low. Let's say that with 70 percent probability, it stays high. So there's a 30 percent chance the battery switches to low. In both cases, we'll say that this decision to search led to the robot collecting exactly four cans. And in line with this, the environment gives the agent a reward of four. The other option is to wait. If the robot has a high battery and then decides to wait, well, waiting doesn't use any battery at all and we'll say that then, it's guaranteed that the battery will again be high at the next time step. In this case, we'll suppose that since the robot wasn't out actively searching, it's able to collect fewer cans and say it's delivered just one can. And again in line with this, the environment gives the agent a reward of one. <br><br>\n",
    "    \n",
    "- Onto the case where the battery is <u>low</u>. \n",
    "    - Again, the robot has three options. If the battery is low and it chooses to wait for people to bring cans, that doesn't use any battery until the state at the next time step is going to be low. And just like when the robot decided to wait when the battery was high, the agent gets a reward of one. If the robot recharges, then it goes back to the docking station and the state of that the next time step is guaranteed to be high. Say it collects no cans along the way and gets a reward of zero. And if it searches, well, that's risky. It's possible that it gets away with this and then at the next time step, the battery is still low but not entirely depleted. But it's probably more likely that the robot depletes its battery, has to be rescued and is carried to a docking station to be charged. So the charge on its battery at the next time step is high. So the robot depletes its battery with 80 percent probability and otherwise gets away with that risky action with 20 percent probability. As for the reward, if the robot needs to be rescued, we want to make sure we're punishing the robot in this case, so say we don't look at all at the number of cans it was able to collect and we just give the robot a reward of negative three for that. But if the robot gets away with it, he collects four cans and get the reward of four. \n",
    "    <img src=\"img/6_2_14_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "    \n",
    "- This picture completely characterizes one method that the environment could use to decide the next state in reward at any point in time. To see this, let's look at a concrete example. <br>\n",
    "    - Say for instance, the last state was high and the agent decided to search. Then, the environment would flip a theoretical coin with 70 percent probability of landing heads, and if that coin landed heads, the environment would decide that the next state was high and the agent would get a reward four. Otherwise, if it landed tails, the next state would be low and the reward would be four. As another example, if the last state was low and the agent decided to search, the environment would again flip a theoretical coin now with 80 percent probability of landing heads. If it landed heads, the environment would decide the next state was high and the agent would get a reward of negative three. Otherwise, if it landed tails, the next state would be low and the reward would be four. <br><br>\n",
    "    \n",
    "- But **what's important to emphasize here is how little information the environment uses to make decisions (me: note that the author now is talking about the response/actions of the environment, not the agent ). It doesn't care what situation was presented to the agent 10 or 100 or even two steps prior. And it doesn't look at the actions that the agent took prior to the last one. And how well the agent is doing or how much reward it's collected has no effect on how the environment chooses to respond to the agent. Of course, it's possible to design environments that have much more complex procedures for interacting with the agent, but this is how it's done in reinforcement learning, and you'll see soon for yourself in your implementations just how powerful this framework is.**\n",
    "\n",
    "Q\\ are all frame works in RL like this !!? aren't there environments that consider the previous states of the agent ??\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.15) Quiz: One Step Dynamics, Part 1\n",
    "- Consider the recycling robot example. In the previous concept, we described one method that the environment could use to decide the state and reward, at any time step. <br><br>\n",
    "\n",
    "- QUESTION 1 OF 2\n",
    "    - Say the current state is high, and the agent decides to wait. How does the environment decide the next state and reward?\n",
    "        - (the right answers are the following)\n",
    "            - The next state is high, and the reward is 1.\n",
    "            \n",
    "- QUESTION 2 OF 2        \n",
    "    - Say the current state is low, and the agent decides to recharge. How does the environment decide the next state and reward?\n",
    "        - (the right answers are the following)\n",
    "            - The next state is high, and the reward is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.16) Quiz: One Step Dynamics, Part 2\n",
    "- It will prove convenient to **represent** the **environment's dynamics** <u>using</u> **mathematical notation**. In this concept, we will introduce this notation (which can be used for any reinforcement learning task) and use the recycling robot as an example.\n",
    "- (my note : they will say the same note that the author said in the video on the graph, but now with mathematical notation) <br><br>\n",
    "\n",
    "At an arbitrary time step tt, the agent-environment interaction has evolved as a sequence of states, actions, and rewards $(S_0, A_0, R_1, S_1, A_1, \\ldots, R_{t-1}, S_{t-1}, A_{t-1}, R_t, S_t, A_t)$\n",
    "\n",
    "When the environment responds to the agent at time step $t+1$, it considers only the state and action at the previous time step $(S_t, A_t)$\n",
    "\n",
    "In particular, it does not care what state was presented to the agent more than one step prior. (In other words, the environment does not consider any of $\\{ S_0, \\ldots, S_{t-1} \\}$\n",
    "\n",
    "And, it does not look at the actions that the agent took prior to the last one. (In other words, the environment does not consider any of $\\{ A_0, \\ldots, A_{t-1} \\}$ \n",
    "\n",
    "Furthermore, how well the agent is doing, or how much reward it is collecting, has no effect on how the environment chooses to respond to the agent. (In other words, the environment does not consider any of $\\{ R_0, \\ldots, R_t \\}$ <br>\n",
    "**(my note: and that's exatlyc what the graph shows, u only need to know the current state and the action, to know the the probability of transitioning to a next particular state with a particular reward),,, i am curios though to know, how could we represent the system if the environment take into consideration the history of the actions and states and rewards of the agent ?? is there a way to represnt that in a graph ? or is it only applicable mathematically ? also how is this described mathematically ?**\n",
    "\n",
    "Because of this, we can completely define how the environment decides the state and reward by specifying\n",
    "\n",
    "$p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a)$ (<u>a note about the meaning of this, below</u>)\n",
    "\n",
    "for each possible $s', r, s, \\text{and } a$. These conditional probabilities are said to specify the **one-step dynamics** of the environment. <br>\n",
    "(**my note: in my mind, i can call** $s' \\text{as } s^{next}$ , so that i understand it better)\n",
    "\n",
    "#### An Example\n",
    "\n",
    "<img src=\"img/6_2_14_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "Let's return to the case that $S_t = \\text{high}$, and $A_t = \\text{search}$\n",
    "\n",
    "- Then, when the environment responds to the agent at the next time step,\n",
    "\n",
    "    - with 70% probability, the next state is high and the reward is 4. In other words, <br>\n",
    "    $p(\\text{high}, 4|\\text{high},\\text{search}) = \\mathbb{P}(S_{t+1}=\\text{high}, R_{t+1}=4|S_{t} = \\text{high}, A_{t}=\\text{search}) = 0.7$\n",
    "        - (**my note: i can read this LHS of the previous line as follows: if the agent is at the state \"high\" and decided to take the action \"search\", what is the probability of transation to the next state \"high\" with a reward of 4 ? it is 0.7**)\n",
    "\n",
    "    - with 30% probability, the next state is low and the reward is 4. In other words, <br>\n",
    "    $p(\\text{low}, 4|\\text{high},\\text{search}) = \\mathbb{P}(S_{t+1}=\\text{low}, R_{t+1}=4|S_{t} = \\text{high}, A_{t}=\\text{search}) = 0.3$.\n",
    "    \n",
    "<br>    \n",
    "#### Now the quiz :\n",
    "\n",
    "- Question 1\n",
    "    - What is $p(\\text{high}, -3|\\text{low},\\text{search}$)?\n",
    "        - (the right answers are the following)\n",
    "            - 80% <br><br>\n",
    "        \n",
    "- Question 2\n",
    "    - What is $p(\\text{high}, 0|\\text{low},\\text{recharge}$)?\n",
    "        - (the right answers are the following)\n",
    "            - 1 <br><br>\n",
    "            \n",
    "- Questions 3 and 4      \n",
    "    - Consider the following probabilities:\n",
    "        - (1) $p(\\text{low}, 1|\\text{low},\\text{search}$)\n",
    "        - (2) $p(\\text{high}, 0|\\text{low},\\text{recharge}$)\n",
    "        - (3) $p(\\text{high}, 1|\\text{low},\\text{wait}$)\n",
    "        - (4) $p(\\text{high}, 1|\\text{high},\\text{wait}$)\n",
    "        - (5) $p(\\text{high}, 1|\\text{high},\\text{search}$) <br><br>\n",
    "        \n",
    "    - QUESTION 3 OF 4  \n",
    "        - Which of the above probabilities is equal to 0? (Select all that apply.) \n",
    "            - (the right answers are the following)\n",
    "                - 1, 3, and 5. <br><br>\n",
    "\n",
    "    - QUESTION 4 OF 4\n",
    "        - Which of the above probabilities is equal to 1? (Select all that apply.)\n",
    "            - (the right answers are the following)\n",
    "                - 2, and 4. <br><br>\n",
    "         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- About this line : $p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t = s, A_t=a)$ \n",
    "    - i don't know if it is a notation in the book or it is my lack of study of probabilty (i think it is the latter, so i must take an intensive course on probability), but anyway i did not understand the difference between the two sides of the equation, so i asked this question in the \"Knowledge\" section of udacity. <br>\n",
    "<img src=\"img/6_2_16_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/>  \n",
    "\n",
    "- here are the two links\n",
    "    - http://www.incompleteideas.net/book/first/ebook/node40.html\n",
    "    - https://www.utdallas.edu/~jjue/cs6352/markov/node3.html <br><br>\n",
    "    \n",
    "- from the links I figured out the following.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.17) MDPs, Part 3\n",
    "\n",
    "- Now that we've looked at an example, you should have the necessary intuition to understand the formal definition of the reinforcement learning framework. <br><br>\n",
    "\n",
    "- So, formally, a **Markov decision process** or **MDP** is **defined** by <br>\n",
    "<u>the set of states, the set of actions, and the set of rewards along with the one-step dynamics of the environment and the discount rate. </u>  <br>\n",
    "    - (note : i still need to know what is the MDP, and from where it got that name!!)\n",
    "        - read the wiki article : https://en.wikipedia.org/wiki/Markov_decision_process\n",
    "            - but as a summary : A Markov decision process (MDP) is a discrete time stochastic control process. It provides a mathematical framework for modeling decision making in situations where outcomes are <u>partly random</u> and <u>partly under the control of a decision maker</u>.\n",
    "    - also look at this link, which tells u the difference between RL and MDP : https://www.quora.com/What-is-the-main-difference-between-reinforcement-Learning-and-Markov-Decision-Process\n",
    "\n",
    "<img src=\"img/6_2_17_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> \n",
    "\n",
    "- We've detail the states, actions, rewards, and one-step dynamics of the environment, but we will also need to talk about the discount rate. And towards this end, **it is important to notice that we've detailed a <u>continuing task</u>**. So, it will prove useful to make the discount factor less than one because otherwise, the agent would have to look infinitely far into the limitless future.  <br><br>\n",
    "\n",
    "- It's common to set the discount rate to **0.9** And that feels like a good choice here. <br><br>\n",
    "\n",
    "- Throughout this course, you'll have the opportunity and your implementations to build some intuition for how to set the discount rate. But **it's important to note now that the discount rate is always set to some number much closer to one than to zero. Otherwise, the agent becomes excessively short-sighted to a fault.** <br><br>\n",
    "\n",
    "- And now, you have fully specified your first MDP. In general, when you have a real world problem in mind, you will need to specify the MDP and that will fully and formally define the problem that you want to your agent to solve. <br><br>\n",
    "\n",
    "- This framework works for continuing and episodic tasks <br><br>\n",
    "\n",
    "- whenever you have a problem that you want to solve with reinforcement learning, whether it entails a self-driving car, a walking robot, or a stock trading agent, this is the framework that you will use. <br><br>\n",
    "\n",
    "- The agent will know the states and actions along with the discount factor. As for the set up rewards and the one-step dynamics, those specify how the environment works and will be unknown to the agent. Despite not having this information, the agent will still have to learn from interaction how to accomplish its goal.\n",
    "\n",
    "<img src=\"img/6_2_17_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.18) Finite MDPs\n",
    "- (my note: this concpet is a reading about the openAi Gym, and CartPole-v0, and Finite MDPs) <br><br>\n",
    "\n",
    "- Every environment comes with first-class `Space` objects that describe the valid actions and observations.\n",
    "    - The `Discrete` space allows a fixed range of non-negative numbers.\n",
    "    - The `Box` space represents an n-dimensional box, so valid actions or observations will be an array of n numbers.\n",
    "    \n",
    "- in the observation (state) space of the CartPole-v0, you will notice that each in the array can be any real number, thus the state space $\\mathcal{S}^+$ is infinite!\n",
    "    - but regarding the **Finite MDPs** :\n",
    "        - Recall from the previous concept that in a finite MDP, the state space $\\mathcal{S}$ (or $\\mathcal{S}^+$, in the case of an episodic task) and action space $\\mathcal{A}$ must both be finite.\n",
    "            - Thus, while the CartPole-v0 environment does specify an MDP, it does not specify a **finite** MDP. In this course, we will first learn how to solve finite MDPs. Then, later in this course, you will learn how to use neural networks to solve much more complex MDPs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-2.19) Summary\n",
    "<img src=\"img/6_2_19_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> \n",
    "\n",
    "#### The Setting, Revisited\n",
    "- The reinforcement learning (RL) framework is characterized by an **agent** learning to interact with its **environment**.\n",
    "- At each time step, the agent receives the environment's **state** (the environment presents a situation to the agent), and the agent must choose an appropriate **action** in response. One time step later, the agent receives (1) a **reward** (the environment indicates whether the agent has responded appropriately to the state) and (2) a new **state**.\n",
    "- All agents have the goal to maximize expected **cumulative reward**, or the expected sum of rewards attained over all time steps.\n",
    "\n",
    "#### Episodic vs. Continuing Tasks\n",
    "- A **task** is an instance of the reinforcement learning (RL) problem.\n",
    "- **Continuing tasks** are tasks that continue forever, without end.\n",
    "- **Episodic tasks** are tasks with a well-defined starting and ending point.\n",
    "    - In this case, we refer to a complete sequence of interaction, from start to finish, as an **episode**.\n",
    "    - Episodic tasks come to an end whenever the agent reaches a **terminal state**.\n",
    "    \n",
    "#### The Reward Hypothesis\n",
    "- **Reward Hypothesis**: All goals can be framed as the maximization of (expected) cumulative reward.\n",
    "\n",
    "#### Goals and Rewards\n",
    "- (Please see **Part 1** and **Part 2** (my note: i think he means 6-2.13 and 6-2.14) to review an example of how to specify the reward signal in a real-world problem.)\n",
    "\n",
    "#### Cumulative Reward\n",
    "- The **return at time step** $t$ is $G_t := R_{t+1} + R_{t+2} + R_{t+3} + \\ldots$\n",
    "- The agent selects actions with the goal of maximizing expected (discounted) return. (Note: discounting is covered in the next concept.) (my note: i think, by \"next concept\" he means \"the next line\" )\n",
    "\n",
    "#### Discounted Return\n",
    "- The **discounted return at time step** $t$ is $G_t := R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots$.\n",
    "- The discount rate $\\gamma$ is something that you set, to refine the goal that you have the agent.\n",
    "    - It must satisfy $0 \\leq \\gamma \\leq 1$.\n",
    "    - If $\\gamma=0$, the agent only cares about the most immediate reward.\n",
    "    - If $\\gamma=1$, the return is not discounted.\n",
    "    - For larger values of $\\gamma$, the agent cares more about the distant future. Smaller values of $\\gamma$ result in more extreme discounting, where - in the most extreme case - agent only cares about the most immediate reward.\n",
    "    \n",
    "#### MDPs and One-Step Dynamics\n",
    "- The **state space** $\\mathcal{S}$ is the set of all (nonterminal) states.\n",
    "- In episodic tasks, we use $\\mathcal{S}^+$ to refer to the set of all states, including terminal states.\n",
    "- The action space $\\mathcal{A}$ is the set of possible actions. (Alternatively, $\\mathcal{A}(s)$ refers to the set of possible actions available in state $s \\in \\mathcal{S}$.)\n",
    "- (Please see **Part 2** (my note: i think he means 6-2.14) to review how to specify the reward signal in the recycling robot example.)\n",
    "- The **one-step dynamics** of the environment determine how the environment decides the state and reward at every time step. The dynamics can be defined by specifying $p(s',r|s,a) \\doteq \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_{t} = s, A_{t}=a)$.\n",
    "- A (finite) Markov Decision Process (MDP) is defined by:\n",
    "    - a (finite) set of states $\\mathcal{S}$ (or $\\mathcal{S}^+$, in the case of an episodic task)\n",
    "    - a (finite) set of actions $\\mathcal{A}$\n",
    "    - a set of rewards $\\mathcal{R}$\n",
    "    - the one-step dynamics of the environment\n",
    "    - the discount rate $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 3 : The RL Framework: The Solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reinforcement learning, agents learn to prioritize different decisions based on the rewards and punishments associated with different outcomes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (13 concepts)\n",
    "- 1- Introduction\n",
    "- 2- Policies\n",
    "- 3- Quiz: Interpret the Policy\n",
    "- 4- Gridworld Example\n",
    "- 5- State-Value Functions\n",
    "- 6- Bellman Equations\n",
    "- 7- Quiz: State-Value Functions\n",
    "- 8- Optimality\n",
    "- 9- Action-Value Functions\n",
    "- 10- Quiz: Action-Value Functions\n",
    "- 11- Optimal Policies\n",
    "- 12- Quiz: Optimal Policies\n",
    "- 13- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.1) Introduction\n",
    "- you've already learned about how to formulate a real-world problem so it can be solved with reinforcement learning in this lesson you'll begin to think about ways to **solve** this problem.\n",
    "\n",
    "- it's important to note that **this lesson is significantly more technical** than the previous one if this is initially uncomfortable to you don't worry and feel free to take the videos at your own pace \n",
    "\n",
    "- we've added extensive examples and quizzes to help you through this relatively difficult content but you'll likely still need to **take your own notes to make sure that the terminology becomes natural to you** \n",
    "\n",
    "- **you should think of reinforcement learning as a language that takes time to learn so don't be too hard on yourself** with all of this in mind let's get started\n",
    "---\n",
    "This lesson covers material in **Chapter 3** (especially 3.5-3.6) of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.2) Policies\n",
    "- We've seen that we use a Markov decision process or MDP as a formal definition of the problem that we'd like to solve with reinforcement learning. <br><br>\n",
    "\n",
    "- **In this video, we specify a formal definition for the <u>solution to this problem</u>.** \n",
    "\n",
    "- We can start to think of the solution as a series of actions that need to be learned by the agent towards the pursuit of a goal. \n",
    "    - For instance, in order to walk, the humanoid robot needs to learn the appropriate way to apply forces to its joint. But as we've seen, the correct action changes the situation. If a robot encounters a wall, the best series of actions will be different than if it had nothing blocking its path. \n",
    "    - Reward is always decided in the context of the state that it was decided in along with the state that follows. With this in mind, as long as the agent learns an appropriate action response to any environment state that it can observe, we have a solution to our problem. \n",
    "    - This motivates the idea of a policy. <br><br>\n",
    "    \n",
    "- The simplest kind of policy is a mapping from the set of environment states to the set of possible actions. \n",
    "<img src=\"img/6_3_2_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "- In case you're new to the idea of a mapping, you can think of a policy as a factory that takes any environment state as input and outputs the corresponding action that the agent will take. \n",
    "    - If the agent wants to keep track of its strategy, all it needs to do is to build this factory or to specify this mapping.  <br><br>\n",
    "\n",
    "- We call this kind of policy a deterministic policy. \n",
    "<img src=\"img/6_3_2_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "- Income is the state, outcome is the action to take. \n",
    "    - And as you can see, it's most common to denote the policy with the Greek letter pi. <br><br><br>\n",
    "    \n",
    "- Another type of policy that we we'll examine is a stochastic policy. The stochastic policy will allow the agent to choose actions randomly. \n",
    "<img src=\"img/6_3_2_3.png\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "- We define a stochastic policy as a mapping that accepts an environment state S and action A and returns the probability that the agent takes action A while in state S. \n",
    "<img src=\"img/6_3_2_4.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "- For clarity, let's revisit the recycling robot example from the previous lesson. The deterministic policy would specify something like whenever the battery is low, recharge it. And whenever the battery has a high amount of charge, search for cans. The stochastic policy does something more like whenever the battery is low, recharge it with 50 percent probability, wait where you are with 40 percent probability. And otherwise, search for cans. Whenever the battery is high, search for cans with 90 percent probability. And otherwise, wait for a can. \n",
    "<img src=\"img/6_3_2_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "- my note here At $02:36$ :\n",
    "    - I want to make it clear by differentiate between these probabalities (Stocastic Policy) and the probabilities that are on the graph. I will make an example\n",
    "        - the stocastic probabilities \"$\\pi (search|high) = 0.9 $\" and \"$\\pi (wait|high) = 0.1 $\" means that if the state is high, you have 0.9 <u>**probability to do the \"action\":search and 0.1 probability to do the (other) \"action\":wait**</u> (note that they add up to one)\n",
    "        - on the other hand, the probabilities on the graph \"say the .7 and .3 below & below-left-corner respectively\" these are the <u>**probabilities of going to a particular next state given that you took the action \"a\"**</u> (here it is search) . (note that those also add up to 1) \n",
    "        - as a summary, \n",
    "            - the probabilities that are on the RHS of the pictures are $\\pi(a|s^{current})$ , but the probabilities on the graph are $p(s^{next},r|s^{current},a)$\n",
    "            - also, the first probability has a special name : policy (here stochastic policy), but the second probability is named : the one-step-dynamic, which specifies the transations<br><br>\n",
    "\n",
    "- It's important to note that any deterministic policy can be expressed using the same notation that we generally reserve for a stochastic policy. For instance, this policy can be expressed as, whenever the battery is low, recharge it with 100 percent probability. Whenever the battery has a high amount of charge, search for cans with 100 percent probability  <br><br>\n",
    "<img src=\"img/6_3_2_6.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- we will explore policies with varying levels of randomness or stochasticity in this course. Now, at this point you might be wondering. \n",
    "- Now that we know how to specify a policy, what steps can we take to make sure that the agent's policy is the best one. We will work towards answering this question in the next few concepts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.3) Quiz: Interpret the Policy\n",
    "- From the looking at the quiz i noticed something and I also have a question:\n",
    "    - consider the notation for deterministic policy $π:S \\rightarrow A$  and stochastic policy $π:S \\times A \\rightarrow [0,1]$\n",
    "        -  note that the first formula mapps from  $S$ to $A$ , while the second one maps from $S \\times A$ to $[0,1]$ (I stress on this last mapping because I mistakenly thought that it maps from S to A via an operator x !!)\n",
    "- now here comes a question, what does $S \\times A$ mean ?     \n",
    "    - is it a cartesian product ? https://en.wikipedia.org/wiki/Cartesian_product\n",
    "        - even if it is like this, why do we do cartesian product?\n",
    "        - i asked on the ai group and found that :\n",
    "---\n",
    "- A **policy** determines how an agent chooses an action in response to the current state. In other words, it specifies how the agent responds to situations that the environment has presented. <br><br>\n",
    "\n",
    "- Consider the recycling robot MDP from the previous lesson.\n",
    "<img src=\"img/6_2_14_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "#### Deterministic Policy: Example\n",
    "- An example deterministic policy $\\pi: \\mathcal{S}\\to\\mathcal{A}$ can be specified as:\n",
    "    - $\\pi(low)=recharge$\n",
    "    - $\\pi(high)=search$\n",
    "- In this case,     \n",
    "    - if the battery level is low, the agent chooses to recharge the battery.\n",
    "    - if the battery level is high, the agent chooses to search for cans.\n",
    "    \n",
    "#### Question 1    \n",
    "- Consider a different deterministic policy $\\pi: \\mathcal{S}\\to\\mathcal{A}$, where:\n",
    "    - $\\pi(low)=search$\n",
    "    - $\\pi(high)=search$ <br><br>\n",
    "    \n",
    "- QUESTION 1 OF 2    \n",
    "    - Which of the following statements are true, if the agent follows the policy? (Select all that apply.)\n",
    "        - (the right answers are the following)\n",
    "            - If the state is *low*, the agent chooses the action *search*.\n",
    "            - The agent will always *search* for cans at every time step (whether the battery level is *low* or *high*).<br><br>\n",
    "            \n",
    "#### Stochastic Policy: Example            \n",
    "- An example stochastic policy $\\pi: \\mathcal{S}\\times\\mathcal{A}\\to [0,1]$ can be specified as:\n",
    "    - $\\pi(recharge∣low)=0.5$\n",
    "    - $\\pi(wait∣low)=0.4$\n",
    "    - $\\pi(search∣low)=0.1$\n",
    "    - $\\pi(search∣high)=0.9$\n",
    "    - $\\pi(wait∣high)=0.1$\n",
    "- In this case,    \n",
    "    - if the battery level is *low*, the agent *recharges* the battery with 50% probability, *waits* for cans with 40% probability, and *searches* for cans with 10% probability.\n",
    "    - if the battery level is *high*, the agent searches for cans with 90% probability and *waits* for cans with 10% probability.\n",
    "    \n",
    "#### Question 2\n",
    "- Consider a different stochastic policy $\\pi: \\mathcal{S}\\times\\mathcal{A}\\to [0,1]$, where:\n",
    "    - $\\pi(recharge∣low)=0.3$\n",
    "    - $\\pi(wait∣low)=0.5$\n",
    "    - $\\pi(search∣low)=0.2$\n",
    "    - $\\pi(search∣high)=0.6$\n",
    "    - $\\pi(wait∣high)=0.4$ <br><br>\n",
    "    \n",
    "- QUESTION 2 OF 2    \n",
    "    - Which of the following statements are true, if the agent follows the policy? (Select all that apply.)\n",
    "        - (the right answers are the following)\n",
    "            - If the battery level is *high*, the agent chooses to *search* for a can with 60% probability, and otherwise *waits* for a can.\n",
    "            - If the battery level is *low*, the agent is most likely to decide to *wait* for cans<br><br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.4) Gridworld Example\n",
    "- To understand how to go about searching for the best policy, it will help to have a running example. <br><br>\n",
    "\n",
    "- So consider this very very small world and an agent who lives in it. \n",
    "- Say the world is primarily composed of nice patches of grass, but two out of the nine locations in the world have large mountains. \n",
    "- Well think of each of these nine possible locations in the world as states and the environment. \n",
    "- At each point in time, let's say the agent can only move up, down, left or right, and can only take actions that lead it to not fall off the grid. \n",
    "    - Here, the arrows show the possible movements that will allow the agent. \n",
    "- Let's also say, the goal of the agent is to get to the bottom right hand corner of the world as quickly as possible. \n",
    "    - Then we'll think of this as an episodic task where an episode finishes when the agent reaches the goal. So we won't have to worry about transitions away from this goal state. \n",
    "- Furthermore, say that the agent receives a reward of negative one for most transitions. But if an action leads it to encounter a mountain, it receives some reward of negative three. And if it reaches the goal state, it gets a reward of five. So we can think of the reward signal as punishing the agent for every timestep that it spends away from the goal. You can think of the mountains as having a special larger punishment because they take even longer to cross than the patches of grass. \n",
    "    - The reward structure encourages the agent to get to the goal as quickly as possible. And when it reaches the goal, it gets a reward of five, and the episode ends. We'll use this example in our search for the best policy. <br><br>\n",
    "\n",
    "- (my note : can u say again the what are the states, actions, and rewards for this example? :D do it to make sure that u have understanded the example correctly)\n",
    "\n",
    "<img src=\"img/6_3_4_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.5) State-Value Functions\n",
    "\n",
    "- So we're working with this grid world example and looking for the best policy that leads us to a goal state as quickly as possible. <br><br> \n",
    "\n",
    "- So, let's start with a very, very bad policy so that we can understand why it's bad, and then work to improve it. \n",
    "    - Specifically, we'll look at a policy where the agent visits every state in this very roundabout manner, and we can ignore the transition that the agent will never take under the policy. So now, towards understanding why this policy is bad, let's calculate the cumulative reward that will result. If the agent starts in the top left corner of the world and follows this policy to get to the goal state, it just collects all of the reward along the way. So that's negative one plus negative one, plus negative one again, and so on, where if I add up all the rewards along the way, I get negative six. Let's say we're not discounting or that the discount rate is one. \n",
    "    \n",
    "<img src=\"img/6_3_5_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "    \n",
    "- _    \n",
    "    - we'll keep track of this negative six and remember, that it represents the fact that if we start at the state at the top left corner, and then just follow the policy for all time steps, that results in a return of negative six. \n",
    "    \n",
    "<img src=\"img/6_3_5_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "    \n",
    "- _    \n",
    "    - But now, say, instead the agent started one location over to the right. Then, what return would be likely to follow under the same policy? Again, we just sum up all the rewards that the agent receives along the way, and when we do that, we get a return of negative five, and let's also keep track of that. We can continue and do this for every state in the world. It makes sense to think of the goal state as resulting in the return of zero. After all, if the agent starts at the goal the episode ends immediately and no reward is received. \n",
    "    \n",
    "<img src=\"img/6_3_5_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- In this way, no matter where the agent starts in the world, we have a way of keeping track of the return that follows. This way of analyzing this horrible policy will help us to improve it. But before we get into exactly how to do that, let's attach a bit of notation and terminology to this process we just followed. \n",
    "    - You can think of this grid of numbers as a function of the environment state. \n",
    "    - For each state, it has a corresponding number, and we refer to this function as the state-value function. \n",
    "    - For each state, it yields the return that's likely to follow if the agent starts in that state and then follows the policy for all time steps. \n",
    "\n",
    "<img src=\"img/6_3_5_4.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- _\n",
    "   - but it's more common to see it equivalently expressed but with a bit more notation. Before I show you that notation, I warn you that it looks a bit complicated, but it's equivalent to what we've already discussed. \n",
    "   - And here it is. \n",
    "\n",
    "<img src=\"img/6_3_5_5.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- _   \n",
    "   - The state-value function for a policy $\\pi$ is a function of the environment state. \n",
    "   - For each state s, it tells us the expected discounted return, if the agent started in that state s, and then use the policy to choose its actions for all time steps\n",
    "   - the state value function will always correspond to a particular policy. So if we change the policy, we change the state-value function, \n",
    "   - we typically denote the function (state-value function) with the lowercase v with the corresponding policy in the subscript.\n",
    "\n",
    "---\n",
    "Note #1: The notation $\\mathbb{E}_\\pi[\\cdot]$ is borrowed from the suggested textbook. $\\mathbb{E}_\\pi[\\cdot]$ is defined as the expected value of a random variable, given that the agent follows policy $\\pi$. \n",
    "\n",
    "Q\\ since the state-value func is definded as a probability, does that mean that the state-value function can be stochastic ? (not deterministic as the previous example)\n",
    "\n",
    "Note #2: In this course, we will use \"return\" and \"discounted return\" interchangably. For an arbitrary time step tt, both refer to $G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$, where $\\gamma \\in [0,1]$. In particular, when we refer to \"return\", it is not necessarily the case that $\\gamma = 1$, and when we refer to \"discounted return\", it is not necessarily true that $\\gamma < 1$. (This also holds for the readings in the recommended textbook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.6) Bellman Equations\n",
    "- If you take the time yourself to calculate the value function for this policy, you might notice that you don't need to start your calculations from scratch every time. In particular, you don't need to look at the first state then add up all the rewards along the way. Then look at the second state, add up all those rewards. Then the third, add up all those and so on. It turns out to be redundant effort. \n",
    "\n",
    "<img src=\"img/6_3_6_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- Instead there's something much faster that you can do. So, let's erase most of these values with the exception of the ones at the bottom. And let's see how we might work backwards to recalculate those values. And along the way we'll discover that the value function has a nice recursive property. \n",
    "\n",
    "<img src=\"img/6_3_6_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- To see that, let's say we're trying to calculate the value of the state that I've highlighted here. \n",
    "\n",
    "<img src=\"img/6_3_6_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- And, the value of that state is just the sum of rewards that follows until we reach the terminal state. And in this case, the agent starts in the state, then follows the policy, gets a reward of negative one, and lands at this next state with the value of two. And what's important to notice here is that this two already corresponds to the sum of all the rewards that follow all the way to the end. So, instead of recalculating that sum, we could just use that value of two to get the value of the state as negative one plus two or one. \n",
    "\n",
    "<img src=\"img/6_3_6_4.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- For the same reason, the value of the next state is negative one plus one or zero, then negative three plus zero or negative three and so on. \n",
    "- In this way, **we see that we can express the value of any state as the sum of the immediate reward plus the value of the state that follows.** \n",
    "\n",
    "<img src=\"img/6_3_6_5.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- **And what's important to note, is that for simplicity, I set the discount rate in this example to one but in general we want to have a framework that takes discounting into account.** So, we'll need to use the discounted value of the state that follows. \n",
    "\n",
    "<img src=\"img/6_3_6_6.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- We can express this idea in terms of what's known as the Bellman expectation equation where for a general MDP we have to calculate **the expected value of the sum.** This is because, in general, with more complicated worlds, the immediate reward and next state cannot be known with certainty. \n",
    "\n",
    "<img src=\"img/6_3_6_7.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- This equation is very important and we'll use it extensively in future lessons. **But for now, all that's important to remember is the main idea. And that idea is, we can express the value of any state in the MDP in terms of the immediate reward and the discounted value of the state that follows.**\n",
    "\n",
    "<img src=\"img/6_3_6_8.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~ Reading  after the video~\n",
    "\n",
    "---\n",
    "before the reading, here are my questions in the reading below : \n",
    "- Q\\ there is a possible error at \"n the event that the agent's policy $\\pi$ is deterministic, the agent selects action $\\pi(s)$\"  i think it should be \" action a(s)\" right?\n",
    "- بس من الكويز اللى بعده أنا شفت حاجة خلتنى أشك إن <br>\n",
    "$\\pi(s)$ is an action ?!!  <br>\n",
    "ولا أنا كدا لخبطت كله فى بعضه؟\n",
    "     - $\\checkmark \\; \\;$  no i am right, recall that the deterministic policy is the mapping $\\pi: \\mathcal{S}\\to\\mathcal{A}$\n",
    "     \n",
    "- Q\\ in \"Bellman Expectation Equation\", the term $R_{t+1}$, does it refere to immediate reward or next reward ?\n",
    "    - $\\checkmark \\; \\;$  i think it is confusing, but $R_{t+1}$ is the immediate reward !!\n",
    "---\n",
    "\n",
    "#### Bellman Equations\n",
    "- In this gridworld example, once the agent selects an action,\n",
    "    - it always moves in the chosen direction (contrasting general MDPs where the agent doesn't always have complete control over what the next state will be), and\n",
    "    - the reward can be predicted with complete certainty (contrasting general MDPs where the reward is a random draw from a probability distribution). <br><br>\n",
    "    \n",
    "- In this simple example, we saw that the value of any state can be calculated as the sum of the immediate reward and the (discounted) value of the next state. <br><br>\n",
    "\n",
    "- Alexis mentioned that for a general MDP, we have to instead work in terms of an expectation, since it's not often the case that the immediate reward and next state can be predicted with certainty. Indeed, we saw in an earlier lesson that the reward and next state are chosen according to the one-step dynamics of the MDP. In this case, where the reward $r$ and next state $s'$ are drawn from a (conditional) probability distribution $p(s',r|s,a)$, the **Bellman Expectation Equation (for** $v_\\pi$**)** expresses the value of any state ss in terms of the expected immediate reward and the expected value of the next state:\n",
    "\n",
    "\\begin{equation*}\n",
    "v_\\pi(s)=E_\\pi[R_{t+1}+\\gamma v_\\pi(S_{t+1})∣S_t=s].\n",
    "\\end{equation*}\n",
    "\n",
    "#### Calculating the Expectation\n",
    "In the event that the agent's policy $\\pi$ is **deterministic**, the agent selects action $\\pi(s)$ when in state $s$, and the Bellman Expectation Equation can be rewritten as the sum over two variables ($s'$ and $r$):\n",
    "\n",
    "\\begin{equation*}\n",
    "v_\\pi(s) =\\sum_{s'\\in\\mathcal{S}^+, \\; r\\in\\mathcal{R}}p(s',r|s,\\pi(s))(r+\\gamma v_\\pi(s'))\n",
    "\\end{equation*}\n",
    "\n",
    "- In this case, we multiply: \n",
    "    - the sum of the reward and discounted value of the next state, (i.e $(r+\\gamma v_\\pi(s'))$) by \n",
    "    - its corresponding probability $p(s',r|s,\\pi(s))$ and \n",
    "- sum over all possibilities to yield the expected value.\n",
    "    - (**my note** : the action of multiplying the probability $p$ by the $(r+\\gamma v_\\pi(s'))$ is what defines the \"expected value\", since expected value means that there is a probability, (do not confuse this with $\\pi$ in this case b/c it is **deterministic** and we will see the equation shortly when it is **stochastic**)\n",
    "    - summing over all possibilities (i.e $\\sum_{s'\\in\\mathcal{S}^+, \\; r\\in\\mathcal{R}}$ ) is because, unlike the example in of the grid world where the value function depends only on the next state and immediate reward, here, the more general is that the value function will depend on all the possible available next states (with thier corresponding immediate reward). so it is also a probability\n",
    "        - this (if my conclusion is right) may asnwer a questio of me earlier , when i said \"can $v_\\pi(s)$ be stochastic\"\n",
    "        - again do not confuse $v_\\pi(s)$ which is in general stochastic (can be deterministic like in the grid world example) , and $\\pi$ which is also in general stochastic but can be deterministic \n",
    "    \n",
    "    \n",
    "\n",
    "If the agent's policy $\\pi$ is **stochastic**, the agent selects action aa with probability $\\pi(a|s)$ when in state $s$, and the Bellman Expectation Equation can be rewritten as the sum over three variables ($s'$, $r$, and $a$):\n",
    "\n",
    "\\begin{equation*}\n",
    "v_\\pi(s) =  \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R},a\\in\\mathcal{A}(s)}\\pi(a|s)p(s',r|s,a)(r+\\gamma v_\\pi(s'))\n",
    "\\end{equation*}\n",
    "\n",
    "- In this case, we multiply the sum of the reward and discounted value of the next state $(r+\\gamma v_\\pi(s'))$ by its corresponding probability $\\pi(a|s)p(s',r|s,a)$ and sum over all possibilities to yield the expected value.\n",
    "    - **my note**: here again u notice that when we convert a variable from deterministic to stochastic, we leave the variable in its place and multiply by its probability (here we introduced the multiplication , $\\pi(a|s)$, that represents a probability)\n",
    "\n",
    "#### There are 3 more Bellman Equations!\n",
    "- In this video, you learned about one Bellman equation, but there are 3 more, for a total of 4 Bellman equations.\n",
    "\n",
    "- **All of the Bellman equations attest to the fact that value functions satisfy recursive relationships.**\n",
    "    - For instance, the **Bellman Expectation Equation (for** $v_\\pi$**)** shows that it is possible to relate the value of a state to the values of all of its possible successor states.\n",
    "\n",
    "After finishing this lesson, you are encouraged to read about the remaining three Bellman equations in sections 3.5 and 3.6 of the textbook. The Bellman equations are incredibly useful to the theory of MDPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.7) Quiz: State-Value Functions\n",
    "In this quiz, you will calculate the value function corresponding to a particular policy.\n",
    "\n",
    "Each of the nine states in the MDP is labeled as one of $\\mathcal{S}^+ = \\{s_1, s_2, \\ldots, s_9 \\}$, where $s_9$ is a terminal state.\n",
    "\n",
    "Consider the (deterministic) policy that is indicated (in orange) in the figure below.\n",
    "\n",
    "<img src=\"img/6_3_7_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "- The policy $\\pi$ is given by:\n",
    "    - $\\pi(s1) = right$\n",
    "    - $\\pi(s2) = right$\n",
    "    - $\\pi(s3) = down$\n",
    "    - $\\pi(s4) = up$\n",
    "    - $\\pi(s5) = right$\n",
    "    - $\\pi(s6) = down$\n",
    "    - $\\pi(s7) = right$\n",
    "    - $\\pi(s8) = right$\n",
    "- Recall that since $s_9$ is a terminal state, the episode ends immediately if the agent begins in this state. So, the agent will not have to choose an action (so, we won't include $s_9$ in the domain of the policy), and $v_\\pi(s_9) = 0$ Take the time now to calculate the state-value function $v_\\pi$ that corresponds to the policy. (You may find that the Bellman expectation equation saves you a lot of work!)\n",
    "\n",
    "Assume $\\gamma = 1$.\n",
    "\n",
    "Once you have finished, use $v_\\pi$ to answer the questions below.  \n",
    "\n",
    "- my solution :\n",
    "    - $v_\\pi(s_9) = 0$\n",
    "    - $v_\\pi(s_8) = 5 + v_\\pi(s_9) = 5 + 0 = 5$\n",
    "    - $v_\\pi(s_7) = -3 + v_\\pi(s_8) = -3 + 5 = 2$\n",
    "    - $v_\\pi(s_6) = 5 + v_\\pi(s_9) = 5 + 0 = 5$\n",
    "    - $v_\\pi(s_5) = -1 + v_\\pi(s_6) = -1 + 5 = 4$\n",
    "    - $v_\\pi(s_5) = -1 + v_\\pi(s_6)  = -1 + 5 = 4$ ... note, i can't calc $v_\\pi(s_4)$ now. it can be calculated after knowing $v_\\pi(s_1)$\n",
    "    - $v_\\pi(s_3) = -1 + v_\\pi(s_6) = -1 + 5 = 4$ \n",
    "    - $v_\\pi(s_2) = -1 + v_\\pi(s_3) = -1 + 4 = 3$\n",
    "    - $v_\\pi(s_1) = -1 + v_\\pi(s_2) = -1 + 3 = 2$\n",
    "    - $v_\\pi(s_4) = -1 + v_\\pi(s_1) = -1 + 2 = 1$\n",
    "\n",
    "#### Question 1\n",
    "- QUESTION 1 OF 3\n",
    "    - What is $v_\\pi(s_4)$?\n",
    "        - (the right answers are the following)\n",
    "            - 1 <br><br>\n",
    "            \n",
    "#### Question 2\n",
    "- QUESTION 2 OF 3\n",
    "    - What is $v_\\pi(s_1)$?\n",
    "        - (the right answers are the following)\n",
    "            - 2 <br><br>       \n",
    "            \n",
    "#### Question 3\n",
    "- QUESTION 3 OF 3 \n",
    "    - Consider the following statements:\n",
    "        - (1) $v_\\pi(s_6) = -1 + v_\\pi(s_5)$ \n",
    "        - (2) $v_\\pi(s_7) = -3 + v_\\pi(s_8)$ \n",
    "        - (3) $v_\\pi(s_1) = -1 + v_\\pi(s_2)$\n",
    "        - (4) $v_\\pi(s_4) = -3 + v_\\pi(s_7)$ \n",
    "        - (5) $v_\\pi(s_8) = -3 + v_\\pi(s_5)$\n",
    "    - Select the statements (listed above) that are true\n",
    "        - (the right answers are the following)\n",
    "            - (2), and (3) <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.8) Optimality\n",
    "- So far in this lesson, we've looked at a particular policy $\\pi$, and calculated its corresponding value function. In the quiz, you calculated the value function corresponding to a different policy which we denoted by Pi-Prime, $\\pi'$. And if you look at each of these value functions, you may notice a pattern or trend. Take the time now to compare them and pause the video if you like.\n",
    "\n",
    "<img src=\"img/6_3_8_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "-  So in reality, there are probably a great number of patterns in these numbers, but the most relevant trend to us now is that when we look at any state in particular and compare the two value functions, the value function for a Pi-Prime is always bigger than or equal to the value function for policy Pi. \n",
    "\n",
    "<img src=\"img/6_3_8_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- For instance, 2 is greater than negative 6, 3 is greater than negative 5, 4 is greater than negative 4, and 1 is equal to 1. So this says, for any state in the environment, it's better to follow policy Pi-prime, right? Because no matter where the agent starts in the grid world, they expected discounted return is larger, and remember that the goal of the agent is to maximize return. So, a greater expected return makes for a better policy. \n",
    "    - This motivates an important definition. \n",
    "    \n",
    "- By definition, we say that a policy Pi-Prime is better than or equal to a policy Pi if it's state-value function is greater than or equal to that of policy Pi for all states. \n",
    "\n",
    "<img src=\"img/6_3_8_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- And so there are a couple of important things to note about this definition. The first is that if you take any two policies, it's not necessarily the case that you're going to be able to decide which is better. In other words, it's possible that they can't be compared. But said, there will always be at least one policy that's better than or equal to all other policies. We call this policy an optimal policy, and it's guaranteed to exist but it may not be unique.\n",
    "\n",
    "<img src=\"img/6_3_8_4.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- And it's important to note that an optimal policy is what the agent is searching for. It's the solution to the MDP and the best strategy to accomplish it's goal. <br><br>\n",
    "\n",
    "\n",
    "- Finally, all optimal policies have the same value function which we denote by Vstar, $v_*$. \n",
    "\n",
    "<img src=\"img/6_3_8_5.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- You might be wondering why it's not written, V sub Pi star $v_{pi *}$. The answer is by convention, and probably because it just looks nicer this way. On that note, it turns out that the policy from the quiz is actually an optimal policy. This is because if you compare it to the value function for any other possible policy, it's value function is always at least as big.\n",
    "\n",
    "<img src=\"img/6_3_8_6.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- but it's not the only optimal policy. For instance, here's another policy that has the same value function.\n",
    "\n",
    "<img src=\"img/6_3_8_7.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- and you might be wondering at this point how I found these optimal policies. Well, this example was simple enough that it was possible to make an educated guess just by staring at the dynamics, but this will often not be the case as many of the MDPs will look at will be far more complicated. So **how might we determine an optimal policy for a much more complicated MDP? To answer this question, we'll need to define another type of value function.** <br>\n",
    "- my note: notice the previous question that motivated us to introduce the Action-Value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.9) Action-Value Functions\n",
    "\n",
    "- So far we've been working with the state value function for a policy. For each state s, it yields the expected discounted return If the agent starts in state as and then uses the policy to choose its actions for all time steps. You've seen a few examples and know how to calculate the state value function corresponding to a policy. \n",
    "\n",
    "<img src=\"img/6_3_9_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- In this concept, we'll define a new type of value function known as the action-value function. \n",
    "    - This value function is denoted with a lowercase q instead of v. \n",
    "    - While the state values ($v_\\pi(s)$) are a function of the environment state, the action values ($q_\\pi(s,a)$) are a function of the environment state and the agent's action. \n",
    "    - For each state s and action a, the action value function yields the expected discounted return If the agent starts in state s then chooses action a and then uses a policy to choose its actions for all future time steps. \n",
    "    \n",
    "<img src=\"img/6_3_9_2.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>    \n",
    "    \n",
    "- Just like with the state value function, it will help your intuition If you calculate this yourself. <br><br>\n",
    "\n",
    "- In the case of the state value function, we kept track of the value of each state with the number on the grid. \n",
    "- We'll do something similar with the action-value function, where we now need up to four values for each state, each corresponding to a different action. \n",
    "\n",
    "<img src=\"img/6_3_9_3.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>  \n",
    "\n",
    "- These four numbers correspond to the same state but the one on top corresponds to action up; the one on the right, corresponds to moving right before following the policy and so on. You'll see this soon. \n",
    "\n",
    "- So with the exception of the terminal state, I've broken up the figure to leave a space for keeping track of the value corresponding to each possible state and action. \n",
    "\n",
    "<img src=\"img/6_3_9_4.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>  \n",
    "\n",
    "- let's see if we can calculate some action values. We'll begin with the state here. Let's calculate the value corresponding to this state (she highlighted the top-left state) and action down. So the agent starts in this state, takes action down and receives a reward of negative one. \n",
    "- Then for every time step in the future, it just follows the policy until it reaches the terminal state. We can then add up all the rewards that it encountered along the way and when we do that, we get zero. So this zero corresponds to the action value for this state where the agent started and action down. \n",
    "\n",
    "<img src=\"img/6_3_9_5.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br> \n",
    "\n",
    "- Let's try calculating another actual value, this time corresponding to this state (she highlighted the left-middle state) and action up. So the agent starts in the state and takes action up, and it received a reward of negative one. Then it just follows a policy for all future time steps. We add up the reward it collected along the way and this yields a cumulative reward of one. So this one corresponds to the action value for this state and action up. \n",
    "\n",
    "<img src=\"img/6_3_9_6.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br> \n",
    "\n",
    "- So you can continue and do this for every state-action pair that makes sense. And when you do this, you get this action-value function. I highly encourage you to calculate and check these values yourself. \n",
    "\n",
    "<img src=\"img/6_3_9_7.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- Before moving on, it's important to revisit some information that you learned earlier. Remember that we have the notation v_star, $v_*$, to refer to the optimal state value function. Similarly, we'll refer to the optimal action value function as q_star, $q_*$. <br>\n",
    "Q\\ what is the definition of the optimal action value function??!\n",
    "\n",
    "<img src=\"img/6_3_9_7.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "---\n",
    "Note: In this course, we will use \"return\" and \"discounted return\" interchangably. For an arbitrary time step tt, both refer to $G_t \\doteq R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\ldots = \\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}$, where $\\gamma \\in [0,1]$. In particular, when we refer to \"return\", it is not necessarily the case that $\\gamma = 1$, and when we refer to \"discounted return\", it is not necessarily true that $\\gamma < 1$. (This also holds for the readings in the recommended textbook.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.10) Quiz: Action-Value Functions\n",
    "\n",
    "<img src=\"img/6_3_10_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- **True or False?**: For a deterministic policy $\\pi$, <br> \n",
    " $v_\\pi(s) = q_\\pi(s, \\pi(s))$ <br>\n",
    " holds for all $s \\in S$ <br>\n",
    " (Feel free to use the state-value and action-value functions (for an example deterministic policy) above to answer this question.)\n",
    "    - (the right answers are the following)\n",
    "        - True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.11) Optimal Policies\n",
    "- Q on the video\\, \n",
    "    - i don't know what exactly is the \"optimal action value function\" $q_*$ ? from what i see, all states have several action values! so where is this optimality ? \n",
    "        - this may be explained later, so if she don't i have to search online, and ask on the ai group\n",
    "        - as she said at the end of the video, getting the optimal value function is what we will explore in the remainder in this course.<br><br>\n",
    "    \n",
    "- Several concepts ago, I've mentioned that we needed to define the action value function before talking about how the agent could search for an optimal policy, and we will see most of the detail for a later lesson. The main idea is this. \n",
    "    - The agent interacts with the environment. And from that interaction, it estimates the optimal action value function. Then, the agent uses that value function to get the optimal policy. \n",
    "    \n",
    "<img src=\"img/6_3_11_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>    \n",
    "    \n",
    "- So this might all seem quite strange, but it will become much clearer in the next lesson when you implement this process yourself. So for now, please bear with me and let's further ignore the question of how the agent uses its experience to estimate the value function. In particular, let's assume it already knows the optimal action value function, but it doesn't know the corresponding optimal policy. **So how does it get the optimal policy? This is what we'll explore in this video.** <br><br>\n",
    "\n",
    "\n",
    "- So we already have the optimal action value function and you've seen some of the optimal policies already, but I've removed those hints here, so let's try to reconstruct an optimal policy from the value function. \n",
    "\n",
    "<img src=\"img/6_3_11_2.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- It's possible to show that for each state, we just need to pick the action that yields the highest expected return. So beginning with the state in the top left corner, the policy will go right instead of down since 2 is larger than zero. Moving right, we see two values of 1 and one value of 3. 3 is the largest value into the policy, we'll go right here. And we can continue in this way, always picking the action with the highest value. So 4 is greater than 2, 5 is higher than 1 or 3. Next, 4 is the largest (my note, she highlighted the center square). We'll skip over the state with 3 values of 1 because it's not quite clear what to do here. But then 2 is larger than 0, and 5 is larger than 1. So now back to the state with three values of 1. It turns out that to construct the optimal policy, we have our choice here. The agent could go up down or right and all three choices would yield optimal policies. So let's just say the policy decides to go right. And just like that, we've arrived at an optimal policy \n",
    "\n",
    "<img src=\"img/6_3_11_3.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "- it's worth summarizing what we've noticed here. If the agent has the optimal action value function, it can quickly obtain an optimal policy, **which is the solution to the MDP that we're looking for.** **This brings us to the question of how the agent could find the optimal value function. This is in fact what we'll explore for the remainder of this course.**    \n",
    "\n",
    "<img src=\"img/6_3_11_4.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.12) Quiz: Optimal Policies\n",
    "- I don't understand the following \n",
    "    - \"To see why this should be the case, note that it must hold that $v_*(s) =\\max_{a\\in\\mathcal{A}(s)} $.\" \n",
    "    - In the event that there is some state $s\\in\\mathcal{S}$ for which multiple actions $a\\in\\mathcal{A}(s)$ maximize the optimal action-value function, you can construct an optimal policy by placing any amount of probability on any of the (maximizing) actions. You need only ensure that the actions that do not maximize the action-value function (for a particular state) are given 0% probability under the policy.\n",
    "\n",
    "#### my explanations\n",
    "\n",
    "- i want to differentiate between \n",
    "    - $\\pi_*(s) =\\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a) $ for all $s\\in\\mathcal{S}$.\n",
    "    - $v_*(s) =\\max_{a\\in\\mathcal{A}(s)} q_*(s,a) $\n",
    "    \n",
    "- but first, i want you to know the the difference between $\\max $ and $ \\arg\\max$\n",
    "- then we will see what is the domain and range for a multivariate function (function of more than one variable), and how to calc its $\\max$ and $\\arg\\max$, is it in 3d space, or in 2d by fixing one arg it varying the other, we will see. <br><br>\n",
    "\n",
    "- so, the difference between $\\max $ and $ \\arg\\max$ from stackoverflow :\n",
    "<img src=\"img/6_3_12_max.jpg\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>\n",
    "\n",
    "- now, in the previous two lines (that we want to compare them),  $\\max$ and $\\arg\\max$ are applied to the optimal state-value function, $q_*(s,a)$ , so for that function :\n",
    "    - domain (horizontal plane) : s, and a. (think of them as a horizontal plane that has two orthogonal axes : s and a)\n",
    "    - range (vertical axis) : $q_*(s,a)$ <br><br>\n",
    "    \n",
    "- now, when we say  $\\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a) $ for all $s\\in\\mathcal{S}$\n",
    "    - the parameter under the word \"$max$\" is \"$a$\" (ignore \"$\\in\\mathcal{A}$\" for now, it is just an aditional info). this means that we are going to get the argument \"$a$\" (i.e its value on the $a$ axis), when q(s,a) is maximum, and we will do that for each (for all) s (leave in S as it is an addiotional informarion). this corresponds to $a_1$, $a_2$, or $a_3$ in the table below<br><br>\n",
    "    \n",
    "- on the other hand, when we say $v_*(s) =\\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$\n",
    "    - the same as the previous demonstration, but now, instead of getting a1, a2, or a3, we get the valaue of $q_*(s,a)$ (the number inside the table below)\n",
    "    \n",
    "    \n",
    "---    \n",
    "#### now the quiz : optimal Policies\n",
    "\n",
    "If the state space $\\mathcal{S}$ and action space $\\mathcal{A}$ are finite, we can represent the optimal action-value function $q_*$ in a table, where we have one entry for each possible environment state $s \\in \\mathcal{S}$ and action $a\\in\\mathcal{A}$.\n",
    "\n",
    "The value for a particular state-action pair $s,a$ is the expected return if the agent starts in state $s$, takes action $a$, and then henceforth follows the optimal policy $\\pi_*$.\n",
    "\n",
    "We have populated some values for a hypothetical Markov decision process (MDP) (where $\\mathcal{S}=\\{ s_1, s_2, s_3 \\}$ and $\\mathcal{A}=\\{a_1, a_2, a_3\\}$) below.\n",
    "\n",
    "<img src=\"img/6_3_12_1.png\" alt=\"Drawing\" style=\"width: 200px;\"/> <br>\n",
    "\n",
    "You learned in the previous concept that once the agent has determined the optimal action-value function $q_*$, it can quickly obtain an optimal policy $\\pi_*$ by setting $\\pi_*(s) = \\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$.\n",
    "\n",
    "To see why this should be the case, note that it must hold that $v_*(s) = \\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$.\n",
    "\n",
    "In the event that there is some state $s\\in\\mathcal{S}$ for which multiple actions $a\\in\\mathcal{A}(s)$ maximize the optimal action-value function, you can construct an optimal policy by placing any amount of probability on any of the (maximizing) actions. You need only ensure that the actions that do not maximize the action-value function (for a particular state) are given 0% probability under the policy.\n",
    "\n",
    "Towards constructing the optimal policy, we can begin by selecting the entries that maximize the action-value function, for each row (or state).\n",
    "\n",
    "<img src=\"img/6_3_12_2.png\" alt=\"Drawing\" style=\"width: 200px;\"/> <br>\n",
    "\n",
    "\n",
    "- Thus, the optimal policy $\\pi_*$ for the corresponding MDP must satisfy:\n",
    "    - $\\pi_*(s_1) = a_2$ .....  (or, equivalently, $\\pi_*(a_2| s_1) = 1$), and\n",
    "    - $\\pi_*(s_2) = a_3$ .....  (or, equivalently, $\\pi_*(a_3| s_2) = 1$).\n",
    "    \n",
    "This is because $a_2 = \\arg\\max_{a\\in\\mathcal{A}(s_1)}q_*(s_1,a)$, and $a_3 = \\arg\\max_{a\\in\\mathcal{A}(s_2)}q_*(s_2,a)$.\n",
    "\n",
    "In other words, under the optimal policy, the agent must choose action $a_2$ when in state $s_1$, and it will choose action $a_3$ when in state $s_2$.\n",
    "\n",
    "As for state $s_3$, note that $a_1$, $a_2$ $\\in \\arg\\max_{a\\in\\mathcal{A}(s_3)}q_*(s_3,a)$. Thus, the agent can choose either action $a_1$ or $a_2$ under the optimal policy, but it can never choose action $a_3$. \n",
    "- That is, the optimal policy $\\pi_*$ must satisfy:\n",
    "    - $\\pi_*(a_1| s_3) = p$,\n",
    "    - $\\pi_*(a_2| s_3) = q$, and\n",
    "    - $\\pi_*(a_3| s_3) = 0$,\n",
    "    \n",
    "where $p,q\\geq 0$, and $p + q = 1$.\n",
    "\n",
    "#### Question\n",
    "Consider a different MDP, with a different corresponding optimal action-value function. Please use this action-value function to answer the following question.\n",
    "\n",
    "<img src=\"img/6_3_12_3.png\" alt=\"Drawing\" style=\"width: 200px;\"/> <br>\n",
    "\n",
    "- **QUIZ QUESTION**\n",
    "    - Which of the following describes a potential optimal policy that corresponds to the optimal action-value function?\n",
    "        - (the right answers are the following)\n",
    "            - The agent always selects action $a_3$ in state $s_1$.\n",
    "            - The agent is free to select either action $a_1$ or action $a_2$ in state $s_2$.\n",
    "            - The agent must select action $a_1$ in state $s_3$.\n",
    "        - (wrong answers)\n",
    "            - The agent selects action $a_1$ in state $s_1$\n",
    "            - The agent must select action $a_3$ in state $s_2$.\n",
    "            - The agent is free to select either action $a_2$ or $a_3$ in state $s_3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-3.13) Summary\n",
    "- my note : note that the \"Bellman expectation equation\" is a recursive equation. (i stress on that b/c for a moment i thought that it only calculates the state-value for two terms only)\n",
    "    - actually it does, if u start from the target (terminal state) and stepped back to the starting states\n",
    "---\n",
    "#### Summary\n",
    "\n",
    "<img src=\"img/6_3_13_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "#### Policies\n",
    "- A **deterministic policy** is a mapping $\\pi: \\mathcal{S}\\to\\mathcal{A}$. For each state $s\\in\\mathcal{S}$, it yields the action $a\\in\\mathcal{A}$ that the agent will choose while in state $s$.\n",
    "- A **stochastic policy** is a mapping $\\pi: \\mathcal{S}\\times\\mathcal{A}\\to [0,1]$. For each state $s\\in\\mathcal{S}$ and action $a\\in\\mathcal{A}$, it yields the probability $\\pi(a|s)$ that the agent chooses action $a$ while in state $s$.\n",
    "\n",
    "#### State-Value Functions\n",
    "- The **state-value function** for a policy $\\pi$ is denoted $v_\\pi$. For each state $s \\in\\mathcal{S}$, it yields the expected return if the agent starts in state $s$ and then uses the policy to choose its actions for all time steps. That is, $v_\\pi(s) \\doteq \\text{} \\mathbb{E}_\\pi[G_t|S_t=s]$. We refer to $v_\\pi(s)$ as the **value of state** $s$ **under policy** $\\pi$.\n",
    "- The notation $\\mathbb{E}_\\pi[\\cdot]$ is borrowed from the suggested textbook, where $\\mathbb{E}_\\pi[\\cdot]$ is defined as the expected value of a random variable, given that the agent follows policy $\\pi$.\n",
    "\n",
    "#### Bellman Equations\n",
    "- The **Bellman expectation equation for** $v_\\pi$ is: $v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t =s]$.\n",
    "    - **my note**: i want u to know the difference between the state-value function and the first bellman equation :\n",
    "        - the deifinition of the state-value function is illustrated above by few lines\n",
    "        - first bellman equation is a mathematical recursive way to calculate the state-value function. this equation defines the relationships between a given state (or state-action pair) to its successors (التالى) .\n",
    "\n",
    "#### Optimality\n",
    "- A policy $\\pi'$  is defined to be better than or equal to a policy $\\pi$ if and only if $v_{\\pi'}(s) \\geq v_\\pi(s)$ for all s\\in\\mathcal{S}.\n",
    "- An **optimal policy** $\\pi_*$ satisfies $\\pi_* \\geq \\pi$ for all policies $\\pi$. An optimal policy is guaranteed to exist but may not be unique.\n",
    "    - **my Q\\** where is the proof of that last claim ?\n",
    "- All optimal policies have the same state-value function $v_*$, called the **optimal state-value function**.\n",
    "\n",
    "#### Action-Value Functions\n",
    "- The **action-value** function for a policy $\\pi$ is denoted $q_\\pi$. For each state $s \\in\\mathcal{S}$ and action $a \\in\\mathcal{A}$, it yields the expected return if the agent starts in state $s$, takes action $a$, and then follows the policy for all future time steps. That is, $q_\\pi(s,a) \\doteq \\mathbb{E}_\\pi[G_t|S_t=s, A_t=a]$. We refer to $q_\\pi(s,a)$ as the **value of taking action** $a$ **in state** $s$ **under a policy** $\\pi$ (or alternatively as the **value of the state-action pair** $s, a$).\n",
    "- All optimal policies have the same action-value function $q_*$, called the **optimal action-value function**.\n",
    "- **my Q\\** i still do not know how the definition of the action-value fuction will help in getting the optimal policy. of course i saw the way they done it to get the optimal policy, but i mean, what is the point of (the definition of the action value function) taking an action then follow the policy ? why not, for example, the agent takes two actions and then follow the policy, would that help in any thing? this is the concept that i am missing and i hope to get an answer to it.(i may search for it or i may post it on the ai group, but i need first to make sure that this is not answered (or not made clear) later in this course.) \n",
    "\n",
    "#### Optimal Policies\n",
    "- Once the agent determines the optimal action-value function $q_*$, it can quickly obtain an optimal policy $\\pi_*$ by setting $\\pi_*(s) = \\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s,a)$.\n",
    "    - remember, the return of the last function \"$\\arg\\max_{a\\in\\mathcal{A}(s)}$\" in the (above) function is : optimal actions that the agent can take at each state (this is the optimal policy) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 4 : Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dynamic programming setting is a useful first step towards tackling the reinforcement learning problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (27 concepts)\n",
    "- 1- Introduction\n",
    "- 2- OpenAI Gym: FrozenLakeEnv\n",
    "- 3- Your Workspace\n",
    "- 4- Another Gridworld Example\n",
    "- 5- An Iterative method, Part 1\n",
    "- 6- An Iterative method, Part 2\n",
    "- 7- Quiz: An Iterative method\n",
    "- 8- Iterative Policy Evaluation\n",
    "- 9- Implementation\n",
    "- 10- Mini Project: DP (Parts 0 and 1)\n",
    "- 11- Action Values\n",
    "- 12- Implementation\n",
    "- 13- Mini Project: DP (Parts 2)\n",
    "- 14- Policy Improvement\n",
    "- 15- Implementation \n",
    "- 16- Mini Project: DP (Parts 3)\n",
    "- 17- Policy Iteration\n",
    "- 18- Implementation\n",
    "- 19- Mini Project: DP (Parts 4)\n",
    "- 20- Truncated Policy Iteration\n",
    "- 21- Implementation\n",
    "- 22- Mini Project: DP (Parts 5)\n",
    "- 23- Value Iteration\n",
    "- 24- Implementation\n",
    "- 25- Mini Project: DP (Parts 6)\n",
    "- 26- Check Your Understadning\n",
    "- 27- Summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.1) Introduction\n",
    "\n",
    "- For this lesson, we'll confine our attention to a problem that's slightly easier than the reinforcement learning problem. Instead of working in a setting where the agent has to learn from interaction, we'll assume that the agent already knows everything about the environment. So the agent knows how the environment decides the next state, and it knows how the environment decides reward. The goal will remain the same. Given this information, the agent would like to find the optimal policy. \n",
    "- Solving the simpler problem first will prove incredibly useful for building intuition before we tackle the full reinforcement learning problem. With this in mind, I'll catch you in the next video.\n",
    "\n",
    "---\n",
    "In the **dynamic programming** setting, the agent has full knowledge of the Markov decision process (MDP) that characterizes the environment. (This is much easier than the **reinforcement learning** setting, where the agent initially knows nothing about how the environment decides state and reward and must learn entirely from interaction how to select actions.)\n",
    "\n",
    "This lesson covers material in **Chapter 4** (especially 4.1-4.4) of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### an added cell by me to make an improtant illustration.\n",
    "~~~ start quote from wiki\n",
    "Dynamic programming is both a mathematical optimization method and a computer programming method. The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics. In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.\n",
    "\n",
    "If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, then there is a relation between the value of the larger problem and the values of the sub-problems.[1] In the optimization literature this relationship is called the Bellman equation.\n",
    "~~~ end quote from wiki\n",
    "\n",
    "#### do not confuse Richard Bellman with richard feynman (the latter is a known physicist)\n",
    "\n",
    "| Richard Bellman | richard feynman  | \n",
    "| --- | --- | \n",
    "| <img src=\"img/richard_bellman.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/> | <img src=\"img/richard_feynman.jpg\" alt=\"Drawing\" style=\"width: 300px;\"/>) |  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.2) OpenAI Gym: FrozenLakeEnv\n",
    "In this lesson, you will write your own Python implementations of all of the algorithms that we discuss. While your algorithms will be designed to work with any OpenAI Gym environment, you will test your code with the FrozenLake environment.\n",
    "\n",
    "In the FrozenLake environment, the agent navigates a 4x4 gridworld. You can read more about the environment in its corresponding [GitHub file](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py), by reading the commented block in the `FrozenLakeEnv` class. For clarity, we have also pasted the description of the environment below:\n",
    "\n",
    "~~~ **start .py file**\n",
    "```python\n",
    "  \"\"\"\n",
    "    Winter is here. You and your friends were tossing around a frisbee at the park\n",
    "    when you made a wild throw that left the frisbee out in the middle of the lake.\n",
    "    The water is mostly frozen, but there are a few holes where the ice has melted.\n",
    "    If you step into one of those holes, you'll fall into the freezing water.\n",
    "    At this time, there's an international frisbee shortage, so it's absolutely imperative that\n",
    "    you navigate across the lake and retrieve the disc.\n",
    "    However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "    The surface is described using a grid like the following\n",
    "        SFFF\n",
    "        FHFH\n",
    "        FFFH\n",
    "        HFFG\n",
    "    S : starting point, safe\n",
    "    F : frozen surface, safe\n",
    "    H : hole, fall to your doom\n",
    "    G : goal, where the frisbee is located\n",
    "    The episode ends when you reach the goal or fall in a hole.\n",
    "    You receive a reward of 1 if you reach the goal, and zero otherwise.\n",
    "\n",
    "    \"\"\"\"\n",
    "```\n",
    "~~~ **end .py file**\n",
    "\n",
    "#### The Dynamic Programming Setting\n",
    "Environments in OpenAI Gym are designed with the reinforcement learning setting in mind. For this reason, OpenAI Gym does not allow easy access to the underlying one-step dynamics of the Markov decision process (MDP).\n",
    "\n",
    "Towards using the FrozenLake environment for the dynamic programming setting, we had to first download the [file](https://github.com/openai/gym/blob/master/gym/envs/toy_text/frozen_lake.py) containing the `FrozenLakeEnv` class. Then, we added a single line of code to share the one-step dynamics of the MDP with the agent.\n",
    "\n",
    "~~~ **start .py file**\n",
    "```python\n",
    "# obtain one-step dynamics for dynamic programming setting\n",
    "self.P = P\n",
    "```\n",
    "~~~ **end .py file**\n",
    "\n",
    "Q\\ what does both sides of the previous assignment mean ? i.e what is self.P and what is the RHS P ?\n",
    "\n",
    "The new `FrozenLakeEnv` class was then saved in a Python file **frozenlake.py**, which we will use (instead of the original OpenAI Gym file) to create an instance of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.3) Your Workspace\n",
    "- You will write all of your implementations within the classroom, using an interface identical to the one shown below. Your Workspace contains five files:\n",
    "    - **frozenlake.py** - contains the `FrozenLakeEnv` class\n",
    "    - **Dynamic_Programming.ipynb** - the mini project notebook where you will write all of your implementations (*this is the **only** file that you will modify!*)\n",
    "    - Dynamic_Programming_Solution.ipynb - the instructor solutions corresponding to the mini project notebook\n",
    "    - **check_test.py** - contains unit tests that you will use to verify that your implementations are correct\n",
    "    - **plot_utils.py** - contains a plotting function for visualizing state-value functions\n",
    "    \n",
    "Note that **Dynamic_Programming.ipynb** is broken into parts, which are designed to be completed at different parts of the lesson. For instance, you will complete Parts 0 and 1 in the concept titled Mini Project: DP (Parts 0 and 1). Then, you should wait to complete Part 2 until you reach the Mini Project: DP (Part 2) concept. DO NOT COMPLETE THE ENTIRE NOTEBOOK ALL AT ONCE! :)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.4) Another Gridworld Example\n",
    "\n",
    "- Let's begin with a very small world and an agent who lives in it.\n",
    "\n",
    "<img src=\"img/6_4_4_1.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- The world is primarily composed of nice patches of grass, but one of the four locations in the world has a large mountain. \n",
    "- We can think of each of these four possible locations in the world as states in the environment. \n",
    "- At each point in time, let's say the agent can only move up, down, left or right and can only take actions that lead it to not fall off the grid. \n",
    "- Here, the arrows show the possible movements that we allow the agent. \n",
    "- let's also say the goal of the agent is to get to the bottom right hand corner of the world as quickly as possible. \n",
    "    - We'll think of this as an episodic task where an episode finishes when the agent reaches the goal, so we won't have to worry about transitions away from the school state. \n",
    "    - Furthermore, say that the agent receives a reward of a negative one for most transitions, but if an action leads it to encounter a mountain, it receives a reward of negative three and if it reaches the goal state, it gets a reward of five. <br><br>\n",
    "        \n",
    "- In the dynamic programming setting, the agent knows this rewards structure and it knows how transitions happen between states. So the agent already knows everything about how the environment operates. \n",
    "- So now what? How can the agent use this information to find the optimal policy?\n",
    "\n",
    "---\n",
    "In this simple gridworld example, you may find it easy to determine the optimal policy by visual inspection. Of course, solving Markov decision processes (MDPs) corresponding to real world problems will prove far more challenging! :)\n",
    "\n",
    "To avoid over-complicating the theory, we'll use this simple example to illustrate the same algorithms that are used to solve much more complicated MDPs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.5) An Iterative method, Part 1\n",
    "- my questions on this video:\n",
    "    - Q\\ how this system of equation is solved while the value of $v_{\\pi}(s_1)$ is always recursive ? \n",
    "        - I think you can do it easily by substituting, for example : substitute the first equation into the second equation. (another way of substitution is illustrated in next concept \"Part 2\")\n",
    "    - Q\\ what is the proof that the `iterative policy evaluation` will always converge to the true value function? \n",
    "---\n",
    "- Let's build off the grid world example and investigate how we might determine the value function corresponding to a particular policy. To begin, we'll enumerate the states. So, state S1 is the state in the top left corner, then S2, S3, and S4. \n",
    "\n",
    "<img src=\"img/6_4_5_1.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- Say, we're trying to <u>evaluate</u> the <u>Stochastic</u> Policy where the agent selects actions uniformly from the set of possible actions. For instance, in state S1, the agent either moves right or down and it essentially decides by just flipping a coin. That is, it moves right with 50 percent probability and otherwise, moves down. In state S2, it moves left half of the time and moves down the other half of the time. The same goes for state S3, where the agent moves up with 50 percent probability and otherwise moves right. \n",
    "\n",
    "<img src=\"img/6_4_5_2.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- So, how about we determine the state value function corresponding to this policy? Let's begin with recording what we know about state <u>S1</u>. \n",
    "    - So, half the time, the agent moves right. In the event that the agent does move right, the expected return it collects can be calculated as negative one plus the value of the state S2 that it lands on. And the other half of the time, the agent moves down and the resultant expected return is just negative three plus the value of the next state S3. So then, the value of state S1 can be calculated as the average of these two values. Since the agent chooses each of the potential actions with equal probability. \n",
    "    \n",
    "<img src=\"img/6_4_5_3.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- you might recognize this equation as just the Bellman equation evaluated at the state S1. \n",
    "    - Here we see that it let's us express the value of state S1 and the context of the values of all of its <u>possible successor states</u>. <br><br>\n",
    "    \n",
    "- Continuing with state <u>S2</u>, \n",
    "    - if the agent moves left, the expected return is negative one plus the value of the state S1 and if the agent moves down, the expected return is five plus the value of the terminal state S4. And to get the value of state S2, we need to just take the average of these two values. And here we've arrived at the Bellman equation again but now, for state S2. \n",
    "    \n",
    "<img src=\"img/6_4_5_4.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>    \n",
    "    \n",
    "- We can continue the trend for state <u>S3</u> \n",
    "    - where we take into account that the agent can move up or right and we take the average of the two values. \n",
    "    \n",
    "<img src=\"img/6_4_5_5.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>    \n",
    "    \n",
    "- Finally, the value of the state S4 will always be zero since it's a terminal state. After all, in the case that the agent starts at the state, the episode ends immediately and no reward is received. \n",
    "\n",
    "<img src=\"img/6_4_5_6.PNG\" alt=\"Drawing\" style=\"width: 150px;\"/> <br> \n",
    "\n",
    "- In this way, we see that we have one equation for each state and we can directly solve this system of equations to get the value of each state. And when you solve the system, you get these values where the values of state S1 and of the terminal state are both zero and each of the other two states has a value of two. \n",
    "\n",
    "<img src=\"img/6_4_5_7.PNG\" alt=\"Drawing\" style=\"width: 150px;\"/> <br> \n",
    "\n",
    "- For each state, we now have the expected return that's likely to follow if the agent starts in that state and then follows the equal probable random policy. \n",
    "- (**my note**: notice now why she will justify the need of the iterative method (i think this is the same concept as using gradient descent vs normal equation) :\n",
    "    - The only problem here is that typically the state space is much larger so directly solving the system of equations is more difficult. In this case, using an iterative method for solving the system generally works better. \n",
    "    \n",
    "    \n",
    "- So instead of solving the system directly, what we can do is start off with a guess for the value of each state. It doesn't have to be any good and what's typically done is to set the value of each state to zero. \n",
    "\n",
    "<img src=\"img/6_4_5_8.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- Then we start by focusing our attention to one state. Say state <u>S1</u>. \n",
    "    - And we'd like to improve our guess for the value of the state. To do this, we'll use the same Bellman equation from before but we'll adapt it so it works as an update rule. \n",
    "    \n",
    "<img src=\"img/6_4_5_9.PNG\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>    \n",
    "    \n",
    "- Here, the capital V denotes our current guess for the value function. We'll use this update rule to obtain a new guess for the value of state S1. <br><br>\n",
    "\n",
    "- <u>The idea is that we'll update a guess with a guess</u> where the current guesses for the values of states S2 and S3 are used to obtain a new guess for the value of state S1. \n",
    "    - So we plug in the estimates for the values of states S2 and S3 and when we do that, we get negative two which is our new guess for the value of state S1 and will record that new value. \n",
    "    \n",
    "<img src=\"img/6_4_5_10.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>    \n",
    "    \n",
    "- Then, we'll do the same for the second state where we update the guess for the value of state S2 using the value of state S1. When we plug in the value, we get one as our new estimate for the value of state S2 and we'll keep track of that. \n",
    "\n",
    "<img src=\"img/6_4_5_11.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- Then onto state S3, we update the value using our current estimate for the value of state S1 (**my note**: it is the new value of S1, not the old value, i mean, we always use the new value, i.e we do not do batch update). Our new estimate is one which we can record. \n",
    "\n",
    "<img src=\"img/6_4_5_12.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- **We won't need to update the value of state S4 since it's a terminal state so it'll always have a value of zero.** \n",
    "\n",
    "- But we can loop back to the first state and continue to proceed in this way with updating our guess based on the current guess for the values of the other states. In this case, the value is updated to negative one which we can record. \n",
    "\n",
    "<img src=\"img/6_4_5_13.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- Then we move on to state S2, S3 and so on over and over and it turns out that this iterative algorithm yields an estimate that converges to the true value function. **This is the idea behind an algorithm known as Iterative Policy Evaluation and we'll go into much more detail soon.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.6) An Iterative method, Part 2\n",
    "- (my note before the reading )\n",
    "    - I need you to keep close attention to the result of multiplying two summations (b/c I am so vulnerable when dealing with summation notaion because i was not introduced to it in the faculty. I may also try to do a correspondance between summations and for-loops so that i may understand summation with the aid of my understanding of for-loops)\n",
    "\n",
    "---\n",
    "~~ the Reading ~~\n",
    "#### An Iterative Method\n",
    "In this concept, we will examine some ideas from the last video in more detail.\n",
    "\n",
    "<img src=\"img/6_4_6_1.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "#### Notes on the Bellman Expectation Equation\n",
    "In the previous video, we derived one equation for each environment state. For instance, for state $s_1$, we saw that:\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3))$.\n",
    "\n",
    "- We mentioned that this equation follows directly from the Bellman expectation equation for $v_\\pi$.\n",
    "    - $v_\\pi(s) = \\text{} \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1}) | S_t=s] = \\sum_{a \\in \\mathcal{A}(s)}\\pi(a|s)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r + \\gamma v_\\pi(s'))$ <br>\n",
    "    (**The Bellman expectation equation for** $v_\\pi$)\n",
    "    \n",
    "- (**my notes**: about the previous equation)\n",
    "    - sum . sum , is it a dot product !! so we do not apply the sum at each function alone\n",
    "        - this way, only one for-loop will do this line of (two summations)\n",
    "    - is pi a vector ?? can be considered a vector ?? (if it is a vector, then if the sum is applied to it, it must be reduced to a single value befor going to the next sum !!)\n",
    "    - the $\\pi(a|s)$ is a probability regarding the policy, so it is something that the agent has learned to get the optimial solution. on the other hand, $p(s',r|s,a)$ is something about the environment, it is something that the agent is trying to learn about the environment (i.e how the environment works). \n",
    "\n",
    "In order to see this, we can begin by looking at what the Bellman expectation equation tells us about the value of state $s_1$ (where we just need to plug in $s_1$ for state $s$).\n",
    "\n",
    "$v_\\pi(s_1) = \\sum_{a \\in \\mathcal{A}(s_1)}\\pi(a|s_1)\\sum_{s' \\in \\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s_1,a)(r + \\gamma v_\\pi(s'))$\n",
    "\n",
    "- Then, it's possible to derive the equation for state $s_1$ by using the following:\n",
    "    - $\\mathcal{A}(s_1)=\\{ \\text{down}, \\text{right} \\}$ \n",
    "        - (*When in state $s_1$, the agent only has two potential actions: down or right.*) <br><br>\n",
    "        \n",
    "    - $\\pi({down}|s_1) = \\pi(\\text{right}|s_1) = \\frac{1}{2}$ \n",
    "        - (*We are currently examining the policy where the agent goes down with 50% probability and right with 50% probability when in state $s_1$.*)<br><br>\n",
    "        \n",
    "    - $p(s_3,-3|s_1,\\text{down}) = 1$  $\\;\\;\\;\\;$    (and $p(s',r|s_1,\\text{down}) = 0$ if $s'\\neq s_3$ or $r\\neq -3$) \n",
    "        - (*If the agent chooses to go down in state $s_1$, then with 100% probability, the next state is $s_3$, and the agent receives a reward of -3.*)<br><br>\n",
    "        \n",
    "    - $p(s_2,-1|s_1,\\text{right}) = 1$  $\\;\\;\\;\\;$     (and $p(s',r|s_1,\\text{right}) = 0$ if $s'\\neq s_2$ or $r\\neq -1$) \n",
    "        - (If the agent chooses to go right in state $s_1$, then with 100% probability, the next state is $s_2$, and the agent receives a reward of -1.)<br><br>\n",
    "        \n",
    "    - $\\gamma = 1$ \n",
    "        - (*We chose to set the discount rate to 1 in this gridworld example.*)\n",
    "\n",
    "If this is not entirely clear to you, please take the time now to plug in the values to derive the equation from the video. Then, you are encouraged to repeat the same process for the other states.\n",
    "\n",
    "#### Notes on Solving the System of Equations\n",
    "In the video, we mentioned that you can directly solve the system of equations:\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3))$\n",
    "\n",
    "$v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$\n",
    "\n",
    "$v_\\pi(s_3) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$\n",
    "\n",
    "$v_\\pi(s_4) = 0$\n",
    "\n",
    "Since the equations for $v_\\pi(s_2)$ and $v_\\pi(s_3)$ are identical, we must have that $v_\\pi(s_2) = v_\\pi(s_3)$\n",
    "\n",
    "Thus, the equations for $v_\\pi(s_1)$ and $v_\\pi(s_2)$ can be changed to:\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_2)) = -2 + v_\\pi(s_2)$\n",
    "\n",
    "$v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + 0) = 2 + \\frac{1}{2}v_\\pi(s_1)$\n",
    "\n",
    "Combining the two latest equations yields\n",
    "\n",
    "$v_\\pi(s_1) = -2 + 2 + \\frac{1}{2}v_\\pi(s_1) = \\frac{1}{2}v_\\pi(s_1)$,\n",
    "\n",
    "which implies $v_\\pi(s_1)=0$. Furthermore, $v_\\pi(s_3) = v_\\pi(s_2) = 2 + \\frac{1}{2}v_\\pi(s_1) = 2 + 0 = 2$.\n",
    "\n",
    "Thus, the state-value function is given by:\n",
    "\n",
    "$v_\\pi(s_1) = 0v$ \n",
    "\n",
    "$v_\\pi(s_2) = 2$\n",
    "\n",
    "$v_\\pi(s_3) = 2$\n",
    "\n",
    "$v_\\pi(s_4) = 0$\n",
    "\n",
    "**Note.** This example serves to illustrate the fact that it is ***possible*** to <u>*directly*</u> solve the system of equations given by the Bellman expectation equation for $v_\\pi$. However, in practice, and especially for much larger Markov decision processes (MDPs), we will instead use an <u>*iterative*</u> solution approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.7) Quiz: An Iterative method\n",
    "\n",
    "#### Quiz: An Iterative Method\n",
    "So far in this lesson, we have discussed how an agent might obtain the state-value function $v_\\pi$ corresponding to a policy $\\pi$.\n",
    "\n",
    "- In the dynamic programming setting, the agent has full knowledge of the Markov decision process (MDP). In this case, it's possible to use the one-step dynamics $p(s',r|s,a)$ of the MDP to obtain a system of equations corresponding to the Bellman expectation equation for $v_\\pi$ \n",
    "    - **my note** : the one step dynamic was shown in the previous concept to be :\n",
    "        - $p(s_3,-3|s_1,\\text{down}) = 1$  $\\;\\;\\;\\;$    (and $p(s',r|s_1,\\text{down}) = 0$ if $s'\\neq s_3$ or $r\\neq -3$) \n",
    "        - $p(s_2,-1|s_1,\\text{right}) = 1$  $\\;\\;\\;\\;$     (and $p(s',r|s_1,\\text{right}) = 0$ if $s'\\neq s_2$ or $r\\neq -1$) \n",
    "\n",
    "In the gridworld example, the system of equations corresponding to the equiprobable random policy was given by:\n",
    "\n",
    "$v_\\pi(s_1) = \\frac{1}{2}(-1 + v_\\pi(s_2)) + \\frac{1}{2}(-3 + v_\\pi(s_3))$\n",
    "\n",
    "$v_\\pi(s_2) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$\n",
    "\n",
    "$v_\\pi(s_3) = \\frac{1}{2}(-1 + v_\\pi(s_1)) + \\frac{1}{2}(5 + v_\\pi(s_4))$\n",
    "\n",
    "$v_\\pi(s_4) = 0$\n",
    "\n",
    "In order to obtain the state-value function, we need only solve the system of equations.\n",
    "\n",
    "While it is always possible to directly solve the system, we will instead use an iterative solution approach.\n",
    "\n",
    "#### An Iterative Method\n",
    "The iterative method begins with an initial guess for the value of each state. In particular, we began by assuming that the value of each state was zero.\n",
    "\n",
    "Then, we looped over the state space and amended the estimate for the state-value function through applying successive update equations.\n",
    "\n",
    "Recall that $V$ denotes the most recent guess for the state-value function, and the update equations are:\n",
    "\n",
    "$V(s_1) \\leftarrow \\frac{1}{2}(-1 + V(s_2)) + \\frac{1}{2}(-3 + V(s_3))$\n",
    "\n",
    "$V(s_2) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5)$\n",
    "\n",
    "$V(s_3) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5)$\n",
    "\n",
    "#### Quiz Question\n",
    "Say that the most recent guess for the state-value function is given in the figure below.\n",
    "\n",
    "<img src=\"img/6_4_7_1.PNG\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "Currently, the estimate for $v_\\pi(s_2)$ is given by $V(s_2) = 1$.\n",
    "\n",
    "Say that the next step in the algorithm is to update $V(s_2)$.\n",
    "\n",
    "What is the new value for $V(s_2)$ after applying the update step once?\n",
    "\n",
    "- my solution : <br>\n",
    "$V(s_2) \\leftarrow \\frac{1}{2}(-1 + V(s_1)) + \\frac{1}{2}(5)$ <br>\n",
    "$V(s_2) \\leftarrow \\frac{1}{2}(-1 + -1) + \\frac{1}{2}(5)$ <br>\n",
    "$V(s_2) \\leftarrow -1 + 2.5$ <br>\n",
    "$V(s_2) \\leftarrow 1.5$ <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.8) Iterative Policy Evaluation\n",
    "- **note before the video**:\n",
    "    - I assume that for larger problems, the iterative process is faster than solving the sys of eqs b/c the later (I assume) envolves inverse of big matrices. So I want to try and and plot the time needed for both methods to get to the answer while I increase the number of inputs. things to look for is : when the iterative method will be faster than the solving-eq method (at what number of inputs (critical number of input) )?\n",
    "        - recall that I got this intuition from the Coursera ML course when Dr.Andrew contrasted the Gradient Descent (which is an iterative method) vs the Normal equation (which is one shot solving-eqs method)\n",
    "---\n",
    "- earlier in this lesson you were introduced to an iterative method for determining the value function corresponding to a policy \n",
    "- in this video we'll discuss the algorithm in more detail  <br><br>\n",
    "\n",
    "- the algorithm is called iterative policy evaluation and <u>it assumes that the agent already has full and perfect knowledge of the MDP that characterizes the environment</u>  <br><br>\n",
    "\n",
    "- remember that this algorithm (in picture below) was motivated by the bellmen expectation equation for the state value function and this equation is really a system of equations since we have one equation for \n",
    "- each environment state each equation relates the value of a state to the values of its successor States. \n",
    "\n",
    "<img src=\"img/6_4_8_1.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- theoretically since we have a system of equations with one equation for each unknown quantity we could solve the system but there's an easier way where instead of solving the system exactly we'll construct an iterative algorithm where each step gets us closer and closer to solving this system of equations \n",
    "- specifically our iterative algorithm will adapt this bellman equation so it can be used as an update rule \n",
    "\n",
    "<img src=\"img/6_4_8_2.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- here the capital V denotes the current guess for the policies value function note that I've only changed two things about the bellman equation which I've underlined in yellow here  <br><br>\n",
    "\n",
    "- so the algorithm begins with an initial guess for the value function of the policy what's typically done is to set the initial guess for each state to 0 okay and so there's no way that's actually the true value function but we just need to start somewhere \n",
    "- then we'll loop over the states and use the update rule to improve our guess for the state value function \n",
    "\n",
    "<img src=\"img/6_4_8_3.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- what's nice is that as long as a few technical conditions are satisfied this algorithm is guaranteed to converge to the value function for the policy \n",
    "- now we can only attain true convergence in the limit of running this algorithm an infinite number of times and that's not feasible in practice so we'll have to stop short of true convergence \n",
    "- and the question is how can we tell when we've gotten close enough \n",
    "    - well in practice when you implement the algorithm you'll notice that the first few times the update step is applied there are big changes to the value function but then eventually at a later time point you'll notice that updates are hardly noticeable and once we get to that point we're applying the update rule doesn't change the estimate of the value function much that's a strong indication that our algorithm has gotten reasonably close to converging to the true value function \n",
    "    - and so inspired by this fact we can design a stopping criterion that terminates the algorithm whenever we've done a complete pass over all the states and none of the values has changed much \n",
    "        - to do this we'll amend the pseudocode where \n",
    "            - the first step is for you to set a very small positive number theta \n",
    "            - next as before you initialize the first guess for the value function to be all zeros \n",
    "            - then you enter the iterative loop where each step you apply the development update and check to see how much the values for each of the states has changed if the maximum change over all states is less than that small number theta that you set then you stop the algorithm and say that your estimate for the value function is good enough\n",
    "            \n",
    "<img src=\"img/6_4_8_4.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- and that's it but now if that update step (she highlighted the bellman assignment line) feels suspicious to you I don't blame you and your instincts to one a proof are correct it's not at all intuitive that this algorithm should work the way it does but here's the main idea behind what that update step is doing\n",
    "\n",
    "<img src=\"img/6_4_8_5.PNG\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- remember that we loop over states and apply the update to one state at a time and from each state the agent could choose any of a number of possible actions that bring it to any of a number of potential next States and the bellman equation helps us relate the value of this parent state, $s$ to the values of all of its possible successor states $s'$. <br><br>\n",
    "\n",
    "- now what we do is look at the estimated value function for the parent state (she underlined $V(s)$) along with the values of the successor States (she underlined $V(s')$) and if we plug in those values to the bellman equation and it's not satisfied, what we'll do is we'll change the value of the parent state so that the bellman equation is satisfied, then we'll do the same for the second state and so on, and what's important to note is that if at some point we go to apply this update rule and there's no change in any of the values of any of the states then that means we have value function that perfectly satisfies the bellman equation and the only value function that does that is the true value function of the policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.9) Implementation\n",
    "#### Implementation: Iterative Policy Evaluation\n",
    "\n",
    "The pseudocode for iterative policy evaluation can be found below.\n",
    "\n",
    "- **my note**: in the most inner for-loop , the line $\\Delta \\leftarrow max(\\Delta, |v-V(s)|)$ , I want to demonstrate why there is a max function between two ipnuts:\n",
    "    - in $|v-V(s)|$ we store the error of the current state\n",
    "    - but in $\\Delta$ we store the maximum error of all states in one loop. this is because we want the maximum error of all state to be less the $\\theta$ (this what breaks the outer loop, not single error of one state being less that $\\theta$, but rather, all error in all states are less than $\\theta$ ) <br><br>\n",
    "    \n",
    "- Note that policy evaluation is guaranteed to converge to the state-value function $v_\\pi$ corresponding to a policy $\\pi$, as long as $v_\\pi(s)$ is finite for each state $s\\in\\mathcal{S}$. For a finite Markov decision process (MDP), this is guaranteed as long as either:\n",
    "    - $\\gamma < 1$, or\n",
    "    - if the agent starts in any state $s\\in\\mathcal{S}$, it is guaranteed to eventually reach a terminal state if it follows policy $\\pi$.\n",
    "    \n",
    "Please use the next concept to complete Part 0: Explore FrozenLakeEnv and Part 1: Iterative Policy Evaluation of Dynamic_Programming.ipynb. Remember to save your work!    \n",
    "\n",
    "#### (Optional) Additional Note on the Convergence Conditions\n",
    "- To see intuitively *why* the conditions for convergence make sense, consider the case that neither of the conditions are satisfied, so:\n",
    "    - $\\gamma = 1$, and\n",
    "    - there is some state $s\\in\\mathcal{S}$ where if the agent starts in that state, it will never encounter a terminal state if it follows policy $\\pi$.\n",
    "\n",
    "- In this case,\n",
    "    - reward is not discounted, and\n",
    "    - an episode may never finish.\n",
    "\n",
    "Then, it is possible that iterative policy evaluation will not converge, and this is because the state-value function may not be well-defined! To see this, note that in this case, calculating a state value could involve adding up an infinite number of (expected) rewards, where the sum may not [converge](https://en.wikipedia.org/wiki/Convergent_series).\n",
    "\n",
    "- In case it would help to see a concrete example, consider an MDP with:\n",
    "    - two states $s_1$ and $s_2$, where $s_2$ is a terminal state\n",
    "    - one action $a$ (*Note: An MDP with only one action can also be referred to as a [Markov Reward Process (MRP)](https://en.wikipedia.org/wiki/Markov_reward_model).*)\n",
    "    - $p(s_1,1|s_1, a) = 1$\n",
    "    \n",
    "In this case, say the agent's policy $\\pi$ is to \"select\" the only action that's available, so $\\pi(s_1) = a$. Say $\\gamma = 1$. According to the one-step dynamics, if the agent starts in state $s_1$, it will stay in that state forever and never encounter the terminal state $s_2$.\n",
    "\n",
    "In this case, $v_\\pi(s_1)$ **is not well-defined**. To see this, remember that $v_\\pi(s_1)$ is the (expected) return after visiting state $s_1$, and we have that\n",
    "\n",
    "$v_\\pi(s_1) = 1 + 1 + 1 + 1 + ...$\n",
    "\n",
    "which [diverges](https://en.wikipedia.org/wiki/Divergent_series) to infinity. (Take the time now to convince yourself that if either of the two convergence conditions were satisfied in this example, then $v_\\pi(s_1)$ would be well-defined. As a very optional next step, if you want to verify this mathematically, you may find it useful to review [geometric series](https://en.wikipedia.org/wiki/Geometric_series) and the [negative binomial distribution](https://en.wikipedia.org/wiki/Negative_binomial_distribution).)\n",
    "\n",
    "(**my note**: i think the video of the boat that decided to loop around regenerated coins instead of going to the finish line, is a problem where the state value will not converge.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.10) Mini Project: DP (Parts 0 and 1)\n",
    "\n",
    "#### Part 0: Explore FrozenLakeEnv\n",
    "Use the code cell below to create an instance of the FrozenLake environment.\n",
    "\n",
    "~~~ **start python code**\n",
    "```python\n",
    "!pip install -q matplotlib==2.2.2\n",
    "from frozenlake import FrozenLakeEnv\n",
    "\n",
    "env = FrozenLakeEnv()\n",
    "```\n",
    "~~~ **end python code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent moves through a $4 \\times 4$ gridworld, with states numbered as follows:\n",
    "```\n",
    "[[ 0  1  2  3]\n",
    " [ 4  5  6  7]\n",
    " [ 8  9 10 11]\n",
    " [12 13 14 15]]\n",
    "```\n",
    "and the agent has 4 potential actions:\n",
    "```\n",
    "LEFT = 0\n",
    "DOWN = 1\n",
    "RIGHT = 2\n",
    "UP = 3\n",
    "```\n",
    "\n",
    "Thus, $\\mathcal{S}^+ = \\{0, 1, \\ldots, 15\\}$, and $\\mathcal{A} = \\{0, 1, 2, 3\\}$.  Verify this by running the code cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~ **start python code**\n",
    "```python\n",
    "# print the state space and action space\n",
    "print(env.observation_space)\n",
    "print(env.action_space)\n",
    "\n",
    "# print the total number of states and actions\n",
    "print(env.nS)\n",
    "print(env.nA)\n",
    "```\n",
    "~~~ **end python code**\n",
    "\n",
    "output : <br>\n",
    "Discrete(16) <br>\n",
    "Discrete(4)  <br>\n",
    "16  <br>\n",
    "4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dynamic programming assumes that the agent has full knowledge of the MDP.  We have already amended the `frozenlake.py` file to make the one-step dynamics accessible to the agent.  \n",
    "\n",
    "Execute the code cell below to return the one-step dynamics corresponding to a particular state and action.  In particular, `env.P[1][0]` returns the the probability of each possible reward and next state, if the agent is in state 1 of the gridworld and decides to go left."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~ **start python code**\n",
    "```python\n",
    "env.P[1][0]\n",
    "```\n",
    "~~~ **end python code**\n",
    "\n",
    "output : <br>\n",
    "[(0.3333333333333333, 1, 0.0, False), <br>\n",
    " (0.3333333333333333, 0, 0.0, False), <br>\n",
    " (0.3333333333333333, 5, 0.0, True)]\n",
    " \n",
    "---\n",
    "- My explanation for the previous results. if the agent is in state 1, and decided to go left, then it has \n",
    "    - $\\frac{1}{3}$ probability of statying in the same state with no reward , and since this state is not terminal, we are not done yet (i.e False)\n",
    "    - $\\frac{1}{3}$ probability of going left to the state 0 with no reward , and since this state is not terminal state, we are not done yet (i.e False)\n",
    "    - $\\frac{1}{3}$ probability of going down to state 5 with no reward , and since this state is a terminal state, we are done (i.e True)\n",
    "        - i think there is no reward here b/c although it is a terminal state, this is not the target\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry takes the form \n",
    "```\n",
    "prob, next_state, reward, done\n",
    "```\n",
    "where: \n",
    "- `prob` details the conditional probability of the corresponding (`next_state`, `reward`) pair, and\n",
    "- `done` is `True` if the `next_state` is a terminal state, and otherwise `False`.\n",
    "\n",
    "Thus, we can interpret `env.P[1][0]` as follows:\n",
    "$$\n",
    "\\mathbb{P}(S_{t+1}=s',R_{t+1}=r|S_t=1,A_t=0) = \\begin{cases}\n",
    "               \\frac{1}{3} \\text{ if } s'=1, r=0\\\\\n",
    "               \\frac{1}{3} \\text{ if } s'=0, r=0\\\\\n",
    "               \\frac{1}{3} \\text{ if } s'=5, r=0\\\\\n",
    "               0 \\text{ else}\n",
    "            \\end{cases}\n",
    "$$\n",
    "\n",
    "To understand the value of `env.P[1][0]`, note that when you create a FrozenLake environment, it takes as an (optional) argument `is_slippery`, which defaults to `True`.  \n",
    "\n",
    "To see this, change the first line in the notebook from `env = FrozenLakeEnv()` to `env = FrozenLakeEnv(is_slippery=False)`.  Then, when you check `env.P[1][0]`, it should look like what you expect (i.e., `env.P[1][0] = [(1.0, 0, 0.0, False)]`).\n",
    "\n",
    "The default value for the `is_slippery` argument is `True`, and so `env = FrozenLakeEnv()` is equivalent to `env = FrozenLakeEnv(is_slippery=True)`.  In the event that `is_slippery=True`, you see that this can result in the agent moving in a direction that it did not intend (where the idea is that the ground is *slippery*, and so the agent can slide to a location other than the one it wanted).\n",
    "\n",
    "Feel free to change the code cell above to explore how the environment behaves in response to other (state, action) pairs.  \n",
    "\n",
    "Before proceeding to the next part, make sure that you set `is_slippery=True`, so that your implementations below will work with the slippery environment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: Iterative Policy Evaluation\n",
    "\n",
    "In this section, you will write your own implementation of iterative policy evaluation.\n",
    "\n",
    "Your algorithm should accept four arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `theta`: This is a very small positive number that is used to decide if the estimate has sufficiently converged to the true value function (default value: `1e-8`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s` under the input policy.\n",
    "\n",
    "<img src=\"img/6_4_10_1.jpg\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_4_9_1.PNG\" alt=\"Drawing\" style=\"width: 477px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def policy_evaluation(env, policy, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS): # for each state\n",
    "            Vs = 0\n",
    "            for a, action_prob in enumerate(policy[s]): # for each available action and its corresponding probability (note I think that the available action is set by the policy). also look here for a refresher about enumerate in python -> https://www.geeksforgeeks.org/enumerate-in-python/\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    Vs += action_prob * prob * (reward + gamma * V[next_state])\n",
    "            delta = max(delta, np.abs(V[s]-Vs))\n",
    "            V[s] = Vs\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### my illustration for the previous code\n",
    "- the target of the previous code is to use the policy (table) to get the value fuction for each state. (note that the policy is given)\n",
    "- normally, i would expect to have only two for loops to go through the 2D policy table and use that in filling the 1D array of the Value-functions, however, since this example is with slippery mode (so for each action in each state there are multiple probabilities fot several next state (of course this is given by the environment), so that's why we needed a third loop.) <br><br>\n",
    "\n",
    "- take, for example, the state s1\n",
    "    - state s1 can take one of four actions, and for each action there are several probabilities of the next state .\n",
    "    - i.e ...  s1 $ \\begin{cases} 0 (LEFT) \\begin{cases} (\\frac{1}{3}, 1, 0.0, False)  \\\\ (\\frac{1}{3}, 0, 0.0, False) \\\\ (\\frac{1}{3}, 5, 0.0, True) \\end{cases} \\\\ 1 (DOWN) \\\\ 2 (RIGHT) \\\\ 3 (UP) \\end{cases} $\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the equiprobable random policy $\\pi$, where $\\pi(a|s) = \\frac{1}{|\\mathcal{A}(s)|}$ for all $s\\in\\mathcal{S}$ and $a\\in\\mathcal{A}(s)$.  \n",
    "\n",
    "Use the code cell below to specify this policy in the variable `random_policy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_policy = np.ones([env.nS, env.nA]) / env.nA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_4_10_2.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"img/6_4_10_3.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to evaluate (using the function i just made above) the equiprobable random policy and visualize the output.  The state-value function has been reshaped to match the shape of the gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import plot_values\n",
    "\n",
    "# evaluate the policy \n",
    "V = policy_evaluation(env, random_policy)\n",
    "\n",
    "plot_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output : \n",
    "    \n",
    "<img src=\"img/6_4_10_4.png\" alt=\"Drawing\" style=\"width: 338px;\"/>    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.11) Action Values\n",
    "In a previous concept, you wrote your own implementation of iterative policy evaluation to estimate the state-value function $v_\\pi$ for a policy $\\pi$. In this concept, you will use the simple gridworld from the videos to practice converting a state-value function $v_\\pi$ to an action-value function $q_\\pi$.\n",
    "\n",
    "Consider the small gridworld that we used to illustrate iterative policy evaluation. The **state-value function** for the equiprobable random policy is visualized below.\n",
    "\n",
    "<img src=\"img/6_4_11_1.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "Take the time now to verify that the below image corresponds to the **action-value function** for the same policy.\n",
    "\n",
    "<img src=\"img/6_4_11_2.png\" alt=\"Drawing\" style=\"width: 300px;\"/> \n",
    "\n",
    "As an example, consider $q_\\pi(s_1, \\text{right})$. This action value can be calculated as\n",
    "\n",
    "$q_\\pi(s_1, \\text{right}) = -1 + v_\\pi(s_2) = -1 + 2 = 1$,\n",
    "\n",
    "where we just use the fact that we can express the value of the state-action pair $s_1$, $\\text{right}$ as the sum of two quantities: (1) the immediate reward after moving right and landing on state $s_2$, and (2) the cumulative reward obtained if the agent begins in state $s_2$ and follows the policy.\n",
    "\n",
    "- **my note** : note that i needn't have the policy known to get the action value function from the state value function, i only apply the method described above and will get the action value function for each state as a sum of two quantities. on the other hand, i must know the policy to be able to get the state value function for each state.\n",
    "\n",
    "Please now use the state-value function $v_\\pi$ to calculate $q_\\pi(s_1, \\text{down})$, $q_\\pi(s_2, \\text{left})$, $q_\\pi(s_2, \\text{down})$, $q_\\pi(s_3, \\text{up})$, and $q_\\pi(s_3, \\text{right})$.\n",
    "\n",
    "#### For More Complex Environments\n",
    "- In this simple gridworld example, the environment is **deterministic**. In other words, after the agent selects an action, the next state and reward are 100% guaranteed and non-random. For deterministic environments, $p(s',r|s,a) \\in \\{ 0,1 \\}$ for all $s', r, s, a$.\n",
    "    - In this case, when the agent is in state $s$ and takes action $a$, the next state $s'$ and reward $r$ can be predicted with certainty, and we must have $q_\\pi(s,a) = r + \\gamma v_\\pi(s')$ <br><br>\n",
    "\n",
    "- In general, the environment need not be deterministic, and instead may be **stochastic**. This is the default behavior of the `FrozenLake` environment from the mini project; in this case, once the agent selects an action, the next state and reward cannot be predicted with certainty and instead are random draws from a [(conditional) probability distribution](https://en.wikipedia.org/wiki/Conditional_probability_distribution) $p(s',r|s,a)$.\n",
    "    - In this case, when the agent is in state $s$ and takes action $a$, the probability of each possible next state $s'$ and reward $r$ is given by $p(s',r|s,a)$. In this case, we must have $q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$, where we take the [expected value](https://en.wikipedia.org/wiki/Expected_value) of the sum $r + \\gamma v_\\pi(s')$.\n",
    "\n",
    "Over the next couple concepts, you'll use this equation to write a function that yields an action-value function $q_\\pi$ corresponding to a policy $\\pi$ for the `FrozenLake` environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.12) Implementation\n",
    "#### Implementation: Estimation of Action Values\n",
    "\n",
    "In the next concept, you will write an algorithm that accepts an estimate $V$ of the state-value function $v_\\pi$, along with the one-step dynamics of the MDP $p(s',r|s,a)$, and returns an estimate $Q$ the action-value function $q_\\pi$.\n",
    "\n",
    "In order to do this, you will need to use the equation discussed in the previous concept, which uses the one-step dynamics $p(s',r|s,a)$ of the Markov decision process (MDP) to obtain $q_\\pi$ from $v_\\pi$. Namely,\n",
    "\n",
    "$q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}^+, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$\n",
    "\n",
    "holds for all $s\\in\\mathcal{S}$ and $a\\in\\mathcal{A}(s)$.\n",
    "\n",
    "You can find the associated pseudocode below.\n",
    "\n",
    "<img src=\"img/6_4_12_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \n",
    "\n",
    "Please use the next concept to complete **Part 2: Obtain** $q_\\pi$ **from** $v_\\pi$ of `Dynamic_Programming.ipynb`. Remember to save your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.13) Mini Project: DP (Parts 2)\n",
    "#### Part 2: Obtain $q_\\pi$ from $v_\\pi$\n",
    "\n",
    "In this section, you will write a function that takes the state-value function estimate as input, along with some state $s\\in\\mathcal{S}$.  It returns the **row in the action-value function** corresponding to the input state $s\\in\\mathcal{S}$.  That is, your function should accept as input both $v_\\pi$ and $s$, and return $q_\\pi(s,a)$ for all $a\\in\\mathcal{A}(s)$.\n",
    "\n",
    "Your algorithm should accept four arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "- `s`: This is an integer corresponding to a state in the environment.  It should be a value between `0` and `(env.nS)-1`, inclusive.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `q`: This is a 1D numpy array with `q.shape[0]` equal to the number of actions (`env.nA`).  `q[a]` contains the (estimated) value of state `s` and action `a`.\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_from_v(env, V, s, gamma=1):\n",
    "    q = np.zeros(env.nA)\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    for a in range(env.nA): # my Q\\ what about if the action is not allowed (i.e if it will take the agent out of the maze),answer in the cell below\n",
    "        for prob, next_state, reward, done in env.P[s][a]:\n",
    "            q[a] += prob * (reward + gamma * V[next_state])\n",
    "    \n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to answer the question in the comment in the cell above : \n",
    "    -  i think this is handeled by the environment, in which it will give such action a probability of 0.\n",
    "        - yes that's right, so for example if the agent was in state 0, and took the action \"left\", here is the reponse of the environment : \n",
    "        [(0.3333333333333333, 0, 0.0, False), <br>\n",
    "         (0.3333333333333333, 0, 0.0, False), <br>\n",
    "         (0.3333333333333333, 4, 0.0, False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to print the action-value function corresponding to the above state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros([env.nS, env.nA])\n",
    "for s in range(env.nS):\n",
    "    Q[s] = q_from_v(env, V, s)\n",
    "print(\"Action-Value Function:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_4_13_1.png\" alt=\"Drawing\" style=\"width: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.14) Policy Improvement\n",
    "\n",
    "- I hope you enjoyed implementing iterative policy evaluation in the first part of the mini project. Feel free to use your algorithm to evaluate a policy for any finite MDP of your choosing. You need not confine yourself to the Frozen Lake environment. Just remember that policy evaluation requires the agent to have full knowledge of the MDP. Then if you like, you can use the second part of the mini project to obtain the corresponding action value function. But for now, let's continue our search for an optimal policy. <br><br>\n",
    "\n",
    "- Policy evaluation gets us partially there (my note: i think, by \"there\" she means to the destination of solving the RL problem). After all, in order to figure out the best policy, it helps to be able to evaluate candidate policies. \n",
    "\n",
    "<img src=\"img/6_4_14_1.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "- In this video, you'll learn about policy improvement, an algorithm that uses the value function for a policy in order to propose a new policy that's at least as good as the current one. \n",
    "\n",
    "<img src=\"img/6_4_14_2.png\" alt=\"Drawing\" style=\"width: 350px;\"/>\n",
    "\n",
    "- But why would we want to do that? Well, immediately we can see how policy evaluation and policy improvement could go quite nicely together. \n",
    "\n",
    "<img src=\"img/6_4_14_3.png\" alt=\"Drawing\" style=\"width: 450px;\"/>\n",
    "\n",
    "- Policy evaluation takes a policy and yields the value function. Then we can take that same value function and use policy improvement to get a potentially new and improved policy. Then it's possible to take that new policy, plug it in to do policy evaluation again, then policy improvement over and over until we converge to an optimal policy. \n",
    "\n",
    "\n",
    "\n",
    "- Let's try to understand this better in the context of the grid world from earlier in the lesson. \n",
    "    - Remember that this was an episodic task where the only terminal state was the bottom right hand corner. \n",
    "\n",
    "<img src=\"img/6_4_14_4.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "- _\n",
    "    - We assume that the agent already knows everything about the environment. So it knows how transitions happen and how reward is decided. And in particular, it does not need to interact with the environment to get this information, but it still needs to determine the optimal policy.\n",
    "\n",
    "<img src=\"img/6_4_14_5.png\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "- _    \n",
    "    - So say the agent starts off with an initial guess for the optimal policy. In a previous video, we saw that it made sense to start with the equiprobable random policy, where in each state the agent randomly picks from a set of available actions. \n",
    "    - So based on this idea, the agent calculates the corresponding value function through iterative policy evaluation. We did this in an earlier video, where we saw that this was the value function. \n",
    "    \n",
    "<img src=\"img/6_4_14_6.png\" alt=\"Drawing\" style=\"width: 700px;\"/>    \n",
    "    \n",
    "- _    \n",
    "    - So now the question becomes, how might we design this policy improvement step to find a policy that's at least as good as the current one? We'll break policy improvement into two steps. \n",
    " \n",
    "<img src=\"img/6_4_14_7.png\" alt=\"Drawing\" style=\"width: 700px;\"/> \n",
    "\n",
    "- _\n",
    "    - The first step is to convert the state value function to an actual value function. You've already seen how to implement this. And when we only follow that same procedure on this small grid world, we get this action-value function. \n",
    "    \n",
    "<img src=\"img/6_4_14_8.png\" alt=\"Drawing\" style=\"width: 700px;\"/> \n",
    "\n",
    "- _    \n",
    "    - So now (i think this is the second step) we need only focus on how to use this action-value function to obtain a policy that's better than the equal probable random policy. \n",
    "    - So here's the idea. For each state, we'll just pick the action that maximizes the action-value function. So beginning with the state in the top left corner, one is greater than negative one, so the policy will be to go right, instead of down. Then (she highlighted the top right corner state) five is greater than negative one, so the policy goes down here. And again, (she highlighted the bottom left corner state) five is greater than negative one, so the policy goes right. \n",
    "    - You might be wondering at this point what would happen if we encountered a state with multiple choices of actions that maximize the action-value function. In this case you have options. \n",
    "        - You could arbitrarily pick one or \n",
    "        - construct a stochastic policy that assigns non-zero probability to any or all of them. We'll go more into this soon.\n",
    "    - But let's dig deeper into exactly why this (i think by \"this\" she means \"choosing the action that maximizes the action-value function\") should work. \n",
    "        - Remember that the value function corresponding to a policy just stores the expected return if the agent follows the policy for all time steps. \n",
    "        - And when we calculate the action-value function, we're looking at what would happen if the agent at some time step t chose an action a and then henceforth followed the policy. \n",
    "        \n",
    "<img src=\"img/6_4_14_9.png\" alt=\"Drawing\" style=\"width: 700px;\"/>       \n",
    "       \n",
    "- _\n",
    "    - _\n",
    "        - And our method for constructing the next policy pi_prime, $\\pi'$, looks at this action-value function and for each state determines that first action $a$ that's best for maximizing return. In this way, it follows that whatever expected return was associated to following the old policy pi for all time steps, the agent has higher expected return If it initially follows policy pi_prime and then henceforth follows policy $\\pi$. \n",
    "        \n",
    "<img src=\"img/6_4_14_10.png\" alt=\"Drawing\" style=\"width: 700px;\"/>         \n",
    "        \n",
    "- _\n",
    "    - _\n",
    "        - That said, it's possible to prove not only is policy pi_prime, $\\pi'$, better to follow for the first time step, it's also better to follow for all time steps. In other words, it's possible to prove that policy pi_prime, $\\pi'$, is better than or equal to policy $\\pi$. \n",
    "        \n",
    "<img src=\"img/6_4_14_11.png\" alt=\"Drawing\" style=\"width: 700px;\"/> \n",
    "\n",
    "- _\n",
    "    - _        \n",
    "       - And remember in order to prove this, we need to show that the value function for policy pi_prime is greater than or equal to the value function for policy pi for all states. \n",
    "       \n",
    "<img src=\"img/6_4_14_12.png\" alt=\"Drawing\" style=\"width: 350px;\"/>       \n",
    "       \n",
    "- _\n",
    "    - _       \n",
    "       - If you'd like to see how to do this in more detail, you're encouraged to check out the optional reading in the textbook. For now we can plug this into our algorithm for policy improvement where as you can see (in the pic belwo), the idea is that you'll first calculate the action-value function from the state value function. Then, in order to construct a better policy for each state, we just need to pick one action that maximizes the action-value function. You'll have the chance to implement this algorithm yourself soon.\n",
    "       \n",
    "<img src=\"img/6_4_14_13.png\" alt=\"Drawing\" style=\"width: 700px;\"/>        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.15) Implementation\n",
    "#### Implementation: Policy Improvement\n",
    "\n",
    "In the last lesson, you learned that given an estimate $Q$ of the action-value function $q_\\pi$ corresponding to a policy $\\pi$, it is possible to construct an improved (or equivalent) policy $\\pi'$, where $\\pi'\\geq\\pi$.\n",
    "\n",
    "For each state $s\\in\\mathcal{S}$, you need only select the action that maximizes the action-value function estimate. In other words,\n",
    "\n",
    "$\\pi'(s) = \\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)$ for all $s\\in\\mathcal{S}$.\n",
    "\n",
    "The full pseudocode for **policy improvement** can be found below.\n",
    "\n",
    "<img src=\"img/6_4_15_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "In the event that there is some state $s\\in\\mathcal{S}$ for which $\\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)$ is not unique, there is some flexibility in how the improved policy $\\pi'$ is constructed.\n",
    "\n",
    "In fact, as long as the policy $\\pi'$ satisfies for each $s\\in\\mathcal{S}$ and $a\\in\\mathcal{A}(s)$:\n",
    "\n",
    "$\\pi'(a|s) = 0$ if a $\\notin \\arg\\max_{a'\\in\\mathcal{A}(s)}Q(s,a')$,\n",
    "\n",
    "it is an improved policy. In other words, any policy that (for each state) assigns zero probability to the actions that do not maximize the action-value function estimate (for that state) is an improved policy. Feel free to play around with this in your implementation!\n",
    "\n",
    "Please use the next concept to complete **Part 3: Policy Improvement** of `Dynamic_Programming.ipynb`. Remember to save your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.16) Mini Project: DP (Parts 3)\n",
    "- my note before going to the cells of the project: since `policy` is initialized with zeros, the only items which are modified in `policy` will have a probability higher than zero.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will write your own implementation of policy improvement. \n",
    "\n",
    "Your algorithm should accept three arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "\n",
    "Please complete the function in the code cell below.  You are encouraged to use the `q_from_v` function you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env, V, gamma=1):\n",
    "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    for s in range(env.nS):\n",
    "        q = q_from_v(env, V, s, gamma)\n",
    "                \n",
    "        # OPTION 1: construct a deterministic policy \n",
    "        # policy[s][np.argmax(q)] = 1\n",
    "        \n",
    "        # OPTION 2: construct a stochastic policy that puts equal probability on maximizing actions\n",
    "        best_a = np.argwhere(q==np.max(q)).flatten()\n",
    "        policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### demonstration of the previous code \n",
    "- `policy = np.zeros([env.nS, env.nA]) / env.nA`\n",
    "    - defines a table of the policy and initialize it with all zeros . (the devide by env.nA is not needed and i think it is a result of copying and pasting from another line) <br><br>\n",
    "    \n",
    "- `q = q_from_v(env, V, s, gamma)`    \n",
    "    - remeber that q is the action value function for a single state. (it has the action values for each action when taken in that state)\n",
    "    - the shape of the action value function (here) is like this :\n",
    "    \n",
    "    <img src=\"img/6_4_16_1.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/> <br><br>\n",
    "    \n",
    "- `np.argmax(q)`\n",
    "    - for the largest action value, return its argument (here is the index of the ndarray) <br><br>\n",
    "    \n",
    "- `np.argwhere(q==np.max(q)).flatten()`\n",
    "    - returns an vertcal array of indecies that meet the condition (q==np.max(q))\n",
    "    - note that the returned array has each element an array of indecies (i think that is because the indecies may be two elements in a 2D arraym or 3 elements in a 3D array etc. so even when the condition is on a 1D array, like here, each element is an array. that's why we need to use flatten() which makes all the element in a single horizontal array) <br><br>\n",
    "    \n",
    "- `policy[s] = np.sum([np.eye(env.nA)[i] for i in best_a], axis=0)/len(best_a)`    \n",
    "    - look at the following picture\n",
    "    <img src=\"img/6_4_16_2.jpg\" alt=\"Drawing\" style=\"width: 700px;\"/> <br><br><br>\n",
    "    \n",
    "    \n",
    "- i need to know the name of the method (in python) of creating an array in this way : <br>\n",
    "`dummy_arr = [np.eye(env.nA)[i] for i in best_a]` \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.17) Policy Iteration\n",
    "\n",
    "- At this point in the lesson, you've used **policy evaluation** to determine how good a policy is by calculating its value function. You've also used **policy improvement** which uses the value function for a policy to construct a new policy that's better than or equal to the current one. \n",
    "\n",
    "<img src=\"img/6_4_17_1.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "- I mentioned that it will make sense to **combine these two algorithms** to produce an algorithm that successively proposes better and better policies. The name for the algorithm that combines these two steps is **policy iteration** and it's our current focus. \n",
    "\n",
    "<img src=\"img/6_4_17_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- The algorithm begins with  (look at the picture below)\n",
    "    - an initial guess for the optimal policy. It makes sense to begin with the equal probable random policy where for each state each action is equally likely to be chosen. \n",
    "    - Then we'll use policy evaluation to obtain the corresponding value function. \n",
    "    - Next, we'll use policy improvement to obtain a better or equivalent policy. \n",
    "    - Then we just repeat this loop over and over with policy evaluation and then policy improvement until finally we encounter a policy improvement step that doesn't result in any change to the policy. \n",
    "    \n",
    "<img src=\"img/6_4_17_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>    \n",
    "    \n",
    "- And, what's great is that in the case of a **<u>finite</u> MDP**, we have **guaranteed convergence to the optimal policy**. In the next concept, you'll have the chance to combine all the code you've already written to finally help your agent use policy iteration to obtain an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.18) Implementation\n",
    "In the previous concept, you learned about **policy iteration**, which proceeds as a series of alternating policy evaluation and improvement steps. Policy iteration is guaranteed to find the optimal policy for any finite Markov decision process (MDP) in a finite number of iterations. The pseudocode can be found below.\n",
    "\n",
    "<img src=\"img/6_4_18_1.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "Please use the next concept to complete **Part 4: Policy Iteration** of `Dynamic_Programming.ipynb`. Remember to save your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.19) Mini Project: DP (Parts 4)\n",
    "#### Part 4: Policy Iteration\n",
    "\n",
    "In this section, you will write your own implementation of policy iteration.  The algorithm returns the optimal policy, along with its corresponding state-value function.\n",
    "\n",
    "Your algorithm should accept three arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `theta`: This is a very small positive number that is used to decide if the policy evaluation step has sufficiently converged to the true value function (default value: `1e-8`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "\n",
    "Please complete the function in the code cell below.  You are strongly encouraged to use the `policy_evaluation` and `policy_improvement` functions you implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def policy_iteration(env, gamma=1, theta=1e-8):\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    while True:\n",
    "        V = policy_evaluation(env, policy, gamma, theta)\n",
    "        new_policy = policy_improvement(env, V)\n",
    "        \n",
    "        # OPTION 1: stop if the policy is unchanged after an improvement step\n",
    "        if (new_policy == policy).all():\n",
    "            break;\n",
    "        \n",
    "        # OPTION 2: stop if the value function estimates for successive policies has converged\n",
    "        # if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) < theta*1e2:\n",
    "        #    break;\n",
    "        \n",
    "        policy = copy.copy(new_policy)\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### demonstration of the new concepts of the previous code \n",
    "- `(new_policy == policy).all()`\n",
    "    - `new_policy == policy` is an 1D boolean array. and u can't test a boolean array, this will give an error : \"The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()\" that's why we use `.all()`\n",
    "    - see these links of the documentaion : \n",
    "        - https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.all.html\n",
    "        - https://docs.scipy.org/doc/numpy/reference/generated/numpy.all.html\n",
    "        - https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.any.html\n",
    "        - https://docs.scipy.org/doc/numpy/reference/generated/numpy.any.html <br><br>\n",
    "        \n",
    "- if np.max(abs(policy_evaluation(env, policy) - policy_evaluation(env, new_policy))) < theta*1e2:\n",
    "    - lets break it down\n",
    "        - the RHS is straigh forward. it is theta multiplied by 100 (i don't know why he did mul by 100, maybe because convergence did not loop `theta` times.).\n",
    "        - LHS\n",
    "            - max of \n",
    "                - abs of \n",
    "                    - subtraction of two things :\n",
    "                    - 1) policy_evaluation(env, policy)\n",
    "                    - 2) policy_evaluation(env, new_policy)\n",
    "    - remember that `policy_evaluation func` returns an array `V`, so we are making sure that the maximum change in any state value function is less than theta*100 before we break (calling that the policy is stable) <br><br>\n",
    "    \n",
    "- `policy = copy.copy(new_policy)`   \n",
    "    - first, i want u to note that this line is inside the while-loop (notice the indetation) (i stress on this b/c i was fooled by the sight of the break statements, and i thought that this line is outside the while-loop)\n",
    "    - similar to improting numpy as np, here we imported copy but without an alias so that's why we use it as is (we could've imported it as \"`import copy as cp`\", then our line will be written as \"`cp.copy(new_policy)`\" )\n",
    "    - now, to copy an array to another, you have three methods : assignmet, shallow copy, deep copy\n",
    "        - they are illustrated in this link https://medium.com/@thawsitt/assignment-vs-shallow-copy-vs-deep-copy-in-python-f70c2f0ebd86\n",
    "    - look at this link for the difference between np.copy and copy.copy\n",
    "        - https://stackoverflow.com/questions/50282638/python-copy-copy-vs-numpy-np-copy\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to solve the MDP and visualize the output.  The optimal state-value function has been reshaped to match the shape of the gridworld. (my note: becuase `V` is 16x1, he must have made it 4x4 to visulaize it better)\n",
    "\n",
    "**Compare the optimal state-value function to the state-value function from Part 1 of this notebook**.  _Is the optimal state-value function consistently greater than or equal to the state-value function for the equiprobable random policy?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the optimal policy and optimal state-value function\n",
    "policy_pi, V_pi = policy_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_pi,\"\\n\")\n",
    "\n",
    "plot_values(V_pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_4_19_1.png\" alt=\"Drawing\" style=\"width: 440px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.20) Truncated Policy Iteration\n",
    "\n",
    "- Congratulations! You've implemented your first algorithm that can solve an MDP. For the remainder of this lesson, we'll look at some variations of this algorithm, and you'll have the chance to implement all of them to compare their performance. \n",
    "    - We'll begin by looking at this policy evaluation step. \n",
    "    \n",
    "<img src=\"img/6_4_20_1.png\" alt=\"Drawing\" style=\"width: 440px;\"/> <br>    \n",
    "    \n",
    "- _    \n",
    "    - Remember that policy evaluation is an iterative algorithm where we rely on a Bellman update rule. The number of iterations needed for a single round of policy evaluation is decided according to some small positive number theta that you set.(look at the picture below) \n",
    "    \n",
    "<img src=\"img/6_4_20_2.png\" alt=\"Drawing\" style=\"width: 440px;\"/> <br>        \n",
    "    \n",
    "- _    \n",
    "    - And the closer you wanted your approximate state by your function to the true value function, the smaller you had to make this hyperparameter theta. But now the question is, can we sacrifice some accuracy here? After all, who's to say how long this policy evaluation algorithm should take. Smaller theta means it will take longer, but exactly how much longer will depend on your MDP. Maybe, instead of terminating the algorithm with this (look at the highlights in thi pic below) stopping criterion, we can give an absolute number of iterations that were willing to calculate. \n",
    "    \n",
    "<img src=\"img/6_4_20_3.png\" alt=\"Drawing\" style=\"width: 440px;\"/> <br>     \n",
    "    \n",
    "- _    \n",
    "    - So we could modify this policy evaluation step to instead terminate after updating the values of each of the states, maybe once, twice, or some positive integer number of times. **The idea is that if our goal is to get an <u>optimal policy</u>, we aren't prevented from doing that by having a <u>value function</u> that's a little bit off**, <u> so we don't need to wait for a really good estimate of the value function before calculating an improved policy</u>. \n",
    "        - For instance, say this is the optimal <u>actual value</u> function for a hypothetical MDP. \n",
    "    \n",
    "<img src=\"img/6_4_20_4.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>         \n",
    "    \n",
    "- _\n",
    "    - _\n",
    "        - One optimal policy then can be obtained by looking at each state separately, and then picking the action that maximizes the function. \n",
    "        \n",
    "<img src=\"img/6_4_20_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>         \n",
    "\n",
    "- _\n",
    "    - _\n",
    "        - But say we don't have this optimal action value function, say instead we're working with a wildly incorrect estimate, where all the values are way off, but the relative values between many state action pairs are correct. \n",
    "        \n",
    "        \n",
    "<img src=\"img/6_4_20_6.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>        \n",
    "        \n",
    "- _\n",
    "    - _\n",
    "        - Then if we use this technically incorrect value function to obtain a policy, that policy will in fact be the same optimal policy\n",
    "        \n",
    "<img src=\"img/6_4_20_7.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- _\n",
    "    - _\n",
    "        - But that's just the main idea: We don't need to have a perfect <u>or even</u> near perfect estimate of the value function to get an optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.21) Implementation\n",
    "#### Implementation: Truncated Policy Iteration\n",
    "In the previous concept, you learned about **truncated policy evaluation**. Whereas <u>(iterative) policy evaluation</u> applies as many Bellman updates as needed to attain convergence, <u>truncated policy evaluation</u> only performs a fixed number of sweeps through the state space.\n",
    "\n",
    "The pseudocode can be found below.\n",
    "\n",
    "<img src=\"img/6_4_21_1.png\" alt=\"Drawing\" style=\"width: 646px;\"/> <br>\n",
    "\n",
    "We can incorporate this amended **policy evaluation** algorithm **into** an algorithm similar to policy iteration, called **truncated policy iteration**.\n",
    "\n",
    "The pseudocode can be found below.\n",
    "\n",
    "<img src=\"img/6_4_21_2.png\" alt=\"Drawing\" style=\"width: 646px;\"/> <br>\n",
    "\n",
    "(Q\\ when i compare this \"truncated policy iteration algorithm\" with the previous \"policy iteraction algorithm\" at `(6-4.18) Implementation`, i want to know why there is change in the order of doing the `policy improvement` and the `policy evaluation` lines between the two algorithms ?!)\n",
    "\n",
    "You may also notice that the stopping criterion for truncated policy iteration differs from that of policy iteration. In policy iteration, we terminated the loop when the policy was unchanged after a single policy improvement step. In truncated policy iteration, we stop the loop only when the value function estimate has converged.\n",
    "\n",
    "You are strongly encouraged to try out both stopping criteria, to build your intuition. However, we note that checking for an unchanged policy is unlikely to work if the hyperparameter `max_iterations` is set too small. (To see this, consider the case that `max_iterations` is set to a small value. Then even if the algorithm is far from convergence to the optimal value function $v_*$ or optimal policy $\\pi_*$, you can imagine that updates to the value function estimate $V$ may be too small to result in any updates to its corresponding policy.)\n",
    "\n",
    "Please use the next concept to complete **Part 5: Truncated Policy Iteration** of `Dynamic_Programming.ipynb`. Remember to save your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.22) Mini Project: DP (Parts 5)\n",
    "#### Part 5: Truncated Policy Iteration\n",
    "\n",
    "In this section, you will write your own implementation of truncated policy iteration.  \n",
    "\n",
    "You will begin by implementing truncated policy evaluation.  Your algorithm should accept five arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "- `max_it`: This is a positive integer that corresponds to the number of sweeps through the state space (default value: `1`).\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_policy_evaluation(env, policy, V, max_it=1, gamma=1):\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    num_it=0\n",
    "    while num_it < max_it:\n",
    "        for s in range(env.nS):\n",
    "            v = 0\n",
    "            q = q_from_v(env, V, s, gamma)\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "                v += action_prob * q[a]\n",
    "            V[s] = v\n",
    "        num_it += 1\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you will implement truncated policy iteration.  Your algorithm should accept five arguments as **input**:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `max_it`: This is a positive integer that corresponds to the number of sweeps through the state space (default value: `1`).\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `theta`: This is a very small positive number that is used for the stopping criterion (default value: `1e-8`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`.\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_policy_iteration(env, max_it=1, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    policy = np.zeros([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    while True:\n",
    "        policy = policy_improvement(env, V)\n",
    "        old_V = copy.copy(V)\n",
    "        V = truncated_policy_evaluation(env, policy, V, max_it, gamma)\n",
    "        if max(abs(V-old_V)) < theta:\n",
    "            break;\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next code cell to solve the MDP and visualize the output.  The state-value function has been reshaped to match the shape of the gridworld.\n",
    "\n",
    "Play with the value of the `max_it` argument.  Do you always end with the optimal state-value function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_tpi, V_tpi = truncated_policy_iteration(env, max_it=2)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_tpi,\"\\n\")\n",
    "\n",
    "# plot the optimal state-value function\n",
    "plot_values(V_tpi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_4_22_1.png\" alt=\"Drawing\" style=\"width: 450px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.23) Value Iteration\n",
    "- note before the video :\n",
    "    - u need to focus in the derivations in this video\n",
    "---\n",
    "- So we have talked about **policy iteration**. \n",
    "\n",
    "<img src=\"img/6_4_23_1.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "- We have also learned about **truncated policy iteration** (pic below). In this case, the policy evaluation step is permitted only a limited number of sweeps through the state space. In other words, we limit the number of times that the estimated value of each state is updated before proceeding to the policy improvement round. \n",
    "\n",
    "<img src=\"img/6_4_23_2.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "- In this video, you will learn about **Value Iteration**, where the policy evaluation step is stopped after a single sweep (so it is a special case from the **truncated policy iteration**. but i think this simplification will allow us to do lots of simplifiactions in the code of the algorithms (as we will see)). It is really as simple as that, but it turns out that you can simplify the underlying code, and we will derive the simple form together over the remainder of this video. \n",
    "\n",
    "<img src=\"img/6_4_23_3.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "\n",
    "- So let us write down the pseudo code corresponding to each of these evaluation and improvement steps. \n",
    "\n",
    "<img src=\"img/6_4_23_4.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "- And the first thing to notice is that we can collapse these lines (in the pic below) to remove a bit of redundancy. In particular, the action value from the first line can be plugged in where it appears in the second line. \n",
    "\n",
    "<img src=\"img/6_4_23_5.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "- Now recall that this policy improvements step is followed by another policy evaluation step. Pi prime is used as the new value of policy pi, which is then put through a single sweep of policy evaluation. \n",
    "\n",
    "<img src=\"img/6_4_23_6.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "\n",
    "- Okay, so we can quickly see another bit of redundancy here. Instead of updating pi-prime and then immediately using it to set the new value of policy pi (left pic), what we can do is just directly update the value of policy pi from the value function (right pic). (note that pi prime is removed from the left pic, and we put pi instead in the line of policy improvement)\n",
    "\n",
    "|left pic| right pic|\n",
    "| --- | --- |\n",
    "| <img src=\"img/6_4_23_7.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>|<img src=\"img/6_4_23_8.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>|\n",
    "\n",
    "\n",
    "- Next, we'll need to look at these two equations (that we ended with in the right pic above) a bit closer. And when we do that, we notice that they're nearly the same. \n",
    "\n",
    "<img src=\"img/6_4_23_9.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "\n",
    "- In particular, everything that appears after the sum is nearly identical, with the exception of the values here. (notice the yellow highlight) \n",
    "\n",
    "<img src=\"img/6_4_23_10.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "- It turns out that we can combine these two lines to produce a single line that accomplishes the same effect. Towards that, we'll first note that pi of S is the ARG Max over all actions of some semi-complicated expression, but we can think of this long thing as a function of the action A. \n",
    "\n",
    "<img src=\"img/6_4_23_11.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "- Then, V of S is a sign that same function, but evaluated at pi of S. \n",
    "\n",
    "<img src=\"img/6_4_23_12.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "- So if pi of S is set to the ARG Max and V of S this is set to the function evaluated at that ARG Max, well, then we could just directly set V of S to the max of the function . (this is exactly the same thing as the example i illustrated about the difference between arg max and max at handwritten picture at (6-3.12) Quiz: Optimal Policies where i wrote : \"hencem max f(x) = f(arg max f(x))\" )\n",
    "- and so we can plug this in to our algorithm. \n",
    "\n",
    "<img src=\"img/6_4_23_13.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "- (now she will talk anout the value iteration algorithm after the simplifications)\n",
    "\n",
    "<img src=\"img/6_4_23_14.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- We begin with an initial guess of the value function. \n",
    "- The algorithm loops over the state space and successively applies the update rule to get a better guess at the value function. \n",
    "- After each update, we check to see if the value function estimate has converged. \n",
    "- And in case it has, we stop this part of the algorithm. \n",
    "- After convergence, there is one last step to get the policy corresponding to the final value function. <br><br>\n",
    "\n",
    "\n",
    "- Once you are ready, follow the instructions in the next concept to implement this algorithm yourself.\n",
    "\n",
    "\n",
    "Q\\ why this simplification is allowed now and was not allowed before the vlaue iteraton algorithm ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.24) Implementation\n",
    "#### Implementation: Value Iteration\n",
    "In the previous concept, you learned about **value iteration**. In this algorithm, each sweep over the state space effectively performs both policy evaluation and policy improvement. Value iteration is guaranteed to find the optimal policy $\\pi_*$ for any finite MDP.\n",
    "\n",
    "The pseudocode can be found below.\n",
    "\n",
    "<img src=\"img/6_4_24_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "Note that the stopping criterion is satisfied when the difference between successive value function estimates is sufficiently small. In particular, the loop terminates if the difference is less than $\\theta$ for each state. And, the closer we want the final value function estimate to be to the optimal value function, the smaller we need to set the value of $\\theta$.\n",
    "\n",
    "Feel free to play around with the value of $\\theta$ in your implementation; note that in the case of the FrozenLake environment, values around `1e-8` seem to work reasonably well.\n",
    "\n",
    "- For those of you who are interested in <u>*more rigorous*</u> guidelines on how exactly to set the value of $\\theta$, you might be interested in perusing [this paper](http://www.leemon.com/papers/1993wb2.pdf), where you are encouraged to pay particular attention to Theorem 3.2. Their main result of interest can be summarized as follows:\n",
    "\n",
    "    - Let $V^{\\text{final}}$ denote the final value function estimate that is calculated by the algorithm. Then it can be shown that $V^{\\text{final}}$ differs from the optimal value function $v_*$ by at most $\\frac{2\\theta\\gamma}{1-\\gamma}$. In other words, for each $s\\in\\mathcal{S}$,\n",
    "    - $\\max_{s\\in\\mathcal{S}}|V^{\\text{final}}(s) - v_*(s)| < \\frac{2\\theta\\gamma}{1-\\gamma}$. <br><br>\n",
    "\n",
    "Please use the next concept to complete **Part 6: Value Iteration** of `Dynamic_Programming.ipynb`. Remember to save your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.25) Mini Project: DP (Parts 6)\n",
    "#### Part 6: Value Iteration\n",
    "\n",
    "In this section, you will write your own implementation of value iteration.\n",
    "\n",
    "Your algorithm should accept three arguments as input:\n",
    "- `env`: This is an instance of an OpenAI Gym environment, where `env.P` returns the one-step dynamics.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "- `theta`: This is a very small positive number that is used for the stopping criterion (default value: `1e-8`).\n",
    "\n",
    "The algorithm returns as **output**:\n",
    "- `policy`: This is a 2D numpy array with `policy.shape[0]` equal to the number of states (`env.nS`), and `policy.shape[1]` equal to the number of actions (`env.nA`).  `policy[s][a]` returns the probability that the agent takes action `a` while in state `s` under the policy.\n",
    "- `V`: This is a 1D numpy array with `V.shape[0]` equal to the number of states (`env.nS`).  `V[s]` contains the estimated value of state `s`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.nS)\n",
    "    \n",
    "    ## TODO: complete the function\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.nS):\n",
    "            v = V[s]\n",
    "            V[s] = max(q_from_v(env, V, s, gamma))\n",
    "            delta = max(delta,abs(V[s]-v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    \n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as a summary for me and a reminder : `policy_improvement func` gets `V` and calcs `q`, then uses `q` to set best actions in the `policy` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Use the next code cell to solve the MDP and visualize the output.  The state-value function has been reshaped to match the shape of the gridworld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_vi, V_vi = value_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_vi,\"\\n\")\n",
    "\n",
    "# plot the optimal state-value function\n",
    "plot_values(V_vi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_4_25_1.png\" alt=\"Drawing\" style=\"width: 450px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.26) Check Your Understadning\n",
    "Congratulations! At this point in the lesson, you have written your own implementations of many classical dynamic programming algorithms. This is no easy feat, and you should be proud of all of your hard work!\n",
    "\n",
    "We encourage you to take your time with this content. Tinker more with the mini project to develop your intuition, and read Chapter 4 (especially 4.1-4.4) of the textbook to supplement your understanding.\n",
    "\n",
    "**You are strongly encouraged to take your own notes**. You may find it useful to compare your notes with the next concept, which contains a summary of the main ideas from the lesson.\n",
    "\n",
    "When you're ready, answer the question below to check your memory of the terminology.\n",
    "\n",
    "<img src=\"img/6_4_26_1.png\" alt=\"Drawing\" style=\"width: 750px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-4.27) Summary\n",
    "\n",
    "<img src=\"img/6_4_27_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "#### Introduction\n",
    "- In the **dynamic programming** setting, the agent has full knowledge of the MDP. (This is much easier than the **reinforcement learning** setting, where the agent initially knows nothing about how the environment decides state and reward and must learn entirely from interaction how to select actions.)\n",
    "\n",
    "#### An Iterative Method\n",
    "- In order to obtain the state-value function $v_\\pi$ corresponding to a policy $\\pi$, we need only solve the system of equations corresponding to the Bellman expectation equation for $v_\\pi$.\n",
    "- While it is possible to analytically solve the system, we will focus on an iterative solution approach.\n",
    "\n",
    "#### Iterative Policy Evaluation\n",
    "- **Iterative policy evaluation** is an algorithm used in the dynamic programming setting to estimate the state-value function $v_\\pi$ corresponding to a policy $\\pi$. In this approach, a Bellman update is applied to the value function estimate until the changes to the estimate are nearly imperceptible.\n",
    "\n",
    "<img src=\"img/6_4_27_2.png\" alt=\"Drawing\" style=\"width: 430px;\"/> <br>\n",
    "\n",
    "#### Estimation of Action Values\n",
    "- In the dynamic programming setting, it is possible to quickly obtain the action-value function $q_\\pi$ from the state-value function $v_\\pi$ with the equation: $q_\\pi(s,a) = \\sum_{s'\\in\\mathcal{S}, r\\in\\mathcal{R}}p(s',r|s,a)(r+\\gamma v_\\pi(s'))$.\n",
    "\n",
    "<img src=\"img/6_4_27_3.png\" alt=\"Drawing\" style=\"width: 430px;\"/> <br>\n",
    "\n",
    "#### Policy Improvement\n",
    "- **Policy improvement** takes an estimate $V$ of the action-value function $v_\\pi$ corresponding to a policy $\\pi$, and returns an improved (or equivalent) policy $\\pi'$, where $\\pi'\\geq\\pi$. The algorithm first constructs the action-value function estimate $Q$. Then, for each state $s\\in\\mathcal{S}$, you need only select the action $a$ that maximizes $Q(s,a)$. In other words, $\\pi'(s) = \\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)$ for all $s\\in\\mathcal{S}$.\n",
    "\n",
    "<img src=\"img/6_4_27_4.png\" alt=\"Drawing\" style=\"width: 430px;\"/> <br>\n",
    "\n",
    "#### Policy Iteration\n",
    "- **Policy iteration** is an algorithm that can solve an MDP in the dynamic programming setting. It proceeds as a sequence of policy evaluation and improvement steps, and is guaranteed to converge to the optimal policy (for an arbitrary <u>*finite*</u> MDP).\n",
    "\n",
    "<img src=\"img/6_4_27_5.png\" alt=\"Drawing\" style=\"width: 430px;\"/> <br>\n",
    "\n",
    "#### Truncated Policy Iteration\n",
    "- **Truncated policy iteration** is an algorithm used in the dynamic programming setting to estimate the state-value function $v_\\pi$ corresponding to a policy $\\pi$. In this approach, the evaluation step is stopped after a fixed number of sweeps through the state space. We refer to the algorithm in the evaluation step as **truncated policy evaluation**.\n",
    "\n",
    "<img src=\"img/6_4_27_6.png\" alt=\"Drawing\" style=\"width: 430px;\"/> <br>\n",
    "\n",
    "#### Value Iteration\n",
    "- **Value iteration** is an algorithm used in the dynamic programming setting to estimate the state-value function $v_\\pi$ corresponding to a policy $\\pi$. In this approach, each sweep over the state space simultaneously performs policy evaluation and policy improvement.\n",
    "\n",
    "<img src=\"img/6_4_27_7.png\" alt=\"Drawing\" style=\"width: 430px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 5 : Monte Carlo Methods\n",
    "- see this post by eng. Mohamed Hammad : https://www.facebook.com/mohamed.hamedhammad/posts/2738975202841679"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (22 concepts)\n",
    "- 1- Introduction \n",
    "- 2- OpenAI Gym: BlackjackEnv\n",
    "- 3- MC Prediction: State Values\n",
    "- 4- Implementation\n",
    "- 5- Mini Project: MC (Parts 0 and 1)\n",
    "- 6- MC Prediction: Action Values\n",
    "- 7- Implementation\n",
    "- 8- Mini Project: MC (Parts 2)\n",
    "- 9- Generalized Policy Iteration\n",
    "- 10- MC Control: Incremental Mean\n",
    "- 11- Quiz: Incremental Mean\n",
    "- 12- MC Control: Policy Evaluation\n",
    "- 13- MC Control: Policy Improvement\n",
    "- 14- Quiz: Epsilon-Greedy Policies\n",
    "- 15- Exploration vs Exploitation\n",
    "- 16- Implementation\n",
    "- 17- Mini Project: MC (Parts 3)\n",
    "- 18- MC Control: Constant-alpha, Part 1\n",
    "- 19- MC Control: Constant-alpha, Part 2\n",
    "- 20- Implementation\n",
    "- 21- Mini Project: MC (Parts 4)\n",
    "- 22- Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.1) Introduction\n",
    "\n",
    "- So far, you've learned all about the Reinforcement Learning Framework. You've learned about agents, and environments, and how we specify MDPs. In the previous lesson, you looked at a simplification of the Reinforcement Learning problem. In the Dynamic Programming setting, the agent already had full knowledge of the environments dynamics. <br><br>\n",
    "\n",
    "- But in this lesson, and for all remaining lessons, we'll consider the Reinforcement Learning problem where the agent is not given full knowledge of how the environment operates, and instead, must learn from interaction.\n",
    "\n",
    "---\n",
    "This lesson covers material in **Chapter 5** (especially 5.1-5.6) of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.2) OpenAI Gym: BlackjackEnv\n",
    "In this lesson, you will write code to teach an agent to play Blackjack.\n",
    "\n",
    "<img src=\"img/6_5_2_1.png\" alt=\"Drawing\" style=\"width: 430px;\"/> <br>\n",
    "\n",
    "Please read about the game of Blackjack in Example 5.1 of the textbook.\n",
    "\n",
    "When you have finished, please review the corresponding [GitHub file](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py), by reading the commented block in the BlackjackEnv class. (*While you do **not** need to understand how all of the code works, please read the commented block that explains the dynamics of the environment.*) For clarity, we have also pasted the description of the environment below:\n",
    "\n",
    "~~~ **start .py file**\n",
    "```python\n",
    "    \"\"\"Simple blackjack environment\n",
    "\n",
    "    Blackjack is a card game where the goal is to obtain cards that sum to as\n",
    "    near as possible to 21 without going over.  They're playing against a fixed\n",
    "    dealer.\n",
    "    Face cards (Jack, Queen, King) have point value 10.\n",
    "    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
    "    This game is placed with an infinite deck (or with replacement).\n",
    "    The game starts with each (player and dealer) having one face up and one\n",
    "    face down card.\n",
    "\n",
    "    The player can request additional cards (hit=1) until they decide to stop\n",
    "    (stick=0) or exceed 21 (bust).\n",
    "\n",
    "    After the player sticks, the dealer reveals their facedown card, and draws\n",
    "    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
    "\n",
    "    If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
    "    decided by whose sum is closer to 21.  The reward for winning is +1,\n",
    "    drawing is 0, and losing is -1.\n",
    "\n",
    "    The observation of a 3-tuple of: the players current sum,\n",
    "    the dealer's one showing card (1-10 where 1 is ace),\n",
    "    and whether or not the player holds a usable ace (0 or 1).\n",
    "\n",
    "    This environment corresponds to the version of the blackjack problem\n",
    "    described in Example 5.1 in Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto (1998).\n",
    "    http://incompleteideas.net/sutton/book/the-book.html\n",
    "    \"\"\"\n",
    "```\n",
    "~~~ **end .py file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.3) MC Prediction: State Values\n",
    "Before the video, i encourage u to read about MC in wiki : https://en.wikipedia.org/wiki/Monte_Carlo_method\n",
    "\n",
    "---\n",
    "- Let's recall the problem at hand. We have an agent and environment. Time has broken into discrete time steps, and every time step, the agent receives a reward and state from the environment and chooses an action to perform in response. In this way, the interaction evolves as a sequence of states, actions, and rewards. \n",
    "\n",
    "<img src=\"img/6_5_3_1.png\" alt=\"Drawing\" style=\"width: 330px;\"/> <br>\n",
    "\n",
    "- In this lesson, we'll confine our attention to <u>episodic tasks</u> where the interaction stops at some time step T when the agent encounters a terminal state. And we refer to this sequence as an episode. \n",
    "- For any episode, the agent's goal is to find the optimal policy in order to maximize expected cumulative reward. \n",
    "\n",
    "<img src=\"img/6_5_3_2.png\" alt=\"Drawing\" style=\"width: 430px;\"/> <br>\n",
    "\n",
    "- Towards this goal, we'll start with the **prediction problem:** Given the policy, how might the agent estimate the value function for that policy? Remember that the environment's dynamics are unknown to the agent. So it will have to estimate the value function by interacting with the environment. And in order to interact with the environment, the agent needs to have a policy in mind. \n",
    "\n",
    "<img src=\"img/6_5_3_3.png\" alt=\"Drawing\" style=\"width: 430px;\"/> <br>\n",
    "\n",
    "- Now, it's possible to have one policy that we'd like to evaluate and a different policy to interact with the environment. This is referred to as an <u>off-policy method</u> for the prediction problem. \n",
    "\n",
    "<img src=\"img/6_5_3_4.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- <u>Off-policy methods</u> entail some complexity that we'll save for later. \n",
    "- Instead, we'll start with an <u>On-policy method</u>, where the agent interacts with the environment by following a policy, whose value function it would like to calculate. \n",
    "\n",
    "<img src=\"img/6_5_3_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- Before we go into the details of the algorithm, let's look at a motivating example. (pic below) \n",
    "\n",
    "<img src=\"img/6_5_3_6.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- _\n",
    "    - Say we're working with an episodic task and the MDP has three states; X,Y, and Z, where Z has a terminal state. \n",
    "    - In case it helps to have a more visual understanding of the MDP, you can think of it as a very, very small frozen world. \n",
    "    - Say states X and Y correspond to different locations in a snowy forest with possible delights (مسرّات) and terrors(أشياء مرعبة) corresponding to possibly positive or negative reward. \n",
    "    - Remember state Z is the terminal state and say it corresponds to a house in the forest. \n",
    "    - If the agent lands at state Z, it enters the house and the episode ends. \n",
    "    - Say there are two potential actions, up and down \n",
    "    - and similar to the frozen lake environment, the world is slippery. So if the agent chooses action down, at the next time step, there is some positive probability that it instead moves up <u>or</u> stays where it is. \n",
    "    - Likewise, if it decides to go up then when it tries to move in that direction, it might instead go down <u>or</u> again not move at all. \n",
    "    - And say we'd like to evaluate the policy where the agent chooses action up in state X and action down in State Y. Then towards this goal the agent could interact with the environment by following this policy. \n",
    "   \n",
    "<img src=\"img/6_5_3_7.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>   \n",
    "   \n",
    "- _\n",
    "    - _\n",
    "        - Say at the beginning of the interaction, the agent observes state X, then following the policy, it chooses action up, as a result. It receives a reward of negative two and the next state of Y. Again, following the policy, it chooses to go down, and as a result, gets a reward of zero and next state of Y. At this point, it follows the policy and chooses action down. As a result, it gets a reward of three and reaches the terminal stage Z, so the interaction ends. Say the agent interacts with the environment in two additional episodes. Then we can use these episodes to estimate the value function. Of course these three short episodes aren't really enough interaction for the agent to get a great understanding of the environment, but for now, let's assume that they're sufficient. \n",
    "        - We'll begin with one state, say state X. Then we look at all of the occurrences of state X and all of the episodes.\n",
    "        \n",
    "<img src=\"img/6_5_3_8.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br> \n",
    "\n",
    "- _\n",
    "    - _\n",
    "        - Then we just calculate the discounted return that followed after that state appeared (she means state X b/c the agent is observing state X after several number of episodes). Say the discount is one ($\\gamma=1$) so in other words, we won't discount. \n",
    "        - Then in the case of the first episode, the return is negative two, plus zero, plus three or one. And in the third episode, the return is negative three plus three or zero. \n",
    "        \n",
    "<img src=\"img/6_5_3_9.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>         \n",
    "        \n",
    "- _\n",
    "    - _\n",
    "        - The Monte Carlo prediction algorithm takes the average of these values and plugs it in as an estimate for the value of state X. In this case, the value of state X is estimated to be one half. \n",
    "        \n",
    "<img src=\"img/6_5_3_10.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br> \n",
    "\n",
    "- _\n",
    "    - _\n",
    "        - This algorithm makes some intuitive sense. Remember that the value of a state is defined to be the expected return after that state is observed. And so the average return that was experienced by the agent makes for a good estimate. \n",
    "        \n",
    "<img src=\"img/6_5_3_11.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>        \n",
    "  \n",
    "- _\n",
    "    - _\n",
    "        - We'll follow the same process to evaluate state Y. \n",
    "        \n",
    "<img src=\"img/6_5_3_12.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>        \n",
    "        \n",
    "- _\n",
    "    - _\n",
    "        - And you'll notice that state Y appears more than once in each episode and it's not quite clear what to do in this case. \n",
    "        - To address this, we'll need to define some additional terminology. \n",
    "            - We define every occurrence of a state in an episode as a **visit** to that state. \n",
    "            \n",
    "<img src=\"img/6_5_3_13.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>             \n",
    "            \n",
    "- _\n",
    "    - _\n",
    "        - _            \n",
    "            - And in the event where a state is visited, more than once in an episode we have two options. \n",
    "                - As a first option, we could for each episode only consider the first visit to the state and average those returns. In this case, the value of state Y would be estimated as the average of three, three and one or seven third's. If we choose this approach, then we say that we are using <u>the first visit MC method</u>. \n",
    "                \n",
    "<img src=\"img/6_5_3_14.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "<img src=\"img/6_5_3_15.png\" alt=\"Drawing\" style=\"width: 250px;\"/> <br>\n",
    "  \n",
    "- _\n",
    "    - _\n",
    "        - _            \n",
    "            - _  \n",
    "                - The other option is to average the return following all visits to state Y in all episodes. In this case, the value of State Y would be estimated as the average of all of these numbers, which can be calculated as fourteen sevenths or two. \n",
    "                \n",
    "<img src=\"img/6_5_3_16.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "<img src=\"img/6_5_3_17.png\" alt=\"Drawing\" style=\"width: 250px;\"/> <br>                \n",
    "                \n",
    "- You'll soon have the chance to implement this algorithm for yourself. Feel free to choose either first visit or every visit MC prediction, or if you like, implement both."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.4) Implementation\n",
    "- my question before the reading:\n",
    "    - I do not understand the `if-statement` in the algorithm! I mean, is there something wrong with \"$N(S_t) \\leftarrow N(S_t)+1$\"? I mean, how do we get the total number of the rewards that follows a state (while he only adds one when that state appears for the first time !) \n",
    "---\n",
    "The pseudocode for (first-visit) MC prediction (for the state values) can be found below. (*Feel free to implement either the first-visit or every-visit MC method. In the game of Blackjack, both the first-visit and every-visit methods return identical results.*)\n",
    "\n",
    "<img src=\"img/6_5_4_1.png\" alt=\"Drawing\" style=\"width: 615px;\"/> <br>\n",
    "\n",
    "If you are interested in learning more about the difference between first-visit and every-visit MC methods, you are encouraged to read Section 3 of [this paper](http://www-anw.cs.umass.edu/legacy/pubs/1995_96/singh_s_ML96.pdf).\n",
    "- Their results are summarized in Section 3.6. The authors show:\n",
    "    - Every-visit MC is [biased](https://en.wikipedia.org/wiki/Bias_of_an_estimator), whereas first-visit MC is unbiased (see Theorems 6 and 7).\n",
    "    - Initially, every-visit MC has [lower mean squared error (MSE)](https://en.wikipedia.org/wiki/Mean_squared_error), but as more episodes are collected, first-visit MC attains better MSE (see Corollary 9a and 10a, and Figure 4).\n",
    "    \n",
    "Both the first-visit and every-visit method are **guaranteed to converge** to the true value function, as the number of visits to each state approaches infinity. (*So, in other words, as long as the agent gets enough experience with each state, the value function estimate will be pretty close to the true value.*) In the case of first-visit MC, convergence follows from the [Law of Large Numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers), and the details are covered in section 5.1 of the textbook.\n",
    "\n",
    "Please use the next concept to complete **Part 0: Explore BlackjackEnv** and **Part 1: MC Prediction: State Values** of `Monte_Carlo.ipynb`. Remember to save your work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.5) Mini Project: MC (Parts 0 and 1)\n",
    "- before the project\n",
    "    - I do not fully understand the code and what it does, i think i need to make animation to visualize the `code` & `data types changes` while the code runs . (i think i must do this systematically using scripts in **After Effects**)\n",
    "    - also i do not understand the plots that is generated ! or what should i infer from it ?!\n",
    "---\n",
    "#### Part 0: Explore BlackjackEnv\n",
    "\n",
    "Use the code cell below to create an instance of the [Blackjack](https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py) environment.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Each state is a 3-tuple of:\n",
    "- the player's current sum $\\in \\{0, 1, \\ldots, 31\\}$,\n",
    "- the dealer's face up card $\\in \\{1, \\ldots, 10\\}$, and\n",
    "- whether or not the player has a usable ace (`no` $=0$, `yes` $=1$).\n",
    "\n",
    "The agent has two potential actions:\n",
    "\n",
    "```\n",
    "    STICK = 0\n",
    "    HIT = 1\n",
    "```\n",
    "Verify this by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output :\n",
    "<img src=\"img/6_5_5_1.png\" alt=\"Drawing\" style=\"width: 363px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code cell below to play Blackjack with a random policy.  \n",
    "\n",
    "(_The code currently plays Blackjack three times - feel free to change this number, or to run the cell multiple times.  The cell is designed for you to get some experience with the output that is returned as the agent interacts with the environment._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i_episode in range(3):\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        print(state)\n",
    "        action = env.action_space.sample()\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print('End game! Reward: ', reward)\n",
    "            print('You won :)\\n') if reward > 0 else print('You lost :(\\n')\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "<img src=\"img/6_5_5_2.png\" alt=\"Drawing\" style=\"width: 187px;\"/> <br>\n",
    "\n",
    "note that the third one , i think the agent had sum =13, then he drew a card of 5, so the sum became 18  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: MC Prediction: State Values\n",
    "\n",
    "In this section, you will write your own implementation of MC prediction (for estimating the state-value function).\n",
    "\n",
    "We will begin by investigating a policy where the player always sticks if the sum of her cards exceeds 18.  The function `generate_episode_from_limit` samples an episode using this policy. \n",
    "\n",
    "The function accepts as **input**:\n",
    "- `bj_env`: This is an instance of OpenAI Gym's Blackjack environment.\n",
    "\n",
    "It returns as **output**:\n",
    "- `episode`: This is a list of (state, action, reward) tuples (of tuples) and corresponds to $(S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_{T})$, where $T$ is the final time step.  In particular, `episode[i]` returns $(S_i, A_i, R_{i+1})$, and `episode[i][0]`, `episode[i][1]`, and `episode[i][2]` return $S_i$, $A_i$, and $R_{i+1}$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_limit(bj_env):\n",
    "    episode = []\n",
    "    state = bj_env.reset()\n",
    "    while True:\n",
    "        action = 0 if state[0] > 18 else 1\n",
    "        next_state, reward, done, info = bj_env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code cell below to play Blackjack with the policy. \n",
    "\n",
    "(*The code currently plays Blackjack three times - feel free to change this number, or to run the cell multiple times.  The cell is designed for you to gain some familiarity with the output of the `generate_episode_from_limit` function.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    print(generate_episode_from_limit(env))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "<img src=\"img/6_5_5_3.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready to write your own implementation of MC prediction.  Feel free to implement either first-visit or every-visit MC prediction; in the case of the Blackjack environment, the techniques are equivalent.\n",
    "\n",
    "Your algorithm has three arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `generate_episode`: This is a function that returns an episode of interaction.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `V`: This is a dictionary where `V[s]` is the estimated value of state `s`.  For example, if your code returns the following output:\n",
    "```\n",
    "{(4, 7, False): -0.38775510204081631, (18, 6, False): -0.58434296365330851, (13, 2, False): -0.43409090909090908, (6, 7, False): -0.3783783783783784, ...\n",
    "```\n",
    "then the value of state `(4, 7, False)` was estimated to be `-0.38775510204081631`.\n",
    "\n",
    "If you are unfamiliar with how to use `defaultdict` in Python, you are encouraged to check out [this source](https://www.accelebrate.com/blog/using-defaultdict-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def mc_prediction_v(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    # initialize empty dictionary of lists\n",
    "    returns = defaultdict(list)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        # generate an episode\n",
    "        episode = generate_episode(env)\n",
    "        # obtain the states, actions, and rewards\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        # prepare for discounting\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        # calculate and store the return for each visit in the episode\n",
    "        for i, state in enumerate(states):\n",
    "            returns[state].append(sum(rewards[i:]*discounts[:-(1+i)]))\n",
    "    # calculate the state-value function estimate\n",
    "    V = {k: np.mean(v) for k, v in returns.items()}\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### demonstration of the new concepts of the previous code \n",
    "مبدأيا أنا فاهم شرح الخوارزمية من الفيديوهات ، وفاهم الكود نفسه لكن مش فاهم علاقته المباشرة بالخوارزمية اللى فى الفيديوهات ، لكن مش فاهم السودوكود اللى مكتوبة بيها الخوارزمية <br><br>\n",
    "أعتقد الكود محتاج أعمل عليه مثال بالداتا ستركشرز و الأعداد\n",
    "\n",
    "لاحظ إن السودوكود، هى فرست فيزيت، أما الكود، أعتقد هو إفرى فيزين و أبقى أتأكد\n",
    "\n",
    "أعتقد إنى فهمت السودوكود\n",
    "\n",
    "i will talk about default dict vs normal dict <br>\n",
    "and why he imported sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to calculate and plot the state-value function estimate.  (_The code for plotting the value function has been borrowed from [this source](https://github.com/dennybritz/reinforcement-learning/blob/master/lib/plotting.py) and slightly adapted._)\n",
    "\n",
    "To check the accuracy of your implementation, compare the plot below to the corresponding plot in the solutions notebook **Monte_Carlo_Solution.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import plot_blackjack_values\n",
    "\n",
    "# obtain the value function\n",
    "V = mc_prediction_v(env, 500000, generate_episode_from_limit)\n",
    "\n",
    "# plot the value function\n",
    "plot_blackjack_values(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "\n",
    "<img src=\"img/6_5_5_4.png\" alt=\"Drawing\" style=\"width: 174px;\"/> <br>  \n",
    "\n",
    "<img src=\"img/6_5_5_5.png\" alt=\"Drawing\" style=\"width: 837px;\"/> <br>\n",
    "\n",
    "<img src=\"img/6_5_5_6.png\" alt=\"Drawing\" style=\"width: 852px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what i should undersrtand from the polts :\n",
    "- what i understanded so far is that : if the agent has learned, then the plot will show high state value when the player's current sum is >= 18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.6) MC Prediction: Action Values\n",
    "\n",
    "- I hope you enjoyed implementing the first part of the mini project which used a Monte Carlo algorithm to address the prediction problem. \n",
    "- In the dynamic programming case, our next step was to use the State value function to obtain an action value function. \n",
    "\n",
    "<img src=\"img/6_5_6_1.png\" alt=\"Drawing\" style=\"width: 485px;\"/> <br>\n",
    "\n",
    "- Remember, that we used this equation to convert the state values to the action values. \n",
    "\n",
    "<img src=\"img/6_5_6_2.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>\n",
    "\n",
    "- And what do you think? Can we follow the same procedure here? Unfortunately, no. Remember that this underlined part of the equation (in the pic below) encodes the one step dynamics of the environment. This was known to the agent in the dynamic programming setting but in the reinforcement learning setting the agent does not know these dynamics so, we cannot directly apply this equation as before. \n",
    "\n",
    "<img src=\"img/6_5_6_3.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>\n",
    "\n",
    "- Instead, to get the action values, we will just make a small modification to our prediction algorithm. For clarity, let's return to the simple example from the previous video. \n",
    "\n",
    "<img src=\"img/6_5_6_4.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>\n",
    "\n",
    "- As before, say the agent interacts with the environment at each step choosing the action dictated by the policy. These (in the pic above) were the three episodes that resulted from that interaction. But now, towards getting the action values. Instead of looking at the visits to each state(left pic), we'll look at the visits to each possible state-action pair (right pic) \n",
    "\n",
    "| left pic | right pic\n",
    "| --- | --- |\n",
    "|<img src=\"img/6_5_6_5.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>|<img src=\"img/6_5_6_6.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>|\n",
    "\n",
    "- then we'll compute the return that followed from each state action pair and average them as before. In this case, the action value corresponding to State X and action up is estimated to be one half. \n",
    "\n",
    "<img src=\"img/6_5_6_7.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>\n",
    "\n",
    "- And when we follow the same process to evaluate State Y and action down, you'll notice that the pair appears more than once in each episode. To address this, we'll need to again define some terminology. \n",
    "\n",
    "<img src=\"img/6_5_6_8.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>\n",
    "\n",
    "- We define every occurrence of the state action pair in an episode as a visit to that pair. And if a pair is visited more than once in an episode we have our choice of either only taking into account the first-visit to the pair or using every-visit to calculate the action value estimate \n",
    "\n",
    "<img src=\"img/6_5_6_9.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>\n",
    "\n",
    "- and you'll notice that each option returns a different value where the first-visit method returns seven-thirds and the every-visit MC method returns two but if the agent gets more experience by collecting more episodes these values will converge to the same number. \n",
    "\n",
    "<img src=\"img/6_5_6_10.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>\n",
    "\n",
    "- So for now, let's just assume that we implement the first-visit MC method. \n",
    "\n",
    "<img src=\"img/6_5_6_11.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>\n",
    "\n",
    "- Okay and it is almost as simple as that. There's just one small problem that needs to be addressed. To see this, let's recall the policy that we are evaluating. It is a deterministic policy where the agent always selects action up when in State X and action down when in State Y, in particular,... (complete after the pic\\) \n",
    "\n",
    "<img src=\"img/6_5_6_12.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>\n",
    "\n",
    "- we'll never be able to estimate the action value corresponding to State X and action down or State Y and action up. \n",
    "\n",
    "<img src=\"img/6_5_6_13.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>\n",
    "\n",
    "- This is because under this policy the agent will never pick action down when in State X and it will never choose action up when in State Y. Remember that our algorithm can only estimate the action values corresponding to state-action pairs that it actually visits. So no matter how long the agent interacts with the environment, the action value function estimate will always be incomplete. \n",
    "- Thankfully, there is a quick solution to this problem that involves making sure that we do not try to evaluate the action value function for a deterministic policy.  ... (complete after the pic)\n",
    "\n",
    "<img src=\"img/6_5_6_14.png\" alt=\"Drawing\" style=\"width: 326px;\"/> <br>\n",
    "\n",
    "- Instead, we'll only work with stochastic policies where from each state, each action has some non-zero probability of being visited. \n",
    "    - For instance, if the agent encounters State X, say it selects action up with 90 percent probability and otherwise, selects action down and if it encounters State Y, it chooses action down with 80 percent probability and otherwise, selects action up.\n",
    "    \n",
    "<img src=\"img/6_5_6_15.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>    \n",
    "- _    \n",
    "    - For policies of this type, each state-action pair is eventually visited by the agent and furthermore, in the limit as the number of episodes goes to infinity so does the number of visits to each state-action pair. And that guarantees that we'll be able to calculate a nice action value function estimate for each state-action pair as long as the agent interacts with the environment in enough episodes. You'll have the chance to test this out yourself soon.\n",
    "    \n",
    "<img src=\"img/6_5_6_16.png\" alt=\"Drawing\" style=\"width: 581px;\"/> <br>    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.7) Implementation\n",
    "\n",
    "The pseudocode for (first-visit) MC prediction (for the action values) can be found below. (*Feel free to implement either the first-visit or every-visit MC method. In the game of Blackjack, both the first-visit and every-visit methods return identical results.*)\n",
    "\n",
    "<img src=\"img/6_5_7_1.png\" alt=\"Drawing\" style=\"width: 616px;\"/> <br>  \n",
    "\n",
    "Both the first-visit and every-visit methods are **guaranteed to converge** to the true value function (my Q: what is meant by value function ? is it the action value func? or the state value func??), as the number of visits to each state-action pair approaches infinity. (*So, in other words, as long as the agent gets enough experience with each state-action pair, the value function estimate will be pretty close to the true value.*)\n",
    "\n",
    "We won't use MC prediction to estimate the action-values corresponding to a deterministic policy; this is because many state-action pairs will *never* be visited (since a deterministic policy always chooses the same action from each state). Instead, so that convergence is guaranteed, we will only estimate action-value functions corresponding to policies where each action has a nonzero probability of being selected from each state.\n",
    "\n",
    "Please use the next concept to complete **Part 2: MC Prediction: Action Values** of `Monte_Carlo.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.8) Mini Project: MC (Parts 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2: MC Prediction: Action Values\n",
    "\n",
    "In this section, you will write your own implementation of MC prediction (for estimating the action-value function).  \n",
    "\n",
    "We will begin by investigating a policy where the player _almost_ always sticks if the sum of her cards exceeds 18.  In particular, she selects action `STICK` with 80% probability if the sum is greater than 18; and, if the sum is 18 or below, she selects action `HIT` with 80% probability.  The function `generate_episode_from_limit_stochastic` samples an episode using this policy. \n",
    "\n",
    "The function accepts as **input**:\n",
    "- `bj_env`: This is an instance of OpenAI Gym's Blackjack environment.\n",
    "\n",
    "It returns as **output**:\n",
    "- `episode`: This is a list of (state, action, reward) tuples (of tuples) and corresponds to $(S_0, A_0, R_1, \\ldots, S_{T-1}, A_{T-1}, R_{T})$, where $T$ is the final time step.  In particular, `episode[i]` returns $(S_i, A_i, R_{i+1})$, and `episode[i][0]`, `episode[i][1]`, and `episode[i][2]` return $S_i$, $A_i$, and $R_{i+1}$, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_limit_stochastic(bj_env):\n",
    "    episode = []\n",
    "    state = bj_env.reset()\n",
    "    while True:\n",
    "        probs = [0.8, 0.2] if state[0] > 18 else [0.2, 0.8]\n",
    "        action = np.random.choice(np.arange(2), p=probs)\n",
    "        next_state, reward, done, info = bj_env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you are ready to write your own implementation of MC prediction.  Feel free to implement either first-visit or every-visit MC prediction; in the case of the Blackjack environment, the techniques are equivalent.\n",
    "\n",
    "Your algorithm has three arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `generate_episode`: This is a function that returns an episode of interaction.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_prediction_q(env, num_episodes, generate_episode, gamma=1.0):\n",
    "    # initialize empty dictionaries of arrays\n",
    "    returns_sum = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    N = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        # generate an episode\n",
    "        episode = generate_episode(env)\n",
    "        # obtain the states, actions, and rewards\n",
    "        states, actions, rewards = zip(*episode)\n",
    "        # prepare for discounting\n",
    "        discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "        # update the sum of the returns, number of visits, and action-value \n",
    "        # function estimates for each state-action pair in the episode\n",
    "        for i, state in enumerate(states):\n",
    "            returns_sum[state][actions[i]] += sum(rewards[i:]*discounts[:-(1+i)])\n",
    "            N[state][actions[i]] += 1.0\n",
    "            Q[state][actions[i]] = returns_sum[state][actions[i]] / N[state][actions[i]]\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### demonstration of the new concepts of the previous code \n",
    "before talking about the code, i want to focus your attention to a trick they usually do in python here : \n",
    "in the code cell bellow, the `mc_prediction_q` func will have the func `generate_episode_from_limit_stochastic` as an argument ! i was first confused by this, i mean, why he did not just use `generate_episode_from_limit_stochastic` inside the `mc_prediction_q` (above) directly ??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to obtain the action-value function estimate $Q$.  We have also plotted the corresponding state-value function.\n",
    "\n",
    "To check the accuracy of your implementation, compare the plot below to the corresponding plot in the solutions notebook **Monte_Carlo_Solution.ipynb**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the action-value function\n",
    "Q = mc_prediction_q(env, 500000, generate_episode_from_limit_stochastic)\n",
    "\n",
    "# obtain the state-value function\n",
    "V_to_plot = dict((k,(k[0]>18)*(np.dot([0.8, 0.2],v)) + (k[0]<=18)*(np.dot([0.2, 0.8],v))) \\\n",
    "         for k, v in Q.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V_to_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output : \n",
    "\n",
    "<img src=\"img/6_5_8_1.png\" alt=\"Drawing\" style=\"width: 176px;\"/> <br>\n",
    "<img src=\"img/6_5_8_2.png\" alt=\"Drawing\" style=\"width: 832px;\"/> <br>\n",
    "<img src=\"img/6_5_8_3.png\" alt=\"Drawing\" style=\"width: 832px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "لاحظ إن الرسم هو لل <br> \n",
    "state value func, not the action value func"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.9) Generalized Policy Iteration\n",
    "\n",
    "- Now that we've explored the <u>Prediction Problem</u>, we're now ready to move on to the <u>Control Problem</u>. So, \n",
    "    - how might an agent learn an optimal policy through interaction with the environment? It's truly exciting that we've gotten to a point in the course where we can answer this question. <br>\n",
    "**my note**: note the distinction (in the pic) between the <u>Prediction Problem</u> and the <u>Control Problem</u>.\n",
    "    \n",
    "<img src=\"img/6_5_9_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>    \n",
    "    \n",
    "- To understand the algorithm for Monte-Carlo Control, It will prove useful to first revisit what we did in the <u>Dynamic Programming setting</u>. (pic below)\n",
    "- In that setting, \n",
    "    - the first control algorithm we examined was Policy Iteration. \n",
    "        - It proceeded as a sequence of evaluation and improvement steps where the evaluation step was allowed to run close to convergence. \n",
    "    - Truncated Policy Iteration was a little bit different \n",
    "        - where instead of forcing the evaluation staff to converge, we instead performed only a fixed number of sweeps through the state's spase. \n",
    "    - Value Iteration \n",
    "        - involved doing only one sweep in the evaluation step before advancing to the improvement step. \n",
    "        \n",
    "<img src=\"img/6_5_9_2.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>         \n",
    "        \n",
    "- Although these algorithms have some differences, it will prove useful to focus on what they have in common. And we'll use the term \"Generalized Policy Iteration\" to refer to this general process without imposing any constraints on how many sweeps of policy evaluation there are or how close it's allowed to run to convergence. And even though we developed these ideas and the dynamic programming setting, all of the reinforcement learning methods we'll investigate in this course fall under this format.\n",
    "\n",
    "<img src=\"img/6_5_9_3.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>         \n",
    "\n",
    "- In the next video, we'll use these ideas to specify an algorithm for Monte-Carlo Control."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.10) MC Control: Incremental Mean\n",
    "\n",
    "- Our <u>Monte Carlo control algorithm</u> will draw inspiration from <u>generalized policy iteration</u>. \n",
    "\n",
    "<img src=\"img/6_5_9_3.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br> \n",
    "\n",
    "- We'll begin with the policy evaluation step. We've already somewhat addressed how to accomplish this and you implemented it in part two of the mini project. In your implementation, the agent needs to play blackjack about 5,000 times to get a good estimate of the value function. \n",
    "\n",
    "<img src=\"img/6_5_10_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "(**my note**: here i made the agent to play for 150 times to see how does the value func look like if the agent did not interact enough, and the pic below is the result)\n",
    "\n",
    "<img src=\"img/6_5_10_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- In the context of policy iteration, this (she means: playing about 5000 times) seems like way too long to spend <u>evaluating</u> a policy before trying to <u>improve</u> it. Maybe it would make more sense to improve the policy after every individual game of blackjack. \n",
    "    - In that case, we could initialize the value of each state action pair to zero and have some starting policy. \n",
    "    - Then, we could use that policy to generate an episode. When that episode finishes, we could update the value function. \n",
    "    - Then, the value function could be used to improve the policy. \n",
    "    - That new policy could then be used to generate the next episode, and so on. \n",
    "    \n",
    "<img src=\"img/6_5_10_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- To do this well, we'll have to change our algorithm for <u>policy evaluation</u>. We'll begin by digging deeper into our current algorithm. \n",
    "- Remember that we begin by running many episodes. Then, we look at some state action pair. We calculate the return that followed in each case and then take the average. \n",
    "\n",
    "<img src=\"img/6_5_10_4.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- As a more general case, say we visited the state action pair some number and times. \n",
    "- We'll denote the <u>corresponding returns</u> by x1, x2 all the way up to xn (لازم تركز فى دى، دلالة المتغير إكس هنا غير المتغير إكس اللى مان فى الصورة السابقى).\n",
    "\n",
    "<img src=\"img/6_5_10_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- Then, our value function estimate is calculated as the average of those values. We'll denote it with a MU n where the n just helps us to remember that we visited the pair n times. \n",
    "\n",
    "<img src=\"img/6_5_10_6.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- And so Instead of calculating that average at the end of all the episodes, maybe what we could do instead <u>is iteratively update the estimate after every visit</u>. \n",
    "- So the first time that we visited, whatever the return was, that's our estimate. \n",
    "- Then the second time we visit, we can update the estimate to be the average of x1 and x2. \n",
    "- And for an arbitrary number of visits K, we just take the average of all the values x1 through xk. \n",
    "- And our final estimate, will just be exactly what we'd end up with if we'd waited to calculate anything until all the episodes finished. <br><br>\n",
    "\n",
    "- And so let's see if we can figure out how to accomplish this in a computationally efficient way. (note: for the following efficient way, I have already derived it myself in 4th year in college, and I will present my derivation here shortly) And towards that goal, we'll have to do a little bit of math. <br><br>\n",
    "\n",
    "- We'll begin by remembering how we calculate the estimate for the return. In particular, the Kth estimate is just the average of the returns that followed from the first K visits. (she highlighted the first line in the pic below) \n",
    "- Then, we'll notice we can rewrite the sum of the first K returns, as the sum of the first K minus one returns plus the Kth return. (she highlighted the second line in the pic below)\n",
    "- After that, we can re-express the sum of the first K minus one terms, as the K minus first mean times K minus one. (she highlighted the \"$(k-1) \\mu_{k-1}$\")\n",
    "\n",
    "<center> snippet pic </center>\n",
    "<img src=\"img/6_5_10_8.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "<img src=\"img/6_5_10_7.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "- It's then just a simple rearrangement of the terms to get the last line. <br><br>\n",
    "\n",
    "- Now this equation (pic below) expresses the Kth mean in terms of the K minus first mean and the Kth return. And we'll use this formula to design an algorithm that keeps a running mean of the returns. \n",
    "\n",
    "<img src=\"img/6_5_10_9.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- in this algorithm, (pic below) we begin by initializing the mean to zero. We'll also need to keep track of the number of returns that we've included in the average already. So at the beginning, we'll initialize that to zero. Then we enter a while loop at every point we increment K by one and then use the most recent return x of K to update the mean. \n",
    "\n",
    "<img src=\"img/6_5_10_10.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "- And that's it. Now while it's not entirely clear yet exactly how to use this in the context of Monte Carlo control, this algorithm will come in very useful soon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.11) Quiz: Incremental Mean\n",
    "\n",
    "- In the previous video, we learned about an algorithm that can keep a running estimate of the mean of a sequence of numbers $(x_1, x_2, \\ldots, x_n)$. The algorithm looked at each number in the sequence in order, and successively updated the mean $\\mu$.\n",
    "\n",
    "<img src=\"img/6_5_11_1.png\" alt=\"Drawing\" style=\"width: 250px;\"/>\n",
    "\n",
    "Use the pseudocode to complete the `running_mean` function below. Your function should accept a list of numbers `x` as input. It should return a list `mean_values`, where `mean_values[k]` is the mean of `x[:k+1]`.\n",
    "\n",
    "**Note**: Pay careful attention to indexing! Here, $x_k$ corresponds to `x[k-1]` (so $x_1$ = `x[0]`, $x_2$ = `x[1]`, etc).\n",
    "\n",
    "my solution (which is based on my derivation)\n",
    "\n",
    "<img src=\"img/6_5_11_2.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n",
    "\n",
    "udacity's solution\n",
    "<img src=\"img/6_5_11_3.png\" alt=\"Drawing\" style=\"width: 300px;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.12) MC Control: Policy Evaluation\n",
    "\n",
    "- So far, we've discussed how we might design the algorithm for <u>Monte Carlo control</u>.\n",
    "\n",
    "<img src=\"img/6_5_12_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- I mentioned in the last video that we still have some work to do with the evaluation step. \n",
    "- Towards this end, we discussed an algorithm for keeping a running estimate of the mean of a sequence of numbers. (pic below) \n",
    "- Remember that this algorithm focused on <u>one state action pair</u>, which was visited some number n times. \n",
    "\n",
    "<img src=\"img/6_5_12_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- The algorithm looks at each visit in order and successively updates the mean new. \n",
    "\n",
    "<img src=\"img/6_5_12_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "- So this algorithm currently, can only update the value function estimate corresponding to a single state action pair. \n",
    "- **Our next step is to change it a bit, so that it can maintain estimates of values for many state action pairs**. \n",
    "    - Then, we'll be able to plug in that algorithm as a new and improved <u>evaluation step</u>. \n",
    "    \n",
    "<img src=\"img/6_5_12_4.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "    \n",
    "- So here's what we'll do.  (pic below)\n",
    "    - The agent begins by sampling an episode from the environment. \n",
    "    - Then, for every time step we look at the corresponding state action pair. \n",
    "    - If it's a first visit, we calculate the corresponding return. \n",
    "    - Then, in accordance with the algorithm we examined for keeping the running mean, we then just update the corresponding estimate of the action value. \n",
    "    \n",
    "<img src=\"img/6_5_12_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/>    \n",
    "    \n",
    "- And so when we go to plug in this stuff into our algorithm for our Monte Carlo control, we also have to be careful to also initialize the number of times we've visited each pair \n",
    "\n",
    "<img src=\"img/6_5_12_6.png\" alt=\"Drawing\" style=\"width: 600px;\"/>\n",
    "\n",
    "- and we're getting there. Your blackjack playing agent will soon have a method for improving its strategy after every game."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.13) MC Control: Policy Improvement\n",
    "- Q before the video \\\n",
    "    - At $3:22$, for the Epsilon-Greedy Policy , I need to understand how these two cases of probabilities add up (i need to use numercial example)\n",
    "---    \n",
    "- Now that we have a nice method for <u>policy evaluation</u>, we'll turn our attention to policy improvement. \n",
    "\n",
    "<img src=\"img/6_5_13_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- So given an action-value function, how may we use it to propose a policy. Is it possible to just copy the algorithm from the dynamic programming case? Maybe we could take the action-value function and construct the corresponding greedy policy. \n",
    "\n",
    "<img src=\"img/6_5_13_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- In other words, for each state we just pick the action with the highest value. And what do you think, is that approach valid here? The answer is, kind of. If the plan is to combine it with the policy evaluation algorithm that you just learned about, we'll need to make some slight amendments. To see this, let's look at an example. <br><br>\n",
    "\n",
    "- Say you're an agent and there are two doors in front of you. You need to decide which one has more value. Let's see how you might determine this using the ideas behind the Monte Carlo method. At the beginning, you have no reason to favor any door over the other. So let's say you initialize your estimate for the value of each door to zero. \n",
    "\n",
    "<img src=\"img/6_5_13_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- And in order to figure out which door to open, let's say you flip a coin, it comes up tails, and so you open door B. And when you do that you receive a reward of zero. And let's say, for simplicity, that an episode finishes after a single door is opened. And so in other words, after opening door B, you received a return of zero. So that doesn't change the estimate of the value function \n",
    "\n",
    "<img src=\"img/6_5_13_4.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- and so it makes sense to just pick a door randomly again. And so you flip a coin, It comes up heads this time, and so you open door A. When you do this, you get a reward of one. This of course updates the estimate for the value of door A to one. \n",
    "\n",
    "<img src=\"img/6_5_13_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- So now if we act greedily with respect to the value function, then we open door A again. Say this time, we get a reward of three. This updates the value of door A to two. \n",
    "\n",
    "<img src=\"img/6_5_13_6.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- And so at the next point in time, the greedy policy says to pick door A again. And say every time we do that we get some positive reward and it's always either one or three. \n",
    "\n",
    "<img src=\"img/6_5_13_7.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- So for all time, we're opening the same door. There's a big problem with this, because we never really got a chance to truly explore what's behind the second door. For instance, consider the case that the mechanism behind door A is what you'd expect. It yields a reward of one or three, where both are equally likely but the mechanism behind door B gives a reward of 0 or 100.\n",
    "\n",
    "<img src=\"img/6_5_13_8.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- Then that's information that you would have liked to discover but following the greedy policy has prevented you. So the point is, that when we got to a situation early in our investigation where door A seemed more favorable than door B, we really needed to spend more time making sure of that, because our early perceptions were incorrect. So instead of constructing that <u>greedy policy</u>, a better policy would be a <u>stochastic</u> one that picked door A with 95% probability and door B with 5% probability let's say. \n",
    "- Then that's still pretty close to the greedy policy. So we're still acting pretty optimally. But there's the added value that if we continue to select door B with some small probability, then at some point we're going to see that return of 100. \n",
    "\n",
    "<img src=\"img/6_5_13_9.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- **This example motivates how we'll define the <u>Monte Carlo version</u> of <u>policy improvement</u>**. <u>Instead of always constructing a greedy policy, what we'll do instead is construct a stochastic policy that's most likely to pick the greedy action, but with some small but non-zero probability picks one of the non-greedy actions instead</u>. \n",
    "- In this case, you will set some small positive number epsilon where the larger it is, the more likely you are to pick one of the non-greedy actions, and we call this an epsilon-greedy policy. \n",
    "\n",
    "<img src=\"img/6_5_13_10.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- (**note**: i will demonstrate the epsilon greedy math in the next concept)\n",
    "- Epsilon must be a number between zero and one. Then with probability one minus epsilon, the agent selects the greedy action, and with probability epsilon, it selects any action randomly. Then as long as epsilon is set to a small number, we have a method for constructing a policy that's really close to the greedy policy with the added benefit that it doesn't prevent the agent from continuing to explore the range of possibilities. \n",
    "\n",
    "<img src=\"img/6_5_13_11.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- So in summary, our algorithm for a Monte Carlo control will look like this, where I plugged in the fact that the improvements step will obtain the Epsilon-greedy policy. (note in the pic below, that the policy improvment now is **epsion-greedy policy** not just **greedy policy** as before in the dynamic programming setting)\n",
    "\n",
    "<img src=\"img/6_5_13_12.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.14) Quiz: Epsilon-Greedy Policies\n",
    "\n",
    "In the previous concept, you learned about $\\epsilon$-greedy policies.\n",
    "\n",
    "- You can think of the agent who follows an $\\epsilon$-greedy policy as always having a (potentially unfair) coin at its disposal, with probability $\\epsilon$ of landing heads. After observing a state, the agent flips the coin.\n",
    "    - If the coin lands tails (so, with probability $1-\\epsilon$), the agent selects the greedy action.\n",
    "    - If the coin lands heads (so, with probability $\\epsilon$), the agent selects an action uniformly at random from the set of available (non-greedy **AND** greedy) actions. (**note**: it is important to notice that if the coin lands heads with orobability of $\\epsilon$, we will not choose from the non-greedy actions only, but also from the greedy actions. (i stress on this b/c i thought that the name \"epsilon-greedy\" shows that it is \"non greedy - greedy\" as if epsilon is the same as non greedy, but that's not right, and we will see this in my demonstration of the math shortly) )\n",
    "    \n",
    "In order to construct a policy $\\pi$ that is $\\epsilon$-greedy with respect to the current action-value function estimate $Q$, we need only set\n",
    "\n",
    "<img src=\"img/6_5_14_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "for each $s\\in\\mathcal{S}$ and $a\\in\\mathcal{A}(s)$. Note that $\\epsilon$ must always be a value between 0 and 1, inclusive (that is, $\\epsilon \\in [0,1]$).\n",
    "\n",
    "#### My demonstration of the math with an example \n",
    "- say that state $s_5$ has 4 available actions: $a_1$, $a_2$, $a_3$, and $a_4$.\n",
    "- say that the action value funtion for state s_5 is like the following:\n",
    "    - $Q(s_5, a_1) = 7$\n",
    "    - $Q(s_5, a_2) = 15$\n",
    "    - $Q(s_5, a_3) = 10$\n",
    "    - $Q(s_5, a_4) = 9$\n",
    "- obviously, action $a_2$ has the largest action value function with state $s_5$, and the other three actions are lower (their relative values will not matter in this algorithm, so they are just \"lower $Q$\" as if they were equal and lower than the max value) <br><br>\n",
    "\n",
    "- say that we set epsilon to 5%, so the probabilities of choosing each action will be as as follows\n",
    "    - $\\pi(a_1, s_5) = \\frac{5%}{4} = 1.25\\% $\n",
    "    - $\\pi(a_2, s_5) = 1 - 5\\% +\\frac{5%}{4} = 95\\% + 1.25\\% = 0.9625 $ (note that it is **not** $1-\\epsilon = 95\\%$) \n",
    "    - $\\pi(a_3, s_5) = \\frac{5%}{4} = 1.25\\% $\n",
    "    - $\\pi(a_4, s_5) = \\frac{5%}{4} = 1.25\\% $\n",
    "    \n",
    "#### now lets continue with the quiz\n",
    "In this quiz, you will answer a few questions to test your intuition.\n",
    "\n",
    "- QUESTION 1 OF 4\n",
    "    - Which of the values for epsilon yields an epsilon-greedy policy that is guaranteed to **always** select the greedy action? Select all that apply.\n",
    "        - (the right answers are the following)\n",
    "            - epsilon = 0\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - epsilon = 0.3\n",
    "            - epsilon = 0.5\n",
    "            - epsilon = 1\n",
    "            - This is a trick question!  The *true answer* is that none of the values for epsilon satisfy this requirement. <br><br>\n",
    "            \n",
    "- QUESTION 2 OF 4\n",
    "    - Which of the values for epsilon yields an epsilon-greedy policy that is guaranteed to **always** select a non-greedy action? Select all that apply.\n",
    "        - (the right answers are the following)\n",
    "            - This is a trick question!  The *true answer* is that none of the values for epsilon satisfy this requirement.\n",
    "                - **my note** : because even if u set $\\epsilon=1$, u will end up with an equiprobable policy (for all actions), haha and this will be the next question.\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - epsilon = 0\n",
    "            - epsilon = 0.3\n",
    "            - epsilon = 0.5\n",
    "            - epsilon = 1 <br><br>\n",
    "            \n",
    "- QUESTION 3 OF 4       \n",
    "    - Which of the values for epsilon yields an epsilon-greedy policy that is equivalent to the equiprobable random policy (where, from each state, each action is equally likely to be selected)?\n",
    "        - (the right answers are the following)\n",
    "            - epsilon = 1\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - epsilon = 0\n",
    "            - epsilon = 0.3\n",
    "            - epsilon = 0.5\n",
    "            - This is a trick question!  The *true answer* is that none of the values for epsilon satisfy this requirement.<br><br>    \n",
    "            \n",
    "- QUESTION 4 OF 4            \n",
    "    - Which of the values for epsilon yields an epsilon-greedy policy where the agent has the *possibility* of selecting a greedy action, but *might* select a non-greedy action instead? In other words, how might you guarantee that the agent selects each of the available (greedy and non-greedy) actions with nonzero probability?\n",
    "        - (the right answers are the following)\n",
    "            - epsilon = 0.3\n",
    "            - epsilon = 0.5\n",
    "            - epsilon = 1\n",
    "                - **my note**: any number larger than 0 (of course it will not exceed 1 also), i.e the answer is $]0.1]$\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - epsilon = 0\n",
    "            - This is a trick question!  The *true answer* is that none of the values for epsilon satisfy this requirement.<br><br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.15) Exploration vs Exploitation\n",
    "#### Solving Environments in OpenAI Gym\n",
    "<img src=\"img/6_5_15_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "In many cases, we would like our reinforcement learning (RL) agents to learn to maximize reward as quickly as possible. This can be seen in many OpenAI Gym environments.\n",
    "\n",
    "For instance, the [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/) environment is considered solved once the agent attains an average reward of 0.78 over 100 consecutive trials.\n",
    "\n",
    "<img src=\"img/6_5_15_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "Algorithmic solutions to the [FrozenLake-v0](https://gym.openai.com/envs/FrozenLake-v0/) environment are ranked according to the number of episodes needed to find the solution.\n",
    "\n",
    "<img src=\"img/6_5_15_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "Solutions to [Taxi-v1](https://gym.openai.com/envs/Taxi-v1/), [Cartpole-v1](https://gym.openai.com/envs/CartPole-v1/), and [MountainCar-v0](https://gym.openai.com/envs/MountainCar-v0/) (along with many others) are also ranked according to the number of episodes before the solution is found. Towards this objective, it makes sense to design an algorithm that learns the optimal policy $\\pi_*$ as quickly as possible.\n",
    "\n",
    "#### Exploration-Exploitation Dilemma\n",
    "Recall that the environment's dynamics are initially unknown to the agent. Towards maximizing return, the agent must learn about the environment through interaction.\n",
    "\n",
    "At every time step, when the agent selects an action, it bases its decision on past experience with the environment. And, towards minimizing the number of episodes needed to solve environments in OpenAI Gym, our first instinct could be to devise a strategy where the agent always selects the action that it believes (*based on its past experience*) will maximize return. With this in mind, the agent could follow the policy that is greedy with respect to the action-value function estimate. We examined this approach in a previous video and saw that it can easily lead to convergence to a sub-optimal policy.\n",
    "\n",
    "To see why this is the case, note that in early episodes, the agent's knowledge is quite limited (and potentially flawed). So, it is highly likely that actions *estimated* to be non-greedy by the agent are in fact better than the *estimated* greedy action.\n",
    "\n",
    "With this in mind, a successful RL agent cannot act greedily at every time step (that is, it cannot always **exploit** its knowledge); instead, in order to discover the optimal policy, it has to continue to refine the estimated return for all state-action pairs (in other words, it has to continue to **explore** the range of possibilities by visiting every state-action pair). That said, the agent should always act *somewhat greedily*, towards its goal of maximizing return *as quickly as possible*. This motivated the idea of an $\\epsilon$-greedy policy.\n",
    "\n",
    "We refer to the need to balance these two competing requirements as the **Exploration-Exploitation Dilemma**. <u>One potential solution to this dilemma is implemented by gradually modifying the value of $\\epsilon$ when constructing $\\epsilon$-greedy policies.</u>\n",
    "\n",
    "#### Setting the Value of $\\epsilon$, in Theory\n",
    "It makes sense for the agent to begin its interaction with the environment by favoring **exploration** over **exploitation**. After all, when the agent knows relatively little about the environment's dynamics, it should distrust its limited knowledge and **explore**, or try out various strategies for maximizing return. With this in mind, the best starting policy is the equiprobable random policy, as it is equally likely to explore all possible actions from each state. You discovered in the previous quiz that setting \\epsilon = 1ϵ=1 yields an \\epsilonϵ-greedy policy that is equivalent to the equiprobable random policy.\n",
    "\n",
    "At later time steps, it makes sense to favor **exploitation** over **exploration**, where the policy gradually becomes more greedy with respect to the action-value function estimate. After all, the more the agent interacts with the environment, the more it can trust its estimated action-value function. You discovered in the previous quiz that setting \\epsilon = 0ϵ=0 yields the greedy policy (or, the policy that most favors exploitation over exploration).\n",
    "\n",
    "<u>Thankfully, this strategy (of initially favoring exploration over exploitation, and then gradually preferring exploitation over exploration) can be demonstrated to be optimal.</u>\n",
    "\n",
    "#### Greedy in the Limit with Infinite Exploration (GLIE)\n",
    "- In order to guarantee that MC control converges to the optimal policy $\\pi_*$, we need to ensure that two conditions are met. We refer to these conditions as **Greedy in the Limit with Infinite Exploration (GLIE)**. In particular, if:\n",
    "    - every state-action pair $s, a$ (for all $s\\in\\mathcal{S}$ and $a\\in\\mathcal{A}$) is visited infinitely many times, and\n",
    "    - the policy converges to a policy that is greedy with respect to the action-value function estimate $Q$,\n",
    "- then MC control is guaranteed to converge to the optimal policy (in the limit as the algorithm is run for infinitely many episodes). These conditions ensure that: \n",
    "    - the agent continues to explore for all time steps, and\n",
    "    - the agent gradually exploits more (and explores less).\n",
    "- One way to satisfy these conditions is to modify the value of $\\epsilon$ when specifying an $\\epsilon$-greedy policy. In particular, let $\\epsilon_i$ correspond to the $i$-th time step. Then, both of these conditions are met if:\n",
    "    - $\\epsilon_i > 0$ for all time steps $i$, and\n",
    "    - $\\epsilon_i$ decays to zero in the limit as the time step $i$ approaches infinity (that is, $\\lim_{i\\to\\infty} \\epsilon_i = 0$).\n",
    "    \n",
    "For example, to ensure convergence to the optimal policy, we could set $\\epsilon_i = \\frac{1}{i}$. (You are encouraged to verify that $\\epsilon_i > 0$ for all $i$, and $\\lim_{i\\to\\infty} \\epsilon_i = 0$.)\n",
    "\n",
    "#### Setting the Value of $\\epsilon$, in Practice\n",
    "As you read in the above section, in order to guarantee convergence, we must let $\\epsilon_i$ decay in accordance with the GLIE conditions. But sometimes \"guaranteed convergence\" isn't good enough in practice, since this really doesn't tell you how long you have to wait! It is possible that you could need trillions of episodes to recover the optimal policy, for instance, and the \"guaranteed convergence\" would still be accurate!\n",
    "\n",
    "> - *Even though convergence is **not** guaranteed by the mathematics, you can often get better results by either:*\n",
    "    - *using fixed $\\epsilon$, or*\n",
    "    - *letting $\\epsilon_i$ decay to a small positive number, like 0.1.*\n",
    "    \n",
    "This is because one has to be very careful with setting the decay rate for $\\epsilon$; letting it get too small too fast can be disastrous. If you get late in training and $\\epsilon$ is really small, you pretty much want the agent to have already converged to the optimal policy, as it will take way too long otherwise for it to test out new actions!\n",
    "\n",
    "As a famous example in practice, you can read more about how the value of $\\epsilon$ was set in the famous DQN algorithm by reading the Methods section of the [research paper](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf):\n",
    "\n",
    "> *The behavior policy during training was epsilon-greedy with epsilon annealed linearly from 1.0 to 0.1 over the first million frames, and fixed at 0.1 thereafter.*\n",
    "\n",
    "When you implement your own algorithm for MC control later in this lesson, you are strongly encouraged to experiment with setting the value of \\epsilonϵ to build your intuition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.16) Implementation\n",
    "#### Implementation: MC Control: GLIE\n",
    "The pseudocode for (first-visit) GLIE MC control can be found below. (*Feel free to implement either the first-visit or every-visit MC method. In the game of Blackjack, both the first-visit and every-visit methods return identical results.*)\n",
    "\n",
    "<img src=\"img/6_5_16_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "Please use the next concept to complete **Part 3: MC Control: GLIE** of `Monte_Carlo.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.17) Mini Project: MC (Parts 3)\n",
    "#### Part 3: MC Control: GLIE\n",
    "\n",
    "In this section, you will write your own implementation of MC control: GLIE.  \n",
    "\n",
    "Your algorithm has three arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: This is a dictionary where `policy[s]` returns the action that the agent chooses after observing state `s`.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_from_Q(env, Q, epsilon, nA):\n",
    "    \"\"\" generates an episode from following the epsilon-greedy policy \"\"\"\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    while True:\n",
    "        action = np.random.choice(np.arange(nA), p=get_probs(Q[state], epsilon, nA)) \\\n",
    "                                    if state in Q else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        episode.append((state, action, reward))\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    return episode\n",
    "\n",
    "def get_probs(Q_s, epsilon, nA):\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    policy_s = np.ones(nA) * epsilon / nA\n",
    "    best_a = np.argmax(Q_s)\n",
    "    policy_s[best_a] = 1 - epsilon + (epsilon / nA)\n",
    "    return policy_s\n",
    "\n",
    "def update_Q_GLIE(env, episode, Q, N, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "        old_Q = Q[state][actions[i]] \n",
    "        old_N = N[state][actions[i]]\n",
    "        Q[state][actions[i]] = old_Q + (sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)/(old_N+1)\n",
    "        N[state][actions[i]] += 1\n",
    "    return Q, N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_GLIE(env, num_episodes, gamma=1.0):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionaries of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    N = defaultdict(lambda: np.zeros(nA))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        # set the value of epsilon\n",
    "        epsilon = 1.0/((i_episode/8000)+1)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q, N = update_Q_GLIE(env, episode, Q, N, gamma)\n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to obtain the estimated optimal policy and action-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "policy_glie, Q_glie = mc_control_GLIE(env, 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output : \n",
    "<img src=\"img/6_5_17_1.png\" alt=\"Drawing\" style=\"width: 173px;\"/> <br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the corresponding state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the state-value function\n",
    "V_glie = dict((k,np.max(v)) for k, v in Q_glie.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V_glie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output : \n",
    "<img src=\"img/6_5_17_2.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>    \n",
    "<img src=\"img/6_5_17_3.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the policy that is estimated to be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import plot_policy\n",
    "\n",
    "# plot the policy\n",
    "plot_policy(policy_glie)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output : \n",
    "<img src=\"img/6_5_17_4.png\" alt=\"Drawing\" style=\"width: 850px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **true** optimal policy $\\pi_*$ can be found on page 82 of the [textbook](http://go.udacity.com/rl-textbook) (and appears below).  Compare your final estimate to the optimal policy - how close are you able to get?  If you are not happy with the performance of your algorithm, take the time to tweak the decay rate of $\\epsilon$ and/or run the algorithm for more episodes to attain better results.\n",
    "\n",
    "<img src=\"img/6_5_17_5.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.18) MC Control: Constant-alpha, Part 1\n",
    "note before the video: for the equation that is discussed in the video, i did not remember when it was first introduced and demonstrated !! i want to go back and find out and see its demonstration!\n",
    "\n",
    "---\n",
    "\n",
    "- Currently your update step for Policy Evaluation looks a bit like this (pic below). You generate an episode, then for each state-action pair that was visited, you calculate the corresponding return that follows. Then, you use that return to get an updated estimate. \n",
    "\n",
    "<img src=\"img/6_5_18_1.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>\n",
    "\n",
    "- We're going to look at this update step a bit closer with the aim of improving it. You can think of it as first calculating the difference between the most recently sampled return, and the corresponding value of the state-action pair. \n",
    "\n",
    "<img src=\"img/6_5_18_2.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>\n",
    "\n",
    "- We denote that by Delta T, and you can think of it as an error turn. After all, it's the difference between what we expect the return to be, and what the return actually was. \n",
    "\n",
    "<img src=\"img/6_5_18_3.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>\n",
    "\n",
    "- In the case that this error is positive (pic below), well that means that the return that we received is more than what the value function expected. In this case, the action value is too low, so we use this update step to increase the estimate. On the other hand, if the error is negative, then that means that the return is higher than what the actual value function expected. So it makes sense to take into account this new evidence, and decrease the estimate and the actual value function. \n",
    "\n",
    "<img src=\"img/6_5_18_4.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>\n",
    "\n",
    "- And exactly how much do we increase or decrease the function? Well currently, the algorithm decreases it by an amount inversely proportional to the number of times that we've visited the state-action pair already. \n",
    "\n",
    "<img src=\"img/6_5_18_5.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>\n",
    "\n",
    "- So the first few times we visit the pair, the change is likely to be quite large. But at future time points, where the denominator of this fraction gets quite big, the changes get smaller and smaller. To understand why this would be the case, you have to remember that this equation (pic above) just calculates the average of all the sampled returns. So if you already have the average of 999 returns, then when you take into account the 1,000th return, it's not going to change the average much. <br><br>\n",
    "\n",
    "- So with this in mind, we'll change the algorithm to instead use a constant step size which I've denoted by Alpha here. \n",
    "\n",
    "<img src=\"img/6_5_18_6.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>\n",
    "\n",
    "- This ensures that returns that come later are more emphasized than those that arrived earlier. In this way, the agent will mostly trust the most recent returns, and gradually forget about those that came in the past. This is quite important because remember that the policy is constantly changing, and with every step becoming more optimal. So in fact, later time steps are quite important to estimating the action values. \n",
    "\n",
    "<img src=\"img/6_5_18_7.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>\n",
    "\n",
    "- I strongly encourage you to make this amendment to your algorithm for Monte Carlo policy evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.19) MC Control: Constant-alpha, Part 2\n",
    "In an earlier quiz (**Quiz: Incremental Mean**), you completed an algorithm that maintains a running estimate of the mean of a sequence of numbers $(x_1, x_2, \\ldots, x_n)$. The `running_mean` function accepted a list of numbers `x` as input and returned a list `mean_values`, where `mean_values[k]` was the mean of `x[:k+1]`.\n",
    "\n",
    "<img src=\"img/6_5_19_1.png\" alt=\"Drawing\" style=\"width: 300px;\"/> <br>\n",
    "\n",
    "When we adapted this algorithm for Monte Carlo control in the following concept (**MC Control: Policy Evaluation**), the sequence $(x_1, x_2, \\ldots, x_n)$ corresponded to returns obtained after visiting the <u>*same*</u> state-action pair.\n",
    "\n",
    "That said, the sampled returns (for the <u>*same*</u> state-action pair) likely corresponds to many <u>*different*</u> policies. This is because the control algorithm proceeds as a sequence of alternating evaluation and improvement steps, where the policy is improved after every episode of interaction. In particular, we discussed that returns sampled at later time steps likely correspond to policies that are more optimal.\n",
    "\n",
    "With this in mind, it made sense to amend the policy evaluation step to instead use a constant step size, which we denoted by $\\alpha$ in the previous video (**MC Control: Constant-alpha, Part 1**). This ensures that the agent primarily considers the most recently sampled returns when estimating the action-values and gradually forgets about returns in the distant past.\n",
    "\n",
    "The analogous pseudocode (for taking a <u>*forgetful*</u> mean of a sequence $(x_1, x_2, \\ldots, x_n)$) can be found below.\n",
    "\n",
    "<img src=\"img/6_5_19_2.png\" alt=\"Drawing\" style=\"width: 3\n",
    "00px;\"/> <br>\n",
    "\n",
    "This change has been implemented in the `forgetful_mean` function below. The function accepts a list of numbers `x` and the step size `alpha` as input. It returns a list `mean_values`, where `mean_values[i]` is the (`i+1`)-st estimated state-action value.\n",
    "\n",
    "The `print_results` function analyzes the difference between the `running_mean` and `forgetful_mean` functions. It passes the same value for `x` to both functions and tests multiple values for `alpha` in the `forgetful_mean` function.\n",
    "\n",
    "Take the time to become familiar with the code below. Then, click on the **[ Test Run ]** button to execute the `print_results` function. Feel free to change the values for `x` and `alpha_values`, if you would like to run more tests to further develop your intuition.\n",
    "\n",
    "<img src=\"img/6_5_19_3.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>\n",
    "<img src=\"img/6_5_19_4.png\" alt=\"Drawing\" style=\"width: 700px;\"/> <br>\n",
    "\n",
    "output:\n",
    "The running_mean function returns: 5.5 <br>\n",
    "The forgetful_mean function returns:<br>\n",
    "1.0427 (alpha=0.01)<br>\n",
    "1.9787 (alpha=0.02)<br>\n",
    "2.8194 (alpha=0.03)<br>\n",
    "3.5745 (alpha=0.04)<br>\n",
    "4.2529 (alpha=0.05)<br>\n",
    "4.8624 (alpha=0.06)<br>\n",
    "5.4099 (alpha=0.07)<br>\n",
    "5.9018 (alpha=0.08)<br>\n",
    "6.3436 (alpha=0.09)<br>\n",
    "6.7403 (alpha=0.1)<br>\n",
    "7.0964 (alpha=0.11)<br>\n",
    "7.4159 (alpha=0.12)<br>\n",
    "7.7025 (alpha=0.13)<br>\n",
    "7.9593 (alpha=0.14)<br>\n",
    "8.1894 (alpha=0.15)<br>\n",
    "8.3953 (alpha=0.16)<br>\n",
    "8.5795 (alpha=0.17)<br>\n",
    "8.7441 (alpha=0.18)<br>\n",
    "8.891 (alpha=0.19)<br>\n",
    "9.0221 (alpha=0.2)<br>\n",
    "9.1389 (alpha=0.21)<br>\n",
    "9.2428 (alpha=0.22)<br>\n",
    "9.3352 (alpha=0.23)<br>\n",
    "9.4173 (alpha=0.24)<br>\n",
    "9.49 (alpha=0.25)<br>\n",
    "9.5544 (alpha=0.26)<br>\n",
    "9.6114 (alpha=0.27)<br>\n",
    "9.6616 (alpha=0.28)<br>\n",
    "9.706 (alpha=0.29)<br>\n",
    "9.745 (alpha=0.3)<br>\n",
    "\n",
    "\n",
    "#### Setting the Value of $\\alpha$\n",
    "Remember that the `forgetful_mean` function is closely related to the **Evaluation** step in constant-$\\alpha$ MC control. You can find the associated pseudocode below.\n",
    "\n",
    "<img src=\"img/6_5_19_5.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "Before moving on to the next concept, use the above coding environment to verify the following facts about about how to set the value of $\\alpha$ when implementing constant-$\\alpha$ MC control.\n",
    "\n",
    "- You should always set the value for $\\alpha$ to a number greater than zero and less than (or equal to) one.\n",
    "    - If $\\alpha=0$, then the action-value function estimate is never updated by the agent.\n",
    "    - If $\\alpha = 1$, then the final value estimate for each state-action pair is always equal to the last return that was experienced by the agent (after visiting the pair).\n",
    "- Smaller values for $\\alpha$ encourage the agent to consider a longer history of returns when calculating the action-value function estimate. Increasing the value of $\\alpha$ ensures that the agent focuses more on the most recently sampled returns.\n",
    "\n",
    "Note that it is also possible to verify the above facts by slightly rewriting the update step as follows:\n",
    "\n",
    "$Q(S_t,A_t) \\leftarrow (1-\\alpha)Q(S_t,A_t) + \\alpha G_t$\n",
    "\n",
    "where it is now more obvious that $\\alpha$ controls how much the agent trusts the most recent return $G_t$ over the estimate $Q(S_t,A_t)$ constructed by considering all past returns.\n",
    "\n",
    "**IMPORTANT NOTE**: It is important to mention that when implementing constant-$\\alpha$ MC control, you must be careful to not set the value of $\\alpha$ too close to 1. This is because very large values can keep the algorithm from converging to the optimal policy $\\pi_*$. However, you must also be careful to not set the value of $\\alpha$ too low, as this can result in an agent who learns too slowly. <u>The best value of $\\alpha$ for your implementation will greatly>depend on your environment and is best gauged through trial-and-error.</u> (my note: this is similar to $\\alpha$ at gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### demonstration of the previous code : \n",
    "-`x = np.hstack((np.ones(10), 10*np.ones(10)))`\n",
    "    - the hstack function generates\n",
    "    - so x is  []\n",
    "    \n",
    "- `running_mean(x)[-1]`\n",
    "- `forgetful_mean(x, alpha)[-1]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.20) Implementation\n",
    "#### Implementation: MC Control: Constant-alpha\n",
    "The pseudocode for (first-visit) constant-\\alphaα MC control can be found below. (*Feel free to implement either the first-visit or every-visit MC method. In the game of Blackjack, both the first-visit and every-visit methods return identical results.*)\n",
    "\n",
    "<img src=\"img/6_5_20_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "Please use the next concept to complete **Part 4: MC Control: Constant-alpha** of `Monte_Carlo.ipynb`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.21) Mini Project: MC (Parts 4)\n",
    "#### Part 4: MC Control: Constant-$\\alpha$\n",
    "\n",
    "In this section, you will write your own implementation of constant-$\\alpha$ MC control.  \n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "- `policy`: This is a dictionary where `policy[s]` returns the action that the agent chooses after observing state `s`.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q_alpha(env, episode, Q, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent episode \"\"\"\n",
    "    states, actions, rewards = zip(*episode)\n",
    "    # prepare for discounting\n",
    "    discounts = np.array([gamma**i for i in range(len(rewards)+1)])\n",
    "    for i, state in enumerate(states):\n",
    "        old_Q = Q[state][actions[i]] \n",
    "        Q[state][actions[i]] = old_Q + alpha*(sum(rewards[i:]*discounts[:-(1+i)]) - old_Q)\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my Q\\ I do not know why the previous function took `env` as an argument (it did not use it !!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_alpha(env, num_episodes, alpha, gamma=1.0):\n",
    "    nA = env.action_space.n\n",
    "    # initialize empty dictionary of arrays\n",
    "    Q = defaultdict(lambda: np.zeros(nA))\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        # set the value of epsilon\n",
    "        epsilon = 1.0/((i_episode/8000)+1)\n",
    "        # generate an episode by following epsilon-greedy policy\n",
    "        episode = generate_episode_from_Q(env, Q, epsilon, nA)\n",
    "        # update the action-value function estimate using the episode\n",
    "        Q = update_Q_alpha(env, episode, Q, alpha, gamma)\n",
    "    # determine the policy corresponding to the final action-value function estimate\n",
    "    policy = dict((k,np.argmax(v)) for k, v in Q.items())\n",
    "    return policy, Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the cell below to obtain the estimated optimal policy and action-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and action-value function\n",
    "policy_alpha, Q_alpha = mc_control_alpha(env, 500000, 0.008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output: \n",
    "<img src=\"img/6_5_21_1.png\" alt=\"Drawing\" style=\"width: 176px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we plot the corresponding state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the state-value function\n",
    "V_alpha = dict((k,np.max(v)) for k, v in Q_alpha.items())\n",
    "\n",
    "# plot the state-value function\n",
    "plot_blackjack_values(V_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "<img src=\"img/6_5_21_2.png\" alt=\"Drawing\" style=\"width: 836px;\"/> <br>\n",
    "<img src=\"img/6_5_21_3.png\" alt=\"Drawing\" style=\"width: 836px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we visualize the policy that is estimated to be optimal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the policy\n",
    "plot_policy(policy_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "<img src=\"img/6_5_21_4.png\" alt=\"Drawing\" style=\"width: 958px;\"/> <br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **true** optimal policy $\\pi_*$ can be found on page 82 of the [textbook](http://go.udacity.com/rl-textbook) (and appears below).  Compare your final estimate to the optimal policy - how close are you able to get?  If you are not happy with the performance of your algorithm, take the time to tweak the decay rate of $\\epsilon$, change the value of $\\alpha$, and/or run the algorithm for more episodes to attain better results.\n",
    "\n",
    "<img src=\"img/6_5_21_5.png\" alt=\"Drawing\" style=\"width: 980px;\"/> <br>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-5.22) Summary\n",
    "<img src=\"img/6_5_22_1.png\" alt=\"Drawing\" style=\"width: 691px;\"/> <br>\n",
    "\n",
    "#### MC Prediction: State Values\n",
    "\n",
    "- Algorithms that solve the **prediction problem** determine the value function $v_\\pi$ (or q_\\pi) corresponding to a policy $\\pi$.\n",
    "- Methods that evaluate a policy $\\pi$ from interaction with the environment fall under one of two categories:\n",
    "    - **On-policy** methods have the agent interact with the environment by following the same policy $\\pi$ that it seeks to evaluate (or improve).\n",
    "    - **Off-policy** methods have the agent interact with the environment by following a policy $b$ (where $b\\neq\\pi$) that is different from the policy that it seeks to evaluate (or improve). <br>\n",
    "    \n",
    "- Each occurrence of state $s\\in\\mathcal{S}$ in an episode is called a **visit to** $s$.\n",
    "- There are two types of Monte Carlo (MC) prediction methods (for estimating $v_\\pi$):\n",
    "    - **First-visit MC** estimates $v_\\pi(s)$ as the average of the returns following only first visits to $s$ (that is, it ignores returns that are associated to later visits).\n",
    "    - **Every-visit** MC estimates $v_\\pi(s)$ as the average of the returns following all visits to $s$.\n",
    "\n",
    "<img src=\"img/6_5_22_2.png\" alt=\"Drawing\" style=\"width: 615px;\"/> <br>\n",
    "\n",
    "#### MC Prediction: Action Values\n",
    "- Each occurrence of the state-action pair $s,a$ ($s\\in\\mathcal{S},a\\in\\mathcal{A}$) in an episode is called a **visit to** $s,a$.\n",
    "- There are two types of MC prediction methods (for estimating $q_\\pi$):\n",
    "    - **First-visit** MC estimates $q_\\pi(s,a)$ as the average of the returns following only first visits to $s,a$ (that is, it ignores returns that are associated to later visits).\n",
    "    - **Every-visit** MC estimates $q_\\pi(s,a)$ as the average of the returns following all visits to $s,a$.\n",
    "\n",
    "<img src=\"img/6_5_22_3.png\" alt=\"Drawing\" style=\"width: 615px;\"/> <br>\n",
    "\n",
    "#### Generalized Policy Iteration\n",
    "- Algorithms designed to solve the **control problem** determine the optimal policy $\\pi_*$ from interaction with the environment.\n",
    "- **Generalized policy iteration (GPI)** refers to the general method of using alternating rounds of policy evaluation and improvement in the search for an optimal policy, All of the reinforcement learning algorithms we examine in this course can be classified as GPI.\n",
    "\n",
    "#### MC Control: Incremental Mean\n",
    "(In this concept, we derived an algorithm that keeps a running average of a sequence of numbers.)\n",
    "\n",
    "#### MC Control: Policy Evaluation\n",
    "(In this concept, we amended the policy evaluation step to update the value function after every episode of interaction.)\n",
    "\n",
    "#### MC Control: Policy Improvement\n",
    "- A policy is **greedy** with respect to an action-value function estimate $Q$ if for every state $s\\in\\mathcal{S}$, it is guaranteed to select an action $a\\in\\mathcal{A}(s)$ such that $a = \\arg\\max_{a\\in\\mathcal{A}(s)}Q(s,a)$. (It is common to refer to the selected action as the **greedy action**.)\n",
    "- A policy is $\\epsilon$-**greedy** with respect to an action-value function estimate $Q$ if for every state $s\\in\\mathcal{S}$,\n",
    "    - with probability $1-\\epsilon$, the agent selects the greedy action, and\n",
    "    - with probability $\\epsilon$, the agent selects an action (uniformly) at random.\n",
    "\n",
    "#### Exploration vs. Exploitation\n",
    "- All reinforcement learning agents face the **Exploration-Exploitation Dilemma**, where they must find a way to balance the drive to behave optimally based on their current knowledge (**exploitation**) and the need to acquire knowledge to attain better judgment (**exploration**).\n",
    "- In order for MC control to converge to the optimal policy, the **Greedy in the Limit with Infinite Exploration (GLIE)** conditions must be met:\n",
    "    - every state-action pair $s, a$ (for all $s\\in\\mathcal{S}$ and $a\\in\\mathcal{A}(s)$) is visited infinitely many times, and\n",
    "    - the policy converges to a policy that is greedy with respect to the action-value function estimate $Q$.\n",
    "\n",
    "<img src=\"img/6_5_22_4.png\" alt=\"Drawing\" style=\"width: 615px;\"/> <br>\n",
    "\n",
    "#### MC Control: Constant-alpha\n",
    "- (In this concept, we derived the algorithm for **constant-$\\alpha$ MC control**, which uses a constant step-size parameter $\\alpha$.)\n",
    "- The step-size parameter $\\alpha$ must satisfy $0 < \\alpha \\leq 1$. Higher values of $\\alpha$ will result in faster learning, but values of $\\alpha$ that are too high can prevent MC control from converging to $\\pi_*$.\n",
    "\n",
    "<img src=\"img/6_5_22_5.png\" alt=\"Drawing\" style=\"width: 615px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 6 : Temporal-Difference Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn about how to apply temporal-difference methods such as <u>Sarsa</u>, <u>Q-Learning</u>, and <u>Expected Sarsa</u> to solve both episodic and continuous tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (17 concepts)\n",
    "- 1- Introduction\n",
    "- 2- OpenAI Gym: CliffWalkingEnv <br><br>\n",
    "- 3- TD Prediciton: TD(0)\n",
    "    - 4- Implementation\n",
    "    - 5- Mini-Project TD (Parts 0 and 1)\n",
    "- 6- TD Prediction: Action Values\n",
    "- 7- TD Control: Sarsa(0)\n",
    "    - 8- Implementation\n",
    "    - 9- Mini-ProjectL TD (Part 2)\n",
    "- 10- TD Control: Sarsamax\n",
    "    - 11- Implementation\n",
    "    - 12- Mini-Project TD (Part 3)\n",
    "- 13- TD Control: Expected Sarsa\n",
    "    - 14- Implementaion\n",
    "    - 15- Mini-Project TD (Part 4)\n",
    "- 16- Analyzing Performance\n",
    "- 17- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.1) Introducution\n",
    "\n",
    "- In this lesson, you will learn about Temporal Difference or TD learning. <br><br>\n",
    "\n",
    "- In order to understand TD learning, it will help to discuss what exactly it would mean to solve this problem of learning from interaction. <br><br>\n",
    "\n",
    "- The solution will come many years into the future, when we've developed artificially intelligent agents that interact with the world much like the way humans do. In order to accomplish this, the agents would need to learn from the kind of online streaming data that we learn from everyday. Real life is far from an episodic task and it requires its agents, it requires us to constantly make decisions all day everyday. We get no break with our interaction with the world. <br><br>\n",
    "\n",
    "- <u>Remember that Monte Carlo learning needed those breaks, it needed the episode to end so that the return could be calculated, and then used as an estimate for the action values</u>. So, we'll need to come up with something else if we want to deal with more realistic learning in a real world setting. <br><br>\n",
    "\n",
    "- So, the main idea is this, if an agent is playing chess, instead of waiting until the end of an episode to see if it's won the game or not, it will at every move be able to estimate the probability that it's winning the game, or a self-driving car at every turn will be able to estimate if it's likely to crash, and if necessary, amend its strategy to avoid disaster. <br><br>\n",
    "\n",
    "- To emphasize, the Monte Carlo approach would have this car crash every time it wants to learn anything, and this is too expensive and also quite dangerous. <br><br>\n",
    "\n",
    "- TD learning will solve these problems. Instead of waiting to update values when the interaction ends, it will amend its predictions at every step, and you'll be able to use it to solve both continuous and episodic tasks. It's also widely used in reinforcement learning and lies at the heart of many state-of-the-art algorithms that you see in the news today. So, let's jump right in.\n",
    "---\n",
    "This lesson covers material in **Chapter 6** (especially 6.1-6.6) of the textbook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.2) OpenAI Gym: CliffWalkingEnv\n",
    "my note berfore the reading:\n",
    "- It is a good general note for me to know the difference between coordinates in \n",
    "    - x-y pairs (in math) and \n",
    "    - row-column pairs (in programming)\n",
    "- the thing is : when i say a point at (1,2) in math , it is a point in the array in (2,1) , because when i change the row , i acually move in a vertical direction (y-direction) , and when i change the column, i actually move in the horizontal direction (x-direction)\n",
    "---\n",
    "In this lesson, you will write your own Python implementations of all of the algorithms that we discuss. While your algorithms will be designed to work with any OpenAI Gym environment, you will test your code with the CliffWalking environment.\n",
    "\n",
    "<img src=\"img/6_6_2_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "In the CliffWalking environment, the agent navigates a 4x12 gridworld. Please read about the cliff-walking task in Example 6.6 of the textbook. When you have finished, you can learn more about the environment in its corresponding [GitHub file](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py), by reading the commented block in the CliffWalkingEnv class. For clarity, we have also pasted the description of the environment below:\n",
    "\n",
    "~~~ **start .py file**\n",
    "```python\n",
    " \"\"\"\n",
    "    This is a simple implementation of the Gridworld Cliff\n",
    "    reinforcement learning task.\n",
    "    Adapted from Example 6.6 from Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto:\n",
    "    http://people.inf.elte.hu/lorincz/Files/RL_2006/SuttonBook.pdf\n",
    "\n",
    "    With inspiration from:\n",
    "    https://github.com/dennybritz/reinforcement-learning/blob/master/lib/envs/cliff_walking.py\n",
    "    The board is a 4x12 matrix, with (using Numpy matrix indexing):\n",
    "        [3, 0] as the start at bottom-left\n",
    "        [3, 11] as the goal at bottom-right\n",
    "        [3, 1..10] as the cliff at bottom-center\n",
    "    Each time step incurs -1 reward, and stepping into the cliff incurs -100 reward \n",
    "    and a reset to the start. An episode terminates when the agent reaches the goal.\n",
    "    \"\"\"\n",
    "```\n",
    "~~~ **end .py file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.3) TD Prediciton: TD(0)\n",
    "- We'll continue with the trend of addressing the prediction problem and reinforcement learning first. So, given a policy, how might we estimate its value function? \n",
    "\n",
    "<img src=\"img/6_6_3_1.png\" alt=\"Drawing\" style=\"width: 594px;\"/> <br>\n",
    "\n",
    "- Let's build up how we did this in the previous lesson. In our Monte Carlo approach, the agent interacted with the environment in episodes. After an episode finished, we looked at every state-action pair in the sequence. If it was a first visit, we calculated the corresponding return and used it to update the action value. And we did this for many, many episodes. It's important to note that this algorithm is a solution for the prediction problem as long as we never change the policy between episodes. And as long as we run the algorithm for long enough, we're guaranteed to end with a nice estimate for the action-value function. \n",
    "\n",
    "<img src=\"img/6_6_3_2.png\" alt=\"Drawing\" style=\"width: 763px;\"/> <br>\n",
    "\n",
    "- But let's move our focus to this update step (first pic below). And there is an analogous equation if we instead want to keep track of the state values (second pic below). \n",
    "\n",
    "<img src=\"img/6_6_3_3.png\" alt=\"Drawing\" style=\"width: 798px;\"/> <br>\n",
    "<img src=\"img/6_6_3_4.png\" alt=\"Drawing\" style=\"width: 708px;\"/> <br>\n",
    "\n",
    "- **Now for the rest of this video, what we'll do is adapt this update step to come up with a new algorithm**. Remember that the main idea behind this line (1st line in pic below) is that the value of any state is defined as the expected return that's likely to follow that state if the agent follows the policy (2nd line in pic below). So averaging sampled returns yields a good estimate (my understanding : she means \"averaging\" as a mathematical tool is a good estimate for the RL concept of \"the expected return that's likely to follow that state if the agent follows the policy\").\n",
    "\n",
    "- **my note**:\n",
    "    - u may ask, where is the averaging in the equation ?!, well remember that it is the constant $\\alpha$ that does the forgetful mean . (before alpha we did the usual mean that used the \"incremental mean technique\" which divides the total quantity by the number if its occurrences)\n",
    "\n",
    "\n",
    "<img src=\"img/6_6_3_5.png\" alt=\"Drawing\" style=\"width: 734px;\"/> <br>\n",
    "\n",
    "- At this point, I'll remind you of the Bellman expectation equation for the state values. \n",
    "\n",
    "<img src=\"img/6_6_3_6.png\" alt=\"Drawing\" style=\"width: 718px;\"/> <br>\n",
    "\n",
    "- It (Bellman expectation equation for the state values) gives us a way to express the value of any state in terms of the values of the states that could potentially follow. And so what if we used the equation just like the one above it to motivate a slightly different update rule? (in the pic below, the video introduced the 4th line)\n",
    "\n",
    "<img src=\"img/6_6_3_7.png\" alt=\"Drawing\" style=\"width: 769px;\"/> <br>\n",
    "\n",
    "- So now instead of averaging sampled returns, we average the sampled value of the sum of the immediate reward plus the discounted value of the next state (the video introduced to underlines in the last two lines). \n",
    "\n",
    "<img src=\"img/6_6_3_8.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- And you'll notice that we now have an update step that understands the value of a state in terms of the values of its successor states. \n",
    "- And **why would we want to do that anyway**? Well, \n",
    "    - the first thing to notice is that **we've removed any mention of the return that comes at the end of the episode. And in fact, this new update step gives us a way to update the state values after every time step***. To see this, let's consider what would happen at an arbitrary time step t. As always, we'll use S_sub_t to denote the state. \n",
    "    \n",
    "<img src=\"img/6_6_3_9.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- _    \n",
    "    - Say the agent uses the policy to pick its action A_sub_t, \n",
    "    \n",
    "<img src=\"img/6_6_3_10.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>    \n",
    "    \n",
    "- _    \n",
    "    - then it receives a reward and next state from the environment. \n",
    "    \n",
    "<img src=\"img/6_6_3_11.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- _    \n",
    "    - So (look at the below pic while reading this paragraph) then what the prediction algorithm could do is use this very small time window of information to update value function. Specifically, we'll update the value of this state at time t. In order to do that, we begin by looking up the values of the states from time t and time t plus one. By also plugging in the reward, we can calculate this entire right hand side, and that's our new estimate for the value of the state at time t. \n",
    "    \n",
    "<img src=\"img/6_6_3_12.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br><br>\n",
    "    \n",
    "- <u>Now it's important to realize that we won't have to wait anymore until the end of the episode to update the values</u>. And this is the first algorithm you can use for the prediction problem when working with <u>continuous tasks</u>. <br><br>\n",
    "\n",
    "- <font color=DARKVIOLET>But before detailing the algorithm in full</font>, let's talk a bit more about what this update step accomplishes. (note, i highlighted the start of the demonstration of the \"full algorithm demonstration\" below with violet color)\n",
    "\n",
    "- So at an arbitrary time step t, <u>before the agent takes an action</u>, it's best estimate for the value of the current state is just what's contained in the value function. \n",
    "\n",
    "<img src=\"img/6_6_3_13.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- But then once it takes an action and receives the reward and next state (video: introduced $A_t$, $R_{t+1}$, $S_{t+1}$), well that's new information. And we can use it to express an alternative estimate for the value of the same state, but in terms of the value of the state that followed (pink underline is introduced). And we refer to this new estimate as the **TD target**. \n",
    "\n",
    "<img src=\"img/6_6_3_14.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- So then what this entire update equation does is find some middle ground between the two estimates. \n",
    "\n",
    "<img src=\"img/6_6_3_15.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- You will set the value of alpha according to which estimate you trust more. To see this more clearly, we'll rewrite the updated equation. \n",
    "\n",
    "<img src=\"img/6_6_3_16.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- Note that alpha must be set to a number between zero and one. When alpha is set to one(pic below), the new estimate is just the TD target, where we completely ignore and replace the previous estimate. \n",
    "\n",
    "<img src=\"img/6_6_3_17.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- And if we were to set alpha to zero, we would completely ignore the target and keep the old estimate unchanged. \n",
    "\n",
    "<img src=\"img/6_6_3_18.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- This is not something that we'd ever want to do because then our agent would never learn. \n",
    "- But it will prove useful to set alpha to a small number that's close to zero. \n",
    "\n",
    "<img src=\"img/6_6_3_19.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- And in general, the smaller alpha is, the less we trust the target when performing an update, and the more we rely on our existing estimate of the state value. You'll soon get a chance to experiment with setting the value of alpha yourself. \n",
    "\n",
    "- <font color=DARKVIOLET>We'll now put this update step back into the context of the full algorithm</font>, which we'll call One-Step temporal difference or TD for short. Of course the \"One-Step\" just refers to the fact that we update the value function after any individual step. You'll also see it referred to as TD zero. \n",
    "\n",
    "<img src=\"img/6_6_3_20.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>\n",
    "\n",
    "- <u>The algorithm is **designed to** determine the state value function corresponding to a policy</u>, which we denote by $\\pi$. (video, \\pi is highligted in the 5th line)<br><br>\n",
    "\n",
    "- We begin (pic above) by initializing the value of each state to zero. \n",
    "    - (line underlined in video :  $V(s) \\leftarrow 0$ for all $ s \\in S^+$ )\n",
    "- Then, at every time step, the agent interacts with the environment, choosing actions that are dictated by the policy. \n",
    "    - (line underlined in video :  Choose action $A_t$ using policy $\\pi$ )\n",
    "    \n",
    "- And immediately after receiving the reward and next state from the environment, ... (continue after the next line)\n",
    "    - (line underlined in video :  observe $R_{t+1}, S_{t+1}$ ) <br>\n",
    "- it updates the value function for the previous state. \n",
    "    - (line underlined in video :  the LHS of the update equation : $V(S_t)$ ) <br> <br>\n",
    "    \n",
    "    \n",
    "- So this is the version for <u>continuous tasks</u>. And as long as the agent interacts with the environment for long enough, the algorithm should return a nice estimate for the value function(the last line is introduced : \"Return V ...\"). <br><br> \n",
    "\n",
    "\n",
    "- Okay. And what about <u>episodic tasks</u>? Well in that case (pic below ), we need only check at every time step if the most recent state is a terminal state. And if so, we run the update step one last time to update the preceding state. Then, we start a new episode, but as you can see the idea is basically the same. <br>\n",
    "\n",
    "(note that i drew brackets around the parts that are identical with the previous picture of the continuous task)\n",
    "<img src=\"img/6_6_3_21.png\" alt=\"Drawing\" style=\"width: 759px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.4) Implementation\n",
    "\n",
    "- The pseudocode for TD(0) (or one-step TD) can be found below.\n",
    "\n",
    "<img src=\"img/6_6_4_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- TD(0) is **guaranteed to converge** to the true state-value function, as long as the step-size parameter $\\alpha$ is sufficiently small. If you recall, this was also the case for constant-$\\alpha$ MC prediction. However, TD(0) has some nice advantages:\n",
    "    - Whereas MC prediction must wait until the end of an episode to update the value function estimate, TD prediction methods update the value function after every time step. Similarly, TD prediction methods work for continuous and episodic tasks, while MC prediction can only be applied to episodic tasks.\n",
    "    - In practice, TD prediction converges faster than MC prediction. (*That said, no one has yet been able to prove this, and it remains an open problem.*) You are encouraged to take the time to check this for yourself in your implementations! For an example of how to run this kind of analysis, check out Example 6.2 in the textbook.\n",
    "    \n",
    "Please use the next concept to complete **Part 0: Explore CliffWalkingEnv** and **Part 1: TD Prediction: State Values** of `Temporal_Difference.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.5) Mini-Project TD (Parts 0 and 1)\n",
    "- Q before the project\\ how do we prevent the agent from taking an action that goes out of bound of the array?\n",
    "    - i think it is the policy (the policy determines what action the agent can/should take in a particualr state)\n",
    "---\n",
    "In this notebook, you will write your own implementations of many Temporal-Difference (TD) methods.\n",
    "\n",
    "While we have provided some starter code, you are welcome to erase these hints and write your code from scratch.\n",
    "\n",
    "#### Part 0: Explore CliffWalkingEnv\n",
    "\n",
    "Use the code cell below to create an instance of the [CliffWalking](https://github.com/openai/gym/blob/master/gym/envs/toy_text/cliffwalking.py) environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CliffWalking-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agent moves through a $4\\times 12$ gridworld, with states numbered as follows:\n",
    "```\n",
    "[[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11],\n",
    " [12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23],\n",
    " [24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35],\n",
    " [36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47]]\n",
    "```\n",
    "At the start of any episode, state `36` is the initial state.  State `47` is the only terminal state, and the cliff corresponds to states `37` through `46`.\n",
    "\n",
    "The agent has 4 potential actions:\n",
    "```\n",
    "UP = 0\n",
    "RIGHT = 1\n",
    "DOWN = 2\n",
    "LEFT = 3\n",
    "```\n",
    "\n",
    "Thus, $\\mathcal{S}^+=\\{0, 1, \\ldots, 47\\}$, and $\\mathcal{A} =\\{0, 1, 2, 3\\}$.  Verify this by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(env.action_space)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "<img src=\"img/6_6_5_1.png\" alt=\"Drawing\" style=\"width: 101px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this mini-project, we will build towards finding the optimal policy for the CliffWalking environment.  The optimal state-value function is visualized below.  Please take the time now to make sure that you understand _why_ this is the optimal state-value function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plot_utils import plot_values\n",
    "\n",
    "# define the optimal state-value function\n",
    "V_opt = np.zeros((4,12))\n",
    "V_opt[0] = -np.arange(3, 15)[::-1]\n",
    "V_opt[1] = -np.arange(3, 15)[::-1] + 1\n",
    "V_opt[2] = -np.arange(3, 15)[::-1] + 2\n",
    "V_opt[3][0] = -13\n",
    "\n",
    "plot_values(V_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "<img src=\"img/6_6_5_2.png\" alt=\"Drawing\" style=\"width: 832px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 1: TD Prediction: State Values\n",
    "\n",
    "In this section, you will write your own implementation of TD prediction (for estimating the state-value function).\n",
    "\n",
    "We will begin by investigating a policy where the agent moves:\n",
    "- `RIGHT` in states `0` through `10`, inclusive,  \n",
    "- `DOWN` in states `11`, `23`, and `35`, and\n",
    "- `UP` in states `12` through `22`, inclusive, states `24` through `34`, inclusive, and state `36`.\n",
    "\n",
    "The policy is specified and printed below.  Note that states where the agent does not choose an action have been marked with `-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = np.hstack([1*np.ones(11), 2, 0, np.zeros(10), 2, 0, np.zeros(10), 2, 0, -1*np.ones(11)])\n",
    "print(\"\\nPolicy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy.reshape(4,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_6_5_3.png\" alt=\"Drawing\" style=\"width: 455px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my visualization for the policy : <br>\n",
    "Policy (UP = $\\uparrow$, RIGHT = $\\rightarrow$, DOWN = $\\downarrow$, LEFT = $\\leftarrow$, N/A = $\\bullet$): <br>\n",
    "\n",
    "[ [$\\rightarrow$, $\\rightarrow$, $\\rightarrow$, $\\rightarrow$, $\\rightarrow$, $\\rightarrow$, $\\rightarrow$, $\\rightarrow$, $\\rightarrow$, $\\rightarrow$, $\\rightarrow$, $\\downarrow$] <br>\n",
    "$\\;\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$,  $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\downarrow$] <br> \n",
    "$\\;\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$,  $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\;$ $\\uparrow$, $\\downarrow$] <br>\n",
    "$\\;\\;$ $\\uparrow$, $\\;$ $\\bullet$, $\\;$ $\\bullet$, $\\;$ $\\bullet$, $\\;$ $\\;$ $\\bullet$, $\\;$ $\\bullet$, $\\;$ $\\bullet$, $\\;$ $\\bullet$, $\\;$ $\\bullet$, $\\;$ $\\bullet$, $\\;$ $\\bullet$, $\\;$ $\\bullet$] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my Q\\ i want to know the how the reward is defined here !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to visualize the state-value function that corresponds to this policy.  Make sure that you take the time to understand why this is the corresponding value function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_true = np.zeros((4,12))\n",
    "for i in range(3):\n",
    "    V_true[0:12][i] = -np.arange(3, 15)[::-1] - i\n",
    "V_true[1][11] = -2\n",
    "V_true[2][11] = -1\n",
    "V_true[3][0] = -17\n",
    "\n",
    "plot_values(V_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "<img src=\"img/6_6_5_4.png\" alt=\"Drawing\" style=\"width: 832px;\"/> <br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure is what you will try to approximate through the TD prediction algorithm.\n",
    "\n",
    "Your algorithm for TD prediction has five arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `policy`: This is a 1D numpy array with `policy.shape` equal to the number of states (`env.nS`).  `policy[s]` returns the action that the agent chooses when in state `s`.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `V`: This is a dictionary where `V[s]` is the estimated value of state `s`.\n",
    "\n",
    "Please complete the function in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, deque\n",
    "import sys\n",
    "\n",
    "def td_prediction(env, num_episodes, policy, alpha, gamma=1.0):\n",
    "    # initialize empty dictionaries of floats\n",
    "    V = defaultdict(float)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        ## TODO: complete the function\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # choose action A\n",
    "            action = policy[state]\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # perform updates\n",
    "            V[state] = V[state] + (alpha * (reward + (gamma * V[next_state]) - V[state]))            \n",
    "            # S <- S'\n",
    "            state = next_state\n",
    "            # end episode if reached terminal state\n",
    "            if done:\n",
    "                break \n",
    "    return V "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code cell below to test your implementation and visualize the estimated state-value function.  If the code cell returns **PASSED**, then you have implemented the function correctly!  Feel free to change the `num_episodes` and `alpha` parameters that are supplied to the function.  However, if you'd like to ensure the accuracy of the unit test, please do not change the value of `gamma` from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import check_test\n",
    "\n",
    "# evaluate the policy and reshape the state-value function\n",
    "V_pred = td_prediction(env, 5000, policy, .01)\n",
    "\n",
    "# please do not change the code below this line\n",
    "V_pred_plot = np.reshape([V_pred[key] if key in V_pred else 0 for key in np.arange(48)], (4,12)) \n",
    "check_test.run_check('td_prediction_check', V_pred_plot)\n",
    "plot_values(V_pred_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/6_6_5_5.png\" alt=\"Drawing\" style=\"width: 839px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How close is your estimated state-value function to the true state-value function corresponding to the policy?  \n",
    "\n",
    "You might notice that some of the state values are not estimated by the agent.  This is because under this policy, the agent will not visit all of the states.  In the TD prediction algorithm, the agent can only estimate the values corresponding to states that are visited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.6) TD Prediction: Action Values\n",
    "- Earlier in this lesson, we detailed an algorithm to calculate the <u>state value function</u> corresponding to a policy. Now, we'll adopt that algorithm to instead return an estimate of the <u>action-value function</u>. \n",
    "\n",
    "<img src=\"img/6_6_6_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- So let's recall exactly how one-step temporal difference works. \n",
    "    - The agent interacts with the environment. \n",
    "    - At time step zero, it receives some state S_sub_zero. \n",
    "    - Then, it uses the policy to pick an action. \n",
    "    - Immediately afterwards, the agent receives a reward and next state.\n",
    "    \n",
    "<img src=\"img/6_6_6_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- _    \n",
    "    - At this point, the agent uses its experience to update its estimate for the value of the state from time zero. \n",
    "    \n",
    "<img src=\"img/6_6_6_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>    \n",
    "    \n",
    "- _    \n",
    "    - At the next point in time, the agent chooses an action, again, by consulting the policy, then it receives a reward and next state. \n",
    "    \n",
    "<img src=\"img/6_6_6_4.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>      \n",
    "\n",
    "- _    \n",
    "    - Then, it uses that information to update the value of the state from time one. \n",
    "  \n",
    "<img src=\"img/6_6_6_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br> \n",
    "\n",
    "- _    \n",
    "    \n",
    "    - Then, the process continues where the agent always consults the same policy to pick an action, receives a reward and next state, and then updates the value function. \n",
    "    \n",
    "<img src=\"img/6_6_6_6.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>     \n",
    "    \n",
    "- So the question is, how might we adapt this process to instead return an estimate of the <u>action values</u>? (my note: i.e instead of the state values) \n",
    "- Well, instead of having an updated equation that relates the values of successive states (top two lines in the pic below), what we'll instead need to do is have an update equation that relates the values of successive state-action pairs (bottom two lines in the pic below). \n",
    "\n",
    "<img src=\"img/6_6_6_7.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>   \n",
    "\n",
    "- Then, instead of updating the values after each state is received (first pic below), the agent will instead update the values after each action is chosen (second pic below). \n",
    "\n",
    "<img src=\"img/6_6_6_8.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br> \n",
    "<img src=\"img/6_6_6_9.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br> \n",
    "\n",
    "- But that's the only difference, and if the agent interacts with the environment for long enough, it will have a pretty good estimate of the action-value function. \n",
    "\n",
    "<img src=\"img/6_6_6_10.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- In the upcoming concepts, you'll learn more about how to use this algorithm in the search for an optimal policy.\n",
    "---\n",
    "Similar to TD(0), this method for estimating the action values is guaranteed to converge to the true action-value function, as long as the step-size parameter $\\alpha$ is sufficiently small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.7) TD Control: Sarsa(0)\n",
    "- Q before the video \\ i still do not know what does \"predition\" and \"control\" in Monte Carlo and in TD !!\n",
    "    - answer: \n",
    "        - prediction problem : given a policy $\\pi$, determine $v_\\pi$ (or $q_\\pi$). (from interaction with the enviernment) \n",
    "        - control problem : getting the optimal policy from the action value.\n",
    "---\n",
    "- Now that we've addressed the Prediction Problem, we're ready to move on to control. So, how might an agent determine an optimal policy? \n",
    "\n",
    "<img src=\"img/6_6_7_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>\n",
    "\n",
    "- We will build off the algorithm that we use to estimate the action value function. In that case, after each action is selected, the agent updates its estimate. \n",
    "\n",
    "<img src=\"img/6_6_7_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- <u>And it's important to note that the agent uses the same policy at every time step to select the actions</u>, **but now**, to adapt this to produce a control algorithm, we'll gradually change the policy, so that it becomes more optimal at every time step.  <br><br>\n",
    "\n",
    "- One of the methods we'll use for this (my note: i need to remember to see what are the other algorithms and summarize them.) is pretty identical to what we did in the Monte-Carlo case, where  ... (continue after the pic)\n",
    "\n",
    "<img src=\"img/6_6_7_3.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- we select the action at every time step by using a policy that's Epsilon-Greedy, with respect to the current estimate of the action values. \n",
    "    - At the initial time step, we begin by setting Epsilon to one ($\\epsilon = 1$). \n",
    "    - Then, $A_0$ and $A_1$ are chosen according to the equal probable random policy. \n",
    "    - Then, at all future time steps after an action is chosen, we update the action-value function and construct the corresponding Epsilon-Greedy policy.  <br><br>\n",
    "    \n",
    "- And as long as we specify appropriate values for Epsilon, the algorithm is guaranteed to converge to the optimal policy.  <br><br>\n",
    "\n",
    "- The name of this algorithm is Sarsa(0), also known as Sarsa for short. The name comes from the fact that each action-value update uses a state-action-reward, next state, next action, tuple of interaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### additional cell to summarize the control algorithms in TD \n",
    "- Sarsa (or Sarsa(0) )\n",
    "- Sarsamax (or Q-Learning )\n",
    "- Expected Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.8) Implementation\n",
    "#### Implementation: Sarsa(0)\n",
    "- The pseudocode for Sarsa (or Sarsa(0)) can be found below.\n",
    "\n",
    "<img src=\"img/6_6_8_1.png\" alt=\"Drawing\" style=\"width: 629px;\"/> <br>\n",
    "\n",
    "Sarsa(0) is **guaranteed to converge** to the optimal action-value function, as long as the step-size parameter $\\alpha$ is sufficiently small, and the **Greedy in the Limit with Infinite Exploration (GLIE)** conditions are met. The GLIE conditions were introduced in the previous lesson, when we learned about MC control. Although there are many ways to satisfy the GLIE conditions, one method involves gradually decaying the value of $\\epsilon$ when constructing $\\epsilon$-greedy policies.\n",
    "\n",
    "- In particular, let $\\epsilon_i$ correspond to the $i$-th time step. Then, if we set $\\epsilon_i$ such that:\n",
    "    - $\\epsilon_i > 0$ for all time steps $i$, and\n",
    "    - $\\epsilon_i$ decays to zero in the limit as the time step $i$ approaches infinity (that is, $\\lim_{i\\to\\infty} \\epsilon_i = 0$), <br>\n",
    "    \n",
    "then the algorithm is guaranteed to yield a good estimate for $q_*$, as long as we run the algorithm for long enough. A corresponding optimal policy $\\pi_*$ can then be quickly obtained by setting $\\pi_*(s) = \\arg\\max_{a\\in\\mathcal{A}(s)} q_*(s, a)$ for all $s\\in\\mathcal{S}$.\n",
    "\n",
    "Please use the next concept to complete **Part 2: TD Control: Sarsa** of `Temporal_Difference.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.9) Mini-ProjectL TD (Part 2)\n",
    "#### Part 2: TD Control: Sarsa\n",
    "\n",
    "In this section, you will write your own implementation of the Sarsa control algorithm.\n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "\n",
    "Please complete the function in the code cell below.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_Q(Qsa, Qsa_next, reward, alpha, gamma):\n",
    "    \"\"\" updates the action-value function estimate using the most recent time step \"\"\"\n",
    "    return Qsa + (alpha * (reward + (gamma * Qsa_next) - Qsa))\n",
    "\n",
    "def epsilon_greedy_probs(env, Q_s, i_episode, eps=None):\n",
    "    \"\"\" obtains the action probabilities corresponding to epsilon-greedy policy \"\"\"\n",
    "    epsilon = 1.0 / i_episode\n",
    "    if eps is not None:\n",
    "        epsilon = eps\n",
    "    policy_s = np.ones(env.nA) * epsilon / env.nA\n",
    "    policy_s[np.argmax(Q_s)] = 1 - epsilon + (epsilon / env.nA)\n",
    "    return policy_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    plot_every = 100\n",
    "    tmp_scores = deque(maxlen=plot_every)\n",
    "    scores = deque(maxlen=num_episodes)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()   \n",
    "        # initialize score\n",
    "        score = 0\n",
    "        # begin an episode, observe S\n",
    "        state = env.reset()   \n",
    "        # get epsilon-greedy action probabilities\n",
    "        policy_s = epsilon_greedy_probs(env, Q[state], i_episode)\n",
    "        # pick action A\n",
    "        action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "        # limit number of time steps per episode\n",
    "        for t_step in np.arange(300):\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # add reward to score\n",
    "            score += reward\n",
    "            if not done:\n",
    "                # get epsilon-greedy action probabilities\n",
    "                policy_s = epsilon_greedy_probs(env, Q[next_state], i_episode)\n",
    "                # pick next action A'\n",
    "                next_action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "                # update TD estimate of Q\n",
    "                Q[state][action] = update_Q(Q[state][action], Q[next_state][next_action], \n",
    "                                            reward, alpha, gamma)\n",
    "                # S <- S'\n",
    "                state = next_state\n",
    "                # A <- A'\n",
    "                action = next_action\n",
    "            if done:\n",
    "                # update TD estimate of Q\n",
    "                Q[state][action] = update_Q(Q[state][action], 0, reward, alpha, gamma)\n",
    "                # append score\n",
    "                tmp_scores.append(score)\n",
    "                break\n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(tmp_scores))\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(scores),endpoint=False),np.asarray(scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(scores))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next code cell to visualize the **_estimated_** optimal policy and the corresponding state-value function.  \n",
    "\n",
    "If the code cell returns **PASSED**, then you have implemented the function correctly!  Feel free to change the `num_episodes` and `alpha` parameters that are supplied to the function.  However, if you'd like to ensure the accuracy of the unit test, please do not change the value of `gamma` from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsa = sarsa(env, 5000, .01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsa = np.array([np.argmax(Q_sarsa[key]) if key in Q_sarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "check_test.run_check('td_control_check', policy_sarsa)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "V_sarsa = ([np.max(Q_sarsa[key]) if key in Q_sarsa else 0 for key in np.arange(48)])\n",
    "plot_values(V_sarsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "\n",
    "<img src=\"img/6_6_9_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- my Questions on the previous output :\n",
    "    - i made 3 red lines in the \"estimated optimal policy array\" , and the question is , how could the optimal policy have an action 0 (=up) in the upper border!! this will go out of range !!\n",
    "    - also there is an action 2 (=down) above an action 0(=up) which will lead to a dead lock !!! how can this happen in an optimal policy !\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.10) TD Control: Sarsamax\n",
    "\n",
    "- So far, you already have one algorithm for temporal difference control (my note: which is Sarsa). \n",
    "- Remember that in the Sarsa algorithm, \n",
    "    - we begin by initializing all action values to zero in constructing the corresponding Epsilon Greedy policy. \n",
    "    - Then, the agent begins interacting with the environment and receives the first state. \n",
    "    - Next, it uses the policy to choose it's action. \n",
    "    - Immediately afterward, it receives a reward and next state. \n",
    "    - Then, the agent again uses the same policy to pick the next action. \n",
    "    - After choosing that action, it updates the action value corresponding to the previous state action pair, and improves the policy to be Epsilon Greedy with respect to the most recent estimate of the action values. \n",
    "    \n",
    "<img src=\"img/6_6_10_1.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- **For the remainder of this video, we'll build off this algorithm to design another control algorithm that works slightly differently. This algorithm is called <u>Sarsamax</u>, but it's also known as <u>Q-Learning</u>**. <br><br>\n",
    "\n",
    "- We'll still begin with the same initial values for the action values and the policy. The agent receives the initial state, the first action is still chosen from the initial policy. But then, after receiving the reward and next state, we're going to do something else. <br>\n",
    "(the below pic is the similarity with of sarsa with sarsamax)\n",
    "\n",
    "<img src=\"img/6_6_10_2.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- so, in sarsamax, after receiving the reward and next state, we're going to do something else (other than sarsa). Namely, we'll update the policy before choosing the next action. And can you guess what action makes sense to put here? (look at the arrow at the pic below)\n",
    "\n",
    "<img src=\"img/6_6_10_3.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- Well, in the Sarsa case, our update step was one step later and plugged in the action that was selected using the Epsilon Greedy policy. And for every step of the algorithm, it was the case that all of the actions we used for updating the action values, exactly coincide with those that were experienced by the agent. **But in general, this does not have to be the case. In particular, consider using the action from the Greedy policy, instead of the Epsilon Greedy policy. This is in fact what Sarsamax or Q-Learning does**. \n",
    "\n",
    "- And in this case, you can rewrite the equation to look like this (below pic) where we rely on the fact that the greedy action corresponding to a state is just the one that maximizes the action values for that state. \n",
    "\n",
    "<img src=\"img/6_6_10_4.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- And so what happens is (look at pic below) after we update the action value for time step zero using the greedy action, we then select A1 using the Epsilon greedy policy corresponding to the action values we just updated. And this continues when we received a reward and next state. Then, we do the same thing we did before where we update the value corresponding to S1 and A1 using the greedy action, then we select A2 using the corresponding Epsilon greedy policy. \n",
    "\n",
    "<img src=\"img/6_6_10_5.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- To understand precisely what this update stuff is doing, we'll compare it to the corresponding step in the Sarsa algorithm (pic below). And in Sarsa, the update step pushes the action values closer to evaluating whatever Epsilon greedy policy is currently being followed by the agent. **And it's possible to show that Sarsamax instead, directly attempts to approximate the optimal value function at every time step**. (الجملة الأخيرة برأيى هى الجواب على سؤال: ليه نستخدم الخوارزمية الجديدة دى عوضا عن السابقة) <br><br>\n",
    "\n",
    "- Soon, you'll have the chance to implement this yourself and directly examine the difference between these two algorithms.\n",
    "\n",
    "---\n",
    "Check out this (optional) [research paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.80.7501&rep=rep1&type=pdf) to read the proof that Sarsamax (or Q-learning) converges."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.11) Implementation\n",
    "#### Implementation: Sarsamax\n",
    "The pseudocode for Sarsamax (or Q-learning) can be found below.\n",
    "\n",
    "<img src=\"img/6_6_11_1.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "Sarsamax is **guaranteed to converge** under the same conditions that guarantee convergence of Sarsa.\n",
    "\n",
    "Please use the next concept to complete **Part 3: TD Control: Q-learning** of `Temporal_Difference.ipynb`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.12) Mini-Project TD (Part 3)\n",
    "#### Part 3: TD Control: Q-learning (= Sarsamax)\n",
    "\n",
    "In this section, you will write your own implementation of the Q-learning control algorithm.\n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "\n",
    "Please complete the function in the code cell below.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    plot_every = 100\n",
    "    tmp_scores = deque(maxlen=plot_every)\n",
    "    scores = deque(maxlen=num_episodes)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # initialize score\n",
    "        score = 0\n",
    "        # begin an episode, observe S\n",
    "        state = env.reset()\n",
    "        while True:\n",
    "            # get epsilon-greedy action probabilities\n",
    "            policy_s = epsilon_greedy_probs(env, Q[state], i_episode)\n",
    "            # pick next action A\n",
    "            action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # add reward to score\n",
    "            score += reward\n",
    "            # update Q\n",
    "            Q[state][action] = update_Q(Q[state][action], np.max(Q[next_state]), \\\n",
    "                                                  reward, alpha, gamma)        \n",
    "            # S <- S'\n",
    "            state = next_state\n",
    "            # until S is terminal\n",
    "            if done:\n",
    "                # append score\n",
    "                tmp_scores.append(score)\n",
    "                break\n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(tmp_scores))\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(scores),endpoint=False),np.asarray(scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(scores))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next code cell to visualize the **_estimated_** optimal policy and the corresponding state-value function. \n",
    "\n",
    "If the code cell returns **PASSED**, then you have implemented the function correctly!  Feel free to change the `num_episodes` and `alpha` parameters that are supplied to the function.  However, if you'd like to ensure the accuracy of the unit test, please do not change the value of `gamma` from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_sarsamax = q_learning(env, 5000, .01)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_sarsamax = np.array([np.argmax(Q_sarsamax[key]) if key in Q_sarsamax else -1 for key in np.arange(48)]).reshape((4,12))\n",
    "check_test.run_check('td_control_check', policy_sarsamax)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_sarsamax)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "plot_values([np.max(Q_sarsamax[key]) if key in Q_sarsamax else 0 for key in np.arange(48)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output:\n",
    "<img src=\"img/6_6_12_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my note : notice how the curve here (in the previous cell) has converged faster than the sarsa algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.13) TD Control: Expected Sarsa\n",
    "\n",
    "- So far, you've implemented Sarsa and Sarsamax and we'lll now discuss one more option. This new option is called expected Sarsa and it closely resembles Sarsamax, where the only difference is in the update step for the action value. \n",
    "\n",
    "<img src=\"img/6_6_13_1.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- Remember that Sarsamax or Q-learning took the maximum over all actions of all possible next state action pairs. In other words, it chooses what value to place here by plugging in the one action that maximizes the action value estimate corresponding to the next state. \n",
    "\n",
    "<img src=\"img/6_6_13_2.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- Expected Sarsa does something a bit different. It uses the expected value of the next state action pair, where the expectation takes into account the probability that the agent selects each possible action from the next state. \n",
    "\n",
    "<img src=\"img/6_6_13_3.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "لسة عاوز أعرف إيه ميزة الخوارزمية الجديدة دى ، يمكن ألاقى دا فى الورقة البحثية اللى مرفق اللنك بتاعها تحت\n",
    "<br>\n",
    "بالفعل الورقة البحثية بتتكلم عن دا\n",
    "<br><br>\n",
    "كمان محتاج أعمل رسومات للداتا ستركشرز و الللا بيحصل فيهم فى كل خوارزمية عشان أفهم الموضوع كويس\n",
    "\n",
    "- Over the next couple concepts, you'll write your own implementation of Expected Sarsa.\n",
    "\n",
    "--- \n",
    "Check out this [research paper](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.216.4144&rep=rep1&type=pdf) to learn more about Expected Sarsa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.14) Implementaion\n",
    "#### Implementation: Expected Sarsa\n",
    "The pseudocode for Expected Sarsa can be found below.\n",
    "\n",
    "<img src=\"img/6_6_14_1.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "Expected Sarsa is **guaranteed to converge** under the same conditions that guarantee convergence of Sarsa and Sarsamax.\n",
    "\n",
    "Remember that ***theoretically***, the as long as the step-size parameter $\\alpha$ is sufficiently small, and the **Greedy in the Limit with Infinite Exploration (GLIE)** conditions are met, the agent is guaranteed to eventually discover the optimal action-value function (and an associated optimal policy). However, ***in practice***, for all of the algorithms we have discussed, it is common to completely ignore these conditions and still discover an optimal policy. You can see an example of this in the solution notebook.\n",
    "\n",
    "Please use the next concept to complete **Part 4: TD Control: Expected Sarsa** of `Temporal_Difference.ipynb`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.15) Mini-Project TD (Part 4) \n",
    "#### Part 4: TD Control: Expected Sarsa\n",
    "\n",
    "In this section, you will write your own implementation of the Expected Sarsa control algorithm.\n",
    "\n",
    "Your algorithm has four arguments:\n",
    "- `env`: This is an instance of an OpenAI Gym environment.\n",
    "- `num_episodes`: This is the number of episodes that are generated through agent-environment interaction.\n",
    "- `alpha`: This is the step-size parameter for the update step.\n",
    "- `gamma`: This is the discount rate.  It must be a value between 0 and 1, inclusive (default value: `1`).\n",
    "\n",
    "The algorithm returns as output:\n",
    "- `Q`: This is a dictionary (of one-dimensional arrays) where `Q[s][a]` is the estimated action value corresponding to state `s` and action `a`.\n",
    "\n",
    "Please complete the function in the code cell below.\n",
    "\n",
    "(_Feel free to define additional functions to help you to organize your code._)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_sarsa(env, num_episodes, alpha, gamma=1.0):\n",
    "    # initialize action-value function (empty dictionary of arrays)\n",
    "    Q = defaultdict(lambda: np.zeros(env.nA))\n",
    "    # initialize performance monitor\n",
    "    plot_every = 100\n",
    "    tmp_scores = deque(maxlen=plot_every)\n",
    "    scores = deque(maxlen=num_episodes)\n",
    "    # loop over episodes\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        # monitor progress\n",
    "        if i_episode % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        # initialize score\n",
    "        score = 0\n",
    "        # begin an episode\n",
    "        state = env.reset()\n",
    "        # get epsilon-greedy action probabilities\n",
    "        policy_s = epsilon_greedy_probs(env, Q[state], i_episode, 0.005)\n",
    "        while True:\n",
    "            # pick next action\n",
    "            action = np.random.choice(np.arange(env.nA), p=policy_s)\n",
    "            # take action A, observe R, S'\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            # add reward to score\n",
    "            score += reward\n",
    "            # get epsilon-greedy action probabilities (for S')\n",
    "            policy_s = epsilon_greedy_probs(env, Q[next_state], i_episode, 0.005)\n",
    "            # update Q\n",
    "            Q[state][action] = update_Q(Q[state][action], np.dot(Q[next_state], policy_s), \\\n",
    "                                                  reward, alpha, gamma)        \n",
    "            # S <- S'\n",
    "            state = next_state\n",
    "            # until S is terminal\n",
    "            if done:\n",
    "                # append score\n",
    "                tmp_scores.append(score)\n",
    "                break\n",
    "        if (i_episode % plot_every == 0):\n",
    "            scores.append(np.mean(tmp_scores))\n",
    "    # plot performance\n",
    "    plt.plot(np.linspace(0,num_episodes,len(scores),endpoint=False),np.asarray(scores))\n",
    "    plt.xlabel('Episode Number')\n",
    "    plt.ylabel('Average Reward (Over Next %d Episodes)' % plot_every)\n",
    "    plt.show()\n",
    "    # print best 100-episode performance\n",
    "    print(('Best Average Reward over %d Episodes: ' % plot_every), np.max(scores))\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the next code cell to visualize the **_estimated_** optimal policy and the corresponding state-value function.  \n",
    "\n",
    "If the code cell returns **PASSED**, then you have implemented the function correctly!  Feel free to change the `num_episodes` and `alpha` parameters that are supplied to the function.  However, if you'd like to ensure the accuracy of the unit test, please do not change the value of `gamma` from the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the estimated optimal policy and corresponding action-value function\n",
    "Q_expsarsa = expected_sarsa(env, 10000, 1)\n",
    "\n",
    "# print the estimated optimal policy\n",
    "policy_expsarsa = np.array([np.argmax(Q_expsarsa[key]) if key in Q_expsarsa else -1 for key in np.arange(48)]).reshape(4,12)\n",
    "check_test.run_check('td_control_check', policy_expsarsa)\n",
    "print(\"\\nEstimated Optimal Policy (UP = 0, RIGHT = 1, DOWN = 2, LEFT = 3, N/A = -1):\")\n",
    "print(policy_expsarsa)\n",
    "\n",
    "# plot the estimated optimal state-value function\n",
    "plot_values([np.max(Q_expsarsa[key]) if key in Q_expsarsa else 0 for key in np.arange(48)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "output: \n",
    "<img src=\"img/6_6_15_1.png\" alt=\"Drawing\" style=\"width: 800px;\"/> <br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.16) Analyzing Performance\n",
    "\n",
    "All of the TD control algorithms we have examined (Sarsa, Sarsamax, Expected Sarsa) converge to the optimal action-value function $q_*$ (and so yield the optimal policy $\\pi_*$) if (1) the value of $\\epsilon$ decays in accordance with the GLIE conditions, and (2) the step-size parameter $\\alpha$ is sufficiently small.\n",
    "\n",
    "- The differences between these algorithms are summarized below:\n",
    "    - Sarsa and Expected Sarsa are both **on-policy** TD control algorithms. In this case, the same ($\\epsilon$-greedy) policy that is evaluated and improved is also used to select actions.\n",
    "    - Sarsamax is an **off-policy** method, where the (greedy) policy that is evaluated and improved is different from the ($\\epsilon$-greedy) policy that is used to select actions.\n",
    "    - On-policy TD control methods (like Expected Sarsa and Sarsa) have better online performance than off-policy TD control methods (like Sarsamax).\n",
    "    - Expected Sarsa generally achieves better performance than Sarsa. <br>\n",
    "    \n",
    "If you would like to learn more, you are encouraged to read Chapter 6 of the textbook (especially sections 6.4-6.6).\n",
    "\n",
    "As an optional exercise to deepen your understanding, you are encouraged to reproduce Figure 6.4. (Note that this exercise is optional!)\n",
    "\n",
    "<img src=\"img/6_6_16_1.png\" alt=\"Drawing\" style=\"width: 774px;\"/> <br>    \n",
    "\n",
    "- The figure shows the performance of Sarsa and Q-learning on the cliff walking environment for constant $\\epsilon = 0.1$. As described in the textbook, in this case,\n",
    "    - Q-learning achieves worse online performance (where the agent collects less reward on average in each episode), but learns the optimal policy, and\n",
    "    - Sarsa achieves better online performance, but learns a sub-optimal \"safe\" policy. <br>\n",
    "    \n",
    "You should be able to reproduce the figure by making only small modifications to your existing code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "my note: <br>\n",
    "i liked the idea in the figure that they smoothed the curves by averaging the reward sums from 10 successive episodes . i mean, i can use this technique if individual points are not important and i want to see the bigger picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-6.17) Summary\n",
    "\n",
    "<img src=\"img/6_6_17_1.png\" alt=\"Drawing\" style=\"width: 450px;\"/> <br>\n",
    "\n",
    "#### TD Prediction: TD(0)\n",
    "- Whereas Monte Carlo (MC) prediction methods must wait until the end of an episode to update the value function estimate, temporal-difference (TD) methods update the value function after every time step.\n",
    "- For any fixed policy, **one-step TD** (or **TD(0)**) is guaranteed to converge to the true state-value function, as long as the step-size parameter $\\alpha$ is sufficiently small.\n",
    "- In practice, TD prediction converges faster than MC prediction.\n",
    "\n",
    "<img src=\"img/6_6_17_2.png\" alt=\"Drawing\" style=\"width: 490px;\"/> <br>\n",
    "\n",
    "#### TD Prediction: Action Values\n",
    "- (In this concept, we discussed a TD prediction algorithm for estimating action values. Similar to TD(0), this algorithm is guaranteed to converge to the true action-value function, as long as the step-size parameter $\\alpha$ is sufficiently small.)\n",
    "\n",
    "#### TD Control: Sarsa(0)\n",
    "- **Sarsa(0)** (or **Sarsa**) is an on-policy TD control method. It is guaranteed to converge to the optimal action-value function $q_*$, as long as the step-size parameter $\\alpha$ is sufficiently small and $\\epsilon$ is chosen to satisfy the **Greedy in the Limit with Infinite Exploration (GLIE)** conditions.\n",
    "\n",
    "<img src=\"img/6_6_17_3.png\" alt=\"Drawing\" style=\"width: 490px;\"/> <br>\n",
    "\n",
    "#### TD Control: Sarsamax\n",
    "- **Sarsamax** (or **Q-Learning**) is an off-policy TD control method. It is guaranteed to converge to the optimal action value function $q_*$, under the same conditions that guarantee convergence of the Sarsa control algorithm.\n",
    "\n",
    "<img src=\"img/6_6_17_4.png\" alt=\"Drawing\" style=\"width: 490px;\"/> <br>\n",
    "\n",
    "#### TD Control: Expected Sarsa\n",
    "- **Expected Sarsa** is an on-policy TD control method. It is guaranteed to converge to the optimal action value function $q_*$, under the same conditions that guarantee convergence of Sarsa and Sarsamax.\n",
    "\n",
    "<img src=\"img/6_6_17_5.png\" alt=\"Drawing\" style=\"width: 490px;\"/> <br>\n",
    "\n",
    "#### Analyzing Performance\n",
    "- On-policy TD control methods (like Expected Sarsa and Sarsa) have better online performance than off-policy TD control methods (like Q-learning).\n",
    "- Expected Sarsa generally achieves better performance than Sarsa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=red>Lesson 7 : Solve OpenAI Gym's Taxi-v2 Task </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With reinforcement learning now in your toolbox, you're ready to explore a mini project using OpenAI Gym!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (3 concepts)\n",
    "- 1- Introduction\n",
    "- 2- Instructions\n",
    "- 3- Mini-Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-7.1) Introduction \n",
    "<img src=\"img/6_7_1_1.png\" alt=\"Drawing\" style=\"width: 266px;\"/> <br>\n",
    "\n",
    "For this coding exercise, you will use OpenAI Gym's `Taxi-v2` environment to design an algorithm to teach a taxi agent to navigate a small gridworld. The goal is to adapt all that you've learned in the previous lessons to solve a new environment!\n",
    "\n",
    "Before proceeding, read the description of the environment in subsection 3.1 of [this paper](https://arxiv.org/pdf/cs/9905014.pdf).\n",
    "\n",
    "You can verify that the description in the paper matches the OpenAI Gym environment by peeking at the code [here](https://github.com/openai/gym/blob/master/gym/envs/toy_text/taxi.py).\n",
    "\n",
    "Answer the quiz questions below to check your understanding of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-7.2) Instructions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <font color=red> (6-7.3) Mini-Project </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 8 : RL in Continuous Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review the fundamental concepts of reinforcement learning, and learn how to adapt traditional algorithms to work with continuous spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (3 concepts)\n",
    "- 1- Deep Reinforcement Learning\n",
    "- 2- Resources\n",
    "- 3- Discrete vs. Continuous Space\n",
    "- 4- Quiz: Space Representation\n",
    "- 5- Discretization\n",
    "- 6- Exercise: Discretization\n",
    "- 7- Tile Coding\n",
    "- 8- Exercise: Tile Coding\n",
    "- 9- Coarse Coding\n",
    "- 10- Function Approximation\n",
    "- 11- Linear Function Approximation\n",
    "- 12- Kernel Functions\n",
    "- 13- Non-Linear Function Approximation\n",
    "- 14- Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.1) Deep Reinforcement Learning\n",
    "\n",
    "- Welcome to Deep Reinforcement Learning. Interest in the field of reinforcement learning seems to have almost exploded with success stories like AlphaGo and platforms like OpenAI. Research in this area has been moving at a steady pace since the 1980s, but it has really taken off with recent advances in deep learning. As we progress through this module, you will design intelligent agents that can learn to carry out complex control tasks. These include simple domains like physics problems and board games to video games where the agent processes raw pixel data and even robotics. My favorite part of reinforcement learning is watching an agent grow and get better and better at a task. This is not always easy to achieve but once you can get your agent to learn the nuances (الفروق الدقيقة) of a task, it can perform it flawlessly from then on. And that is the most rewarding experience in the world. <br><br> \n",
    "\n",
    "- Before we dive into \"Deep Reinforcement Learning\", let's quickly review some fundamental concepts. Reinforcement learning problems are typically framed as Markov Decision Processes or MDPs. An MDP consists of a set of states S and actions A along with probabilities P, rewards R and a discount factor gamma. \n",
    "\n",
    "<img src=\"img/6_8_1_1.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- P captures how frequently different transitions and rewards occur, often modeled as a single joint probability where the state and reward at any time step t plus one depend only on the state and action taken at the previous time step t. This characteristic of certain environments is known as the Markov property. \n",
    "\n",
    "<img src=\"img/6_8_1_2.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- There are two quantities that we are typically interested in. \n",
    "    - The value of a state V(S), which we try to estimate or predict. \n",
    "    - And the value of an action taken in a certain state, Q(S, A) which can help us decide what action to take. \n",
    "    \n",
    "<img src=\"img/6_8_1_3.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>    \n",
    "    \n",
    "- These two mappings or functions are very much interrelated and help us find an optimal policy for our problem pie star that maximizes the total reward received. \n",
    "\n",
    "<img src=\"img/6_8_1_4.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- Note that since MDPs are probabilistic in nature, we can't predict with complete certainty what future rewards we will get and for how long. So, we typically aim for total expected reward. This is where the discount factor gamma comes into play as well. It is used to assign a lower weighted to future rewards when computing state and action values. \n",
    "\n",
    "<img src=\"img/6_8_1_5.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- Reinforcement learning algorithms are generally classified into two groups. \n",
    "    - Model-based approaches \n",
    "        - such as policy iteration and value iteration require a known transition and reward model. They essentially apply dynamic programming to iteratively compute the desired value functions and optimal policies using that model. <br>\n",
    "        \n",
    "<img src=\"img/6_8_1_6.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>        \n",
    "        \n",
    "On the other hand, \n",
    "- _        \n",
    "    - model-free approaches \n",
    "        - including Monte Carlo Methods and Temporal-Difference Learning don't require an explicit model. They sample the environment by carrying out exploratory actions and use the experience gained to directly estimate value functions. \n",
    "        \n",
    "<img src=\"img/6_8_1_7.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>           \n",
    "        \n",
    "- Okay, obviously there is more to it but that's <u>reinforcement learning</u> in a nutshell. <u>**Deep** Reinforcement Learning</u> is a relatively recent term that refers to approaches that use deep learning, mainly, Multi-Layer Neural Networks to solve reinforcement learning problems. **Now, reinforcement learning is typically characterized by finite MDPs, where the number of states and actions is limited. But there are so many problems where the space of states and actions is very large or even made of continuous real value numbers. Traditional algorithms use a table or a dictionary or other finite structure to capture state and action values. They no longer work for such problems.**  <br><br>\n",
    "\n",
    "- So, \n",
    "    - the first thing you will learn in this module (pic below) is how to generalize these algorithms to work with large and continuous spaces. \n",
    "    - That lays the foundation for developing Deep Reinforcement Learning algorithms including value-based techniques like Deep Q-Learning and \n",
    "    - those that directly try to optimize the policy, such as Policy Gradients. \n",
    "    - Finally, you will look at more advanced approaches that try to combine the best of both worlds, Actor-Critic Methods. \n",
    "    \n",
    "<img src=\"img/6_8_1_8.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>           \n",
    "    \n",
    "- These algorithms can be hard to understand. So, don't worry if you find them challenging at first. Make sure you practice implementing the core competence of these algorithms and apply them to various environments. Observe how they perform. That is the only way to master Deep Reinforcement Learning.\n",
    "\n",
    "---\n",
    "Note: $\\mathcal{R}$ is the set of all rewards. The reward probability is jointly specified with the transition probability as: $p(s', r | s, a) = \\mathbb{P}(S_{t+1}=s', R_{t+1}=r|S_t=s, A_t=a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.2) Resources\n",
    "\n",
    "#### Exercises\n",
    "To complete the exercises in this lesson, clone the following GitHub repository:\n",
    "\n",
    "> github.com/udacity/reinforcement-learning/\n",
    "\n",
    "Make sure you have Python 3 installed, either natively or through Anaconda, along with the following packages:\n",
    "`numpy, scipy, pandas, matplotlib, scikit-learn, tqdm, ipython, jupyter, gym`\n",
    "\n",
    "Additional packages, if needed, may be listed at the beginning of specific exercise notebooks.\n",
    "\n",
    "#### OpenAI Gym\n",
    "We'll be using **OpenAI Gym** for coding exercises throughout this class. It is an open-source library and platform for developing and sharing reinforcement learning algorithms. If you haven't used it before, now is a good time to get familiar with it.\n",
    "\n",
    "Read the instructions in the [OpenAI Gym documentation](https://gym.openai.com/docs/) to learn the basic syntax and usage.\n",
    "\n",
    "> *The documentation has instructions for installing OpenAI Gym on your computer that might be helpful. Some environments have additional dependencies that you may need to install (e.g. a physics or rendering engine.)*\n",
    "\n",
    "You're also encouraged to take the time to check out the [leaderboard](https://github.com/openai/gym/wiki/Leaderboard), which contains the best solutions to each task.\n",
    "\n",
    "Check out this [blog post](https://blog.openai.com/openai-gym-beta/) to read more about how OpenAI Gym is used to accelerate reinforcement learning (RL) research.\n",
    "\n",
    "#### Textbook: Sutton & Barto, 2nd Ed.\n",
    "We will recommend you to read excerpts from this classic textbook on reinforcement learning. The topics we cover in Deep Reinforcement Learning are discussed in **Part II: Approximate Solution Methods**. In addition, we'll refer to important papers that provide further details about specific algorithms and techniques.\n",
    "\n",
    "> *Note that all of the suggested readings are optional! But they are highly recommended, esp. if you find a topic interesting and want to know more about it, or if something is unclear and you need an alternate explanation.*\n",
    "\n",
    "Check out this [GitHub repository](https://github.com/ShangtongZhang/reinforcement-learning-an-introduction) to see Python implementations of most of the figures in the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.3) Discrete vs. Continuous Space\n",
    "\n",
    "- Let us first take a look at what we mean by discrete and continuous spaces. \n",
    "- Recall the definition of a Markov Decision Process where (pic below) we assume that the environment state at any time is drawn from a set of possible states. When the set is finite, we can call it a discrete state space. Similarly with actions, if there is a finite set of them, the environment is said to have a discrete action space. \n",
    "\n",
    "<img src=\"img/6_8_3_1.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>  \n",
    "\n",
    "- Having discrete spaces simplifies things for us. \n",
    "- For starters, it allows us to represent any function of states and actions as a dictionary or look-up table. Consider the state value function V (pic below) which is a mapping from the set of states to a real number. If you encode states as integers, you can code up the value function as a dictionary using each state as a key. \n",
    "- Similarly, consider the action value function Q that maps every state action pair to a real number. Again, you could use a dictionary here or store the value function as a table or matrix, where each row corresponds to a state, and each column to an action. \n",
    "\n",
    "<img src=\"img/6_8_3_2.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>  \n",
    "\n",
    "- Discreet spaces are also critical to a number of reinforcement learning algorithms. \n",
    "    - For instance, in \"value iteration\" (algorithm) (note: this is an example of model-based learning), this internal `for-loop` (pic bleow) goes over each state as one by one, and updates the corresponding value estimate V of s. This is impossible if you have an infinite state space. The loop would go on forever even for discrete state spaces with a lot of states this can quickly become infeasible. \n",
    "\n",
    "<img src=\"img/6_8_3_3.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>  \n",
    "\n",
    "- _    \n",
    "    - Model-free methods like Q-learning assume discrete spaces as well. Here, this `max` (pic below) is being computed over all possible actions from state S prime which is easy when you have a finite set of actions. But this tiny step itself becomes a full-blown optimization problem if your action space is continuous. \n",
    "    \n",
    "<img src=\"img/6_8_3_4.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>      \n",
    "    \n",
    "- So what do we exactly mean by continuous spaces. The term continuous is used to contrast with discrete. That is a continuous space is not restricted to a set of distinct values like integers. Instead it can take a range of values, typically real numbers. This means, quantities like state values that could be depicted as, say a bar chart, for a discrete case, one bar for every state (left chart in pic below), will now need to be thought of as a a density plot over a desired range (right chart in pic below). \n",
    "\n",
    "<img src=\"img/6_8_3_5.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>      \n",
    "\n",
    "- The same notion extends to environments where the <u>state</u> is no longer a single real valued number but a <u>vector of</u> such <u>numbers</u>. This is <u>still referred to as a continuous space</u> just with more than one dimension.  <br><br>\n",
    "\n",
    "- Okay, before we go any further, let's try to build some intuition for why continuous state spaces are important. Where do they even come from? \n",
    "- When you consider a high-level decision making task like playing chess, you can often think of the set of possible **states** as discrete. What piece is in which square on the board. You don't need to bother with precisely where each piece is located within its square or which way it is facing. Although these details are available for you to inspect and wonder about, why is your knight staring at my queen, these things are not relevant to the problem at hand and you can abstract them away in your model of the game. \n",
    "- In general, grid-based worlds are very popular in reinforcement learning. They give you a glimpse at how agents might act in spatial environments. But real physical spaces are not always neatly divided up into grids. There is no cell (5,3) for the vacuum cleaner robot to go to. \n",
    "\n",
    "<img src=\"img/6_8_3_6.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br> \n",
    "\n",
    "- It (vacuum cleaner robot) has to chart a course from its current position to say 2.5 meters from the west wall by 1.8 meters from the north wall. It also has to keep track of its heading and turn smoothly to face the direction it wants to move in. These are all real numbers that the agent may need to process and represent as part of the **state**. \n",
    "\n",
    "<img src=\"img/6_8_3_7.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- **Actions** too can be continuous. Take for example a robot that plays darts. It has to set the height and angle it wants to release the dart at, choose an appropriate level of power with which to throw et cetera. Even small differences in these values can have a large impact on where the dart ultimately lands on the board. \n",
    "\n",
    "<img src=\"img/6_8_3_8.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>\n",
    "\n",
    "- In general, most **actions** that need to take place in a physical environment are continuous in nature. Clearly, we need to modify our representation or algorithms or both to accommodate continuous spaces. \n",
    "- The two main strategies we'll be looking at are \n",
    "    - Discretization and \n",
    "    - Function Approximation.\n",
    "    \n",
    "<img src=\"img/6_8_3_9.png\" alt=\"Drawing\" style=\"width: 650px;\"/> <br>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.4) Quiz: Space Representation\n",
    "<img src=\"img/6_8_4_1.png\" alt=\"Drawing\" style=\"width: 400px;\"/> <br>    \n",
    "\n",
    "- QUIZ QUESTION\n",
    "    - Which of the following state or action spaces can be encoded using **discrete** representations?\n",
    "        - (the right answers are the following)\n",
    "            - A hand of cards when playing Poker\n",
    "            - Board positions for a 9x9 Go game\n",
    "            - Keys to play on a musical keyboard\n",
    "        - (wrong answers with my explanation are the following)\n",
    "            - Force applied when grasping with a robotic arm\n",
    "            - GPS coordinates for autonomous driving\n",
    "\n",
    "---\n",
    "note from udacity when submitting the quiz:\n",
    "Yes, these spaces can be reasonably encoded using discrete representations!\n",
    "\n",
    "Check out this [table of the environments](https://github.com/openai/gym/wiki/Table-of-environments) available in OpenAI Gym. Here `Discrete(...)` refers to a discrete space, and `Box(...)` indicates a continuous space. See [documentation](https://gym.openai.com/docs/#spaces) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.5) Discretization\n",
    "\n",
    "- As the name suggests, discretization is basically converting a continuous space into a discrete one. \n",
    "- Remember our continuous vacuum cleaner world? All we're saying is let's bring back a grid structure with discrete positions identified. \n",
    "\n",
    "<img src=\"img/6_8_5_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>   \n",
    "\n",
    "- Note that we're not really forcing our agent to be in exactly the center of these positions. Since the underlying world is continuous, we don't have control over that. But in our representation of the state space, we only identify certain positions as relevant. For instance, whether the robot is at (3.1, 2.4), or (2.9, 1.8), we can round that off to (3, 2). Yes, this will almost always be a little incorrect, but for some environments, discretizing the **state-space** can work out very well. It enables us to use existing algorithms with little, or no modification. <br><br>\n",
    "\n",
    "- **Actions** can be discretized as well. For example, angles can be divided into whole degrees, or even 90 degrees increments, if appropriate. Now, let's imagine there are objects in this discretized world, obstacles that the robot may need to avoid. \n",
    "\n",
    "<img src=\"img/6_8_5_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>  \n",
    "\n",
    "- With our grid representation, all we can do is mark off the cells where an object is present, even by a little. This is known as an occupancy grid. \n",
    "\n",
    "<img src=\"img/6_8_5_3.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>  \n",
    "\n",
    "- But our choice of discretization may lead the agent into thinking, there is no path across these obstacles to reach some desired locations. Instead, if we could vary the grid according to these obstacles, then we could open up a feasible path for the agent. \n",
    "\n",
    "<img src=\"img/6_8_5_4.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>  \n",
    "\n",
    "- An alternate approach would be to divide up the grid into smaller cells where required. (to be coninued)\n",
    "\n",
    "<img src=\"img/6_8_5_5.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>  \n",
    "\n",
    "- (continiue...) It would still be an approximation. But it'll allow us to allocate more of our state representation to where it matters. Better than dividing the entire state space into finer cells, which may increase the total number of states, and in turn, the time needed to compute value functions. If you're familiar with \"binary space partitioning\", or \"quad trees\", this is exactly the same idea. <br><br>\n",
    "\n",
    "- Now, you may be wondering. This sort of discretization makes sense in spacial domains like gridworlds. But what about other state spaces? Let's change gears and look at a different domain. Most cars these days have automatic transmission. Have you ever wondered how the car decides to pick what gear to switch to, and when? Here's a simplified plot of how fuel consumption varies with speed for different gears in a typical car. \n",
    "\n",
    "<img src=\"img/6_8_5_6.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>  \n",
    "\n",
    "- Let's assume that our state only consists of the vehicle speed, and which gear we are in. And our reward is inversely proportional to fuel consumption. The actions available to our agent are essentially switching up, or down. \n",
    "- Now, although speed is a continuous value, it can be discretized into ranges, such that a single gear is the most optimal in each range. Note that these ranges can be of different lengths, that is, the discretization is non-uniform. \n",
    "\n",
    "<img src=\"img/6_8_5_7.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>  \n",
    "\n",
    "- If there were other dimensions to the state-space such as throttle position, then they could be subdivided non-uniformly as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (6-8.6) Exercise: Discretization </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.7) Tile Coding\n",
    "\n",
    "- If you have prior knowledge about the state space, you can manually design an appropriate discretisation scheme. Like in our gears switching example, we knew the relationship between fuel consumption and speed. **But** in order to function in arbitrary environments, we need a more generic method. \n",
    "\n",
    "- One elegant approach for this is tile coding. Here, the underlying state space is continuous and two dimensional. \n",
    "\n",
    "<img src=\"img/6_8_7_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>  \n",
    "\n",
    "- We overlay multiple grids or tilings on top of the space, each slightly offset from each other. \n",
    "\n",
    "<img src=\"img/6_8_7_2.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>  \n",
    "\n",
    "- Now, any position S in the state space can be coarsely identified by the tiles that it activates. \n",
    "\n",
    "<img src=\"img/6_8_7_3.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- If we assign a bit to each tile, then we can represent our new discretised state as a bit vector, with ones for the tiles that get activated, and zeros elsewhere. This, by itself, is a very efficient representation. <br><br>\n",
    "\n",
    "- But the genius lies in how the state value function is computed using the scheme. Instead of storing a separate value for each state V of S, it is defined in terms of the bit vector for that state and a weight for each tile. \n",
    "\n",
    "<img src=\"img/6_8_7_4.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- The tile coding algorithm in turn updates these weights iteratively. This ensures nearby locations that share tiles also share some component of state value, effectively smoothing the learned value function.  <br><br>\n",
    "\n",
    "- Tile coding does have some drawbacks. Just like a simple grid based approach we have to manually select the tile sizes, there offsets, number of tilings, etc, ahead of time. <br><br>\n",
    "\n",
    "\n",
    "- A more flexible approach is adaptive tile coding, which starts with fairly large tiles, and divides each tile into two whenever appropriate. \n",
    "\n",
    "<img src=\"img/6_8_7_5.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- How do we know when to split? Well, we can use a hero stick for that. Basically, we want to split the state space when we realize that we are no longer learning much with the current representation. That is, when our value function isn't changing. We can stop when we have reached some upper limit on the number of splits, or some max iterations. \n",
    "- In order to figure out which tile to split, we have to look at which one is likely to have the greatest effect on the value function. For this, we need to keep track of subtiles and their projected weights. Then, we can pick the tile with the greatest difference between subtile weights. \n",
    "- There are many other heuristics you can use, but the main advantage of adaptive tile coding, is that it does not rely on a human to specify a discretisation ahead of time. The resulting state space is appropriately partitioned based on its complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (6-8.8) Exercise: Tile Coding </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.9) Coarse Coding\n",
    "(a language note from me : coarse = خشن أو غليظ)\n",
    "\n",
    "---\n",
    "- Coarse coding is just like Tile coding, but uses a sparser set of features to encode the state space. <br><br>\n",
    "\n",
    "- Imagine dropping a bunch of circles on your 2D continuous state space. Take any state S which is a position in this space, and mark all the circles that it belongs to. \n",
    "\n",
    "<img src=\"img/6_8_9_1.png\" alt=\"Drawing\" style=\"width: 500px;\"/> <br>\n",
    "\n",
    "- Prepare a bit vector with a one for those circles and 0 for the rest. And that's your sparse coding representation of the state. \n",
    "- Looking at a 2D space helps us visualize the basic idea. But it also extends to higher dimensions where circles become spheres and hyper spheres.  <br><br>\n",
    "\n",
    "- There are some neat properties of coarse coding. Using smaller circles (left in pic below) results in less generalization across the space. The learning algorithm has to work a bit longer, but you have greater effective resolution. Larger circles (middle in pic below) lead to more generalization, and in general a smoother value function. You can use fewer large circles to cover the space, thus reducing your representation, but you would lose some resolution. \n",
    "- It's not just the size of these circles that we can vary. We can change them in other ways like making them taller or wider to get more resolution along one dimension versus the other (right in pic below). In fact, this same technique generalizes to pretty much any shape (فهمى يعنى نقدر نعمل استطالة أى شكل من الأشكال الأخرى ، فمثلا نعمل استطالة للمربع أو لشكل النجمة ، هل فهمى صح ؟) . \n",
    "\n",
    "<img src=\"img/6_8_9_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "-  In coarse coding, just like in tile coding, are resulting state representation is a binary vector. Think of each tile or circle as a feature. One, if it is an active, zero if it is not. \n",
    "- A natural extension to this idea is to use the distance from the center of each circle as a measure of how active that feature is (pic below). This measure or response can be made to fall off smoothly using a Gaussian or a bell-shaped curve centered on the circle, which is known as a radial basis function (RBF). Of course, the resulting feature values will no longer be discrete so we'll end up with yet another continuous state vector. But what is cool is that the number of features can be drastically reduced this way. \n",
    "\n",
    "<img src=\"img/6_8_9_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- We'll look at RBFs more closely later when we discuss function approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.10) Function Approximation\n",
    "\n",
    "- So far, we've looked at ways to discretize continuous state spaces. This enables us to use existing reinforcement learning algorithms with little or no modification. But there are some limitations: \n",
    "    - When the underlying space is complicated, the number of discrete states needed can become very large. Thus, we lose the advantage of discretization. \n",
    "    - Moreover, if you think about positions in the state space that are nearby, you would expect their values to be similar, or smoothly changing. Discretization doesn't always exploit this characteristic, failing to generalize well across the space. \n",
    "    \n",
    "- What we're after is the true state value function $v_{\\pi}(s)$, or action value function $q_{\\pi}(s, a)$. Which is typically smooth and continuous over the entire space. \n",
    "\n",
    "<img src=\"img/6_8_10_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- As you can imagine, **capturing this completely is practically infeasible except for some very simple problems. So our best hope is function approximation**. It is still an <u>*approximation*</u> because we don't know what the true underlying function is.  <br><br>\n",
    "\n",
    "- A general way to define such an approximation is to introduce a parameter vector W that shapes the function. Our tasks then, reduces to tweaking this parameter vector till we find the desired approximation. \n",
    "\n",
    "<img src=\"img/6_8_10_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- Note that the approximating function can either map \n",
    "    - a state to its value, or \n",
    "    - a state action pair to the corresponding q value. \n",
    "    - Another form is where we map from one state to a number of different q values, one for each action all at once. This is especially useful for q learning as we'll see later. \n",
    "    \n",
    "<img src=\"img/6_8_10_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>    \n",
    "    \n",
    "- Let's focus on this first case. Approximating a state value function. \n",
    "- Now, we have this box here in the middle that's supposed to do some magic. And convert the state s, and parameter vector W into a scalar value. But how? \n",
    "\n",
    "<img src=\"img/6_8_10_4.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>    \n",
    "\n",
    "- The first thing we need to do is to ensure we have a vector representing the state. Your state might already be a vector in which case you don't need to do anything. In general, we'll define a transformation that converts any given state s into a feature vector X(s). \n",
    "\n",
    "<img src=\"img/6_8_10_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br> \n",
    "\n",
    "- This also gives us more flexibility, since we don't have to operate on the raw state values. We can use any computed or derived features instead. \n",
    "- Okay, we now have a feature vector X(S), and a parameter vector W, and we want a scalar value. What do we do when we have two vectors, and want to produce a scalar? Dot Product. \n",
    "\n",
    "<img src=\"img/6_8_10_6.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br> \n",
    "\n",
    "- Yes. It's the simplest thing we could do. In fact, this is the same as computing a linear combination of features. Multiply each feature with the corresponding weight, and sum it up. \n",
    "\n",
    "<img src=\"img/6_8_10_7.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- This is known as linear function approximation. That is we are trying to approximate the underlying value function with a linear function.\n",
    "\n",
    "--- \n",
    "#### Function Approximation\n",
    "Given a problem domain with continuous states $s \\in \\mathcal{S} = {\\mathbb{R}^{n}}$, we wish to find a way to represent the value function $v_{\\pi}(s)$ (for prediction) or $q_{\\pi}(s, a)$ (for control).\n",
    "\n",
    "We can do this by choosing a parameterized function that <u>*approximates*</u> the true value function:\n",
    "\n",
    "$\\hat{v}(s, \\mathbf{w}) \\approx v_{\\pi}(s)$\n",
    "\n",
    "$\\hat{q}(s, a, \\mathbf{w}) \\approx q_{\\pi}(s, a)$\n",
    "\n",
    "Our goal then reduces to finding a set of parameters $\\mathbf{w}$ that yield an optimal value function. We can use the general reinforcement learning framework, with a Monte-Carlo or Temporal-Difference approach, and modify the update mechanism according to the chosen function.\n",
    "\n",
    "#### Feature Vectors\n",
    "A common intermediate step is to compute a feature vector that is representative of the state: $\\mathbf{x}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.11) Linear Function Approximation\n",
    "\n",
    "- Let's take a closer look at linear function approximation and how to estimate the parameter vector w. As you've seen already, a linear function is a simple sum over all the features multiplied by their corresponding weights. Let's assume you have initialized these weights randomly and computed the value of a state v hat (s,w). \n",
    "\n",
    "<img src=\"img/6_8_11_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- How would you tweak w to bring the approximation closer and closer to the true function? Sounds like a numerical optimization problem.  <br><br>\n",
    "\n",
    "- Let's use gradient descent to find the optimal parameter vector. \n",
    "    - Firstly, note that since v hat is a linear function, its derivative with respect to w is simply the feature vector X(s) (pic belwo). This is a nice thing about linear functions and why they are so popular. We'll use this shortly. \n",
    "    \n",
    "<img src=\"img/6_8_11_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- _    \n",
    "    - Now let's think about what we are trying to optimize. We said we want to reduce or minimize the difference between the true value function v pi and the approximate value function v hat. \n",
    "    - Let's write that down as a squared difference, since we are not concerned with the sign of the error and we simply want to drive the difference down toward zero (pic below). To be more accurate, since reinforcement learning domains are typically stochastic, this is the expected squared error. \n",
    "    \n",
    "<img src=\"img/6_8_11_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>    \n",
    "\n",
    "- _    \n",
    "    - Alright we now have an objective function to minimize. To do that using gradient descent, let's find the gradient or derivative of this function with respect to w (pic below). Using the chain rule of differentiation, we get minus two times the value difference times the derivative of v hat, which we earlier noted was simply the feature vector (x,s). Note that we remove the expectation operator here to focus on the error gradient indicated by a single state s, which we assume has been chosen stochastically. If we are able to sample enough states, we can come close to the expected value. \n",
    "    \n",
    "<img src=\"img/6_8_11_4.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>        \n",
    "    \n",
    "- _    \n",
    "    - Let's plug this in to the general form of a gradient descent update rule with alpha as a step-size or learning rate parameter. Note that the minus half here is just to cancel out the minus two we got in the derivative. \n",
    "    \n",
    "<img src=\"img/6_8_11_5.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>    \n",
    "    \n",
    "- This is the basic formulation we will use to iteratively reduce the error for each sample state till the approximate and true function are almost equal.  <br><br>\n",
    "\n",
    "- Here's an intuitive explanation of how gradient descent optimizes the parameter vector. In each iteration, you change the weights a small step away from the error direction. \n",
    "\n",
    "<img src=\"img/6_8_11_6.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>    \n",
    "\n",
    "- So, the feature vector here points out what direction is bad so that we can move away from it. <br><br>\n",
    "\n",
    "- So far, we've only been talking about approximating the <u>state-value function</u>. In order to solve a model-free control problem, that is, to take actions in an unknown environment, we need to approximate the <u>action-value function</u>. We can do this by defining a feature transformation that utilizes both the state and action, then we can use the same gradient descent method as we did for the state-value function. \n",
    "\n",
    "<img src=\"img/6_8_11_7.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- Finally, let's look at the case where we wish the approximation function to compute all of the action-values at once. We can think of this as producing an action vector. For this purpose, we can continue to use the same feature transformation as before, taking in both the state and action. But now, how do we generate the different action-values? (continue ...)\n",
    "\n",
    "<img src=\"img/6_8_11_8.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- (continue ...) One way of thinking about it is that we are trying to find n different action-value functions, one for each action dimension. But intuitively, we know that these functions are related. So, it makes sense to compute them together. We can do this by extending our weight vector and turning it into a matrix. \n",
    "\n",
    "<img src=\"img/6_8_11_9.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- Each column of the matrix emulates a separate linear function, but the common features computed from the state and action keep these functions tied to each other. If we have a problem domain with a continuous state space, but a discrete action space which is very common, we can easily select the action with the maximum value. Without this sort of parallel processing, we would have to parcel each action one by one and then find their maximum. If our action space is also continuous, then this form allows us to output more than a single value at once. For example, if we were driving a car, we'd want to control both steering and throttle at the same time. <br><br>\n",
    "\n",
    "- The primary limitation of linear function approximation is that we can only represent linear relationships between inputs and outputs. With one-dimensional input, this is basically a line. In 2D, it becomes a plane, and so on. \n",
    "\n",
    "<img src=\"img/6_8_11_10.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- What if our underlying value function has a non-linear shape? A linear approximation may give a very bad result. That's when we need to start looking at non-linear functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.12) Kernel Functions\n",
    "\n",
    "- A simple extension to linear function approximation can help us capture non-linear relationships. At the heart of this approach is our feature transformation. Remember how we defined it in a generic sense? Something that takes a state or a state action pair and produces a feature vector? (continue ...)\n",
    "\n",
    "<img src=\"img/6_8_12_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- (continue ...) Each element of this vector can be produced by a separate function, which can be non-linear. For example, let's assume our state S is a single real number. Then we can define say X1(S) equals S, X2(S) equals X squared, X3(S) equals X cube, et cetera. (continue ...) \n",
    "\n",
    "<img src=\"img/6_8_12_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- (continue ...) These are called Kernel Functions or Basis Functions. They transform the input state into a different space. But note that since our value function is still defined as a linear combination of these features, we can still use linear function approximation. What this allows the value function to do is represent non-linear relationships between the input state and output value.  <br><br>\n",
    "\n",
    "- Radial Basis Functions are a very common form of Kernels used for this purpose. You might've heard of them. (continue ...)\n",
    "\n",
    "<img src=\"img/6_8_12_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- (continue ...) Essentially, think of the current state S as a location in the continuous state space here depicted as a rectangular plane. Each Basis Function is shown as a blob. The closer the state is to the center of the blob, the higher the response returned by the function. And the farther you go, the response falls off gradually with the radius. Hence the name Radial Basis Function. Mathematically, this can be achieved by associating a Gaussian Kernel with each Basis Function with its mean serving as the center of the blob and standard deviation determining how sharply or smoothly the response falls off. So, for any given state, we can reduce the state representation to a vector of responses from these Radial Basis Functions. From that point onwards, we can use our same function approximation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.13) Non-Linear Function Approximation\n",
    "\n",
    "- Non-linear function approximation, this is what we've been building up to in this lesson.  <br><br>\n",
    "\n",
    "- Recall from our previous discussion how we can capture non-linear relationships between input state and output value using arbitrary kernels like radial basis functions as our feature transformation. In this model, our output value is still linear with respect to the features. \n",
    "\n",
    "<img src=\"img/6_8_13_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- What if our underlying value function was truly non-linear with respect to a combination of these feature values? To capture such complex relationships, let's pass our linear response obtained using the dot product through some nonlinear function f. \n",
    "\n",
    "<img src=\"img/6_8_13_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- Does this look familiar? Yes, it is the basis of artificial neural networks. Such a non-linear function is generally called an activation function and immensely increases the representational capacity of our approximator. We can iteratively update the parameters of any such function using gradient descent (pic below): Learning rate alpha times value difference times the derivative of the function with respect to the weights.\n",
    "\n",
    "<img src=\"img/6_8_13_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-8.14) Summary\n",
    "\n",
    "- In summary, here's what you learned in this lesson. Traditional reinforcement learning techniques use a finite MDP to model an environment which limits us to environments with discrete state and action spaces. \n",
    "\n",
    "<img src=\"img/6_8_14_1.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- In order to extend our learning algorithms to continuous spaces, we can do one of two things. \n",
    "    - **Discretize** the state space or \n",
    "    - **directly try to approximate** desired value functions. <br><br>\n",
    "    \n",
    "- **Discretization** can be performed using a constant grid, tile coding, or course coding. This **indirectly** leads to an **approximation** of the value function. \n",
    "\n",
    "<img src=\"img/6_8_14_2.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- **Directly approximating** a continuous value function can be done by first, defining a feature transformation and then computing a linear combination of those features. Using non-linear feature transforms like radial basis functions, allows us to use the same linear combination framework to capture some non-linear relationships. \n",
    "\n",
    "<img src=\"img/6_8_14_3.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- In order to represent non-linear relationships across combinations of features, we can apply an activation function. \n",
    "\n",
    "<img src=\"img/6_8_14_4.png\" alt=\"Drawing\" style=\"width: 600px;\"/> <br>\n",
    "\n",
    "- And this sets us up to use deep neural networks for reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 9 : Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extend value-based reinforcement learning methods to complex problems using deep neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.1) Intro to Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.2) Neural Nets as Value Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.3) Monte Carlo Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.4) Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.5) Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.6) Deep Q Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.7) Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.8) Fixed Q Targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.9) Deep Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.10) DQN Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.11) Implementing Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=red> (6-9.12) TensorFlow Implementation </font>\n",
    "<font color=red> Warning: I did not download this. </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-9.13) Wrap Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 10 : Policy-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy-based methods try to directly optimize for the optimal policy. Learn how they work, and why they are important, especially for domains with continuous action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.1) Policy-Based Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.2) Why Policy-Based Methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.3) Policy Function Approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.4) Stochastic Policy Search\n",
    "- First, I wanted to know the difference between stochastic and random, so using google :\n",
    "    - A variable is random. A process is stochastic. Apart from this difference, the two words are synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.5) Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.6) Monte Carlo Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.7) Constrained Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-10.8) Recap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 11 : Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn how to combine value-based and policy-based methods, bringing together the best of both worlds, to solve challenging reinforcement learning problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.1) Actor-Critic Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.2) A Better Score Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.3) Two Function Approximators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.4) The Actor and The Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.5) Advantage Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.6) Actor-Critic with Advantage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (6-11.7) Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "links to tensor flow by jay alammar https://www.kaggle.com/jalammar/intro-to-data-input-pipelines-with-tf-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a tweet by Mat Leonard, in which someone,in a blog, talks about why we may leave matplotlib in favor of Altair for visualization, the blog starts by saying \"Sadly, in Python, we do not have a ggplot2.\" (the visualization library in R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Side effects in python !!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999964\n"
     ]
    }
   ],
   "source": [
    "print(8.5-8.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "see this link for explanation -> https://www.quora.com/In-Python-why-does-8-5-8-4-give-0-099999999999999964"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 403,
   "position": {
    "height": "40px",
    "left": "921px",
    "right": "20px",
    "top": "79px",
    "width": "335px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
