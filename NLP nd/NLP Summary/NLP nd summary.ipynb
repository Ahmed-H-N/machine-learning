{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP nd Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content\n",
    "**<center> Parts</center>**  <br>\n",
    "- [**Part 1**](#Concept-1) **Welcome to Natural Language Processing**<br>\n",
    "- [**Part 2**](#Concept-2) **Building an NLP Pipeline** <br>\n",
    "- [**Part 3**](#Concept-3) **Voice User Interfaces** <br>\n",
    "- [**Part 4**](#Concept-4) **What's Next?** <br><br>\n",
    "- [**Part 5**](#Concept-5) **Introduction to Natural Language Processing**---------------$\\downarrow$\n",
    "- [**Part 6**](#Concept-6) **Computing with Natural Language**  $\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;$| -------$\\rightarrow$ the important parts\n",
    "- [**Part 7**](#Concept-7) **Communicating with Natural Language**------------------------$\\uparrow$\n",
    "---\n",
    "---\n",
    "---\n",
    "- [**Part 8**](#Concept-8) (Elective) **Recurrent Neural Networks**<br>\n",
    "- [**Part 9**](#Concept-9) (Elective) **Keras** <br>\n",
    "- [**Part 10**](#Concept-10) (Elective) **Sentiment Analysis Extras** <br>\n",
    "- [**Part 11**](#Concept-11) (Elective) **TensorFlow** <br>\n",
    "- [**Part 12**](#Concept-12) (Elective) **Embeddings and Word2Vec** <br>  \n",
    "- [**Part 13**](#Concept-13) **Project** <br>\n",
    "\n",
    "**<center> Parts and Lessons</center>**  <br>\n",
    "- [**Part 1**](#Concept-1) **Welcome to Natural Language Processing**<br>\n",
    "    - Lesson 1 --> take a sneak peek at what our program offers.\n",
    "- [**Part 2**](#Concept-2) **Building an NLP Pipeline** <br>\n",
    "    - Lesson 1 --> Learn about text processing, feature extraction, and part-of-speech tagging  \n",
    "- [**Part 3**](#Concept-3) **Voice User Interfaces** <br>\n",
    "    - Lesson 1 --> Learn about phonetics, acoustic models, and deep neural networks.    \n",
    "- [**Part 4**](#Concept-4) **What's Next?** <br>\n",
    "    - Lesson 1 --> Here's how to keep building your NLP expertise!<br><br>\n",
    "    \n",
    "- [**Part 5**](#Concept-5) **Introduction to Natural Language Processing** <br>\n",
    "    - Module 1\n",
    "        - Lesson 1 : Welcome to Natural Language Processing\n",
    "        - Lesson 2 : Udacity Support\n",
    "        - Lesson 3 : Intro to NLP\n",
    "        - Lesson 4 : Text Processing\n",
    "        - Lesson 5 : Spam Classifier with Naive Bayes\n",
    "        - Lesson 6 : Part of Speech Tagging with HMMs\n",
    "        - Lesson 7 : Project: Part of Speech Tagging\n",
    "        - Lesson 8 (Optional) IBM Watson Bookworm Lab\n",
    "    - Module 2 : Career Services\n",
    "        - Lesson 1 : Jobs in NLP\n",
    "        - Lesson 2 : Optimize Your GitHub Profile \n",
    "- [**Part 6**](#Concept-6) **Computing with Natural Language**<br>\n",
    "    - Module 1\n",
    "        - Lesson 1 : Feature extraction and embeddings\n",
    "        - Lesson 2 : Topic Modeling\n",
    "        - Lesson 3 : Sentiment Analysis\n",
    "        - Lesson 4 : Sequence to Sequence\n",
    "        - Lesson 5 : Deep Learning Attention\n",
    "        - Lesson 6 : RNN Keras Lab\n",
    "        - Lesson 7 : Cloud Computing Setup Instructions\n",
    "        - Lesson 8 : Project: Machine Translation\n",
    "    - Module 2 : Career Services\n",
    "        - Lesson 1 : Strengthen Your Online Presence Using LinkedIn\n",
    "- [**Part 7**](#Concept-7) **Communicating with Natural Language**<br>\n",
    "    - Lesson 1 : Intro to Voice User Interfaces\n",
    "    - Lesson 2 : (Optional) Alexa History Skill\n",
    "    - Lesson 3 : Speech Recognition\n",
    "    - Lesson 4 : Project: DNN Speech Recognizer\n",
    "    ---\n",
    "    ---\n",
    "    ---\n",
    "- [**Part 8**](#Concept-8) (Elective) **Recurrent Neural Networks**<br>\n",
    "    - Lesson 1 : Recurrent Neural Networks\n",
    "    - Lesson 2 : Long Short-Term Memory Networks (LSTM)    \n",
    "- [**Part 9**](#Concept-9) (Elective) **Keras** <br>\n",
    "    - Lesson 1 : Keras    \n",
    "- [**Part 10**](#Concept-10) (Elective) **Sentiment Analysis Extras** <br>\n",
    "    - Lesson 1 : Sentiment Analysis with Andrew Trask\n",
    "    - Lesson 2 : Sentiment Prediction RNN    \n",
    "- [**Part 11**](#Concept-11) (Elective) **TensorFlow** <br>\n",
    "    - Lesson 1 : TensorFlow    \n",
    "- [**Part 12**](#Concept-12) (Elective) **Embeddings and Word2Vec** <br>\n",
    "    - Lesson 1 : Embeddings and Word2Vec    \n",
    "- [**Part 13**](#Concept-13) **Project** <br>\n",
    "    - Lesson 1 : Project: Part of Speech Tagging        \n",
    "\n",
    "**<center> Parts and Lessons and Concepts</center>**  <br>\n",
    "- [**Part 1**](#Concept-1) **Welcome to Natural Language Processing**<br>\n",
    "    - Lesson 1 --> take a sneak peek at what our program offers.\n",
    "        - Concept 1: Program Syllabus\n",
    "        - Concept 2: School of AI Team\n",
    "        - Concept 3: NLP Overview\n",
    "        - Concept 4: Context Is Everything\n",
    "        - Concept 5: NLP and IBM Watson\n",
    "        - Concept 6: Towards Augmented Intelligence<br><br>\n",
    "        \n",
    "- [**Part 2**](#Concept-2) **Building an NLP Pipeline** <br>\n",
    "    - Lesson 1 --> Learn about text processing, feature extraction, and part-of-speech tagging.\n",
    "        - Concept 1: NLP and Pipelines\n",
    "        - Concept 2: How NLP Pipelines Work\n",
    "        - Concept 3: Text Processing\n",
    "        - Concept 4: Counting Words\n",
    "        - Concept 5: Feature Extraction\n",
    "        - Concept 6: Modeling\n",
    "        - Concept 7: Quiz: Split Sentences\n",
    "        - Concept 8: Part-of-Speech Tagging\n",
    "        - Concept 9: Named Entity Recognition\n",
    "        - Concept 10: Bag of Words\n",
    "        - Concept 11: TF-IDF\n",
    "        - Concept 12: One-Hot Encoding\n",
    "        - Concept 13: Word Embeddings\n",
    "        - Concept 14: t-SNE <br><br>\n",
    "    \n",
    "- [**Part 3**](#Concept-3) **Voice User Interfaces** <br>\n",
    "    - Lesson 1 --> Learn about phonetics, acoustic models, and deep neural networks.\n",
    "        - Concept 1: Welcome to Voice User Interfaces!\n",
    "        - Concept 2: VUI Overview\n",
    "        - Concept 3: VUI Applications\n",
    "        - Concept 4: Conversational AI with Alexa\n",
    "        - Concept 5: Lab: Space Geek\n",
    "        - Concept 6: Challenges in ASR\n",
    "        - Concept 7: Phonetics\n",
    "        - Concept 8: Quiz: Phonetics\n",
    "        - Concept 9: Acoustic Models and the Trouble with Time\n",
    "        - Concept 10: Language Models\n",
    "        - Concept 11: Deep Neural Networks as Speech Models\n",
    "    \n",
    "- [**Part 4**](#Concept-4) **What's Next?** <br>\n",
    "    - Lesson 1 --> Here's how to keep building your NLP expertise!\n",
    "        - Concept 1: Ready to Level Up with NLP?\n",
    "    \n",
    "- [**Part 5**](#Concept-5) **Introduction to Natural Language Processing** <br>\n",
    "This section provides an overview of the program and introduces the fundamentals of Natural Language Processing through symbolic manipulation, including text cleaning, normalization, and tokenization. You'll then build a part of speech tagger using hidden Markov models.\n",
    "    - Module 1\n",
    "        - Lesson 1 : Welcome to Natural Language Processing\n",
    "            - Concept 01: Welcome to the Natural Language Processing Nanodegree\n",
    "            - Concept 02: Deadline Policy\n",
    "            - Concept 03: Community Guidelines\n",
    "            - Concept 04: Lesson Plan - Week 1\n",
    "        - Lesson 2 : Udacity Support\n",
    "            - Concept 01: Udacity Support\n",
    "        - Lesson 3 : Intro to NLP\n",
    "            - Concept 01: Introducing Arpan\n",
    "            - Concept 02: NLP Overview\n",
    "            - Concept 03: Structured Languages\n",
    "            - Concept 04: Grammar\n",
    "            - Concept 05: Unstructured Text\n",
    "            - Concept 06: Counting Words\n",
    "            - Concept 07: Context Is Everything\n",
    "            - Concept 08: NLP and Pipelines\n",
    "            - Concept 09: How NLP Pipelines Work\n",
    "            - Concept 10: Text Processing\n",
    "            - Concept 11: Feature Extraction\n",
    "            - Concept 12: Modeling\n",
    "        - Lesson 4 : Text Processing\n",
    "            - Concept 01: Text Processing\n",
    "            - Concept 02: Coding Exercises\n",
    "            - Concept 03: Introduction to GPU Workspaces\n",
    "            - Concept 04: Workspaces: Best Practices\n",
    "            - Concept 05: Workspace\n",
    "            - Concept 06: Capturing Text Data\n",
    "            - Concept 07: Cleaning\n",
    "            - Concept 08: Normalization\n",
    "            - Concept 09: Tokenization\n",
    "            - Concept 10: Stop Word Removal\n",
    "            - Concept 11: Part-of-Speech Tagging\n",
    "            - Concept 12: Named Entity Recognition\n",
    "            - Concept 13: Stemming and Lemmatization\n",
    "            - Concept 14: Summary\n",
    "        - Lesson 5 : Spam Classifier with Naive Bayes\n",
    "            - Concept 01: Intro\n",
    "            - Concept 02: Guess the Person\n",
    "            - Concept 03: Known and Inferred\n",
    "            - Concept 04: Guess the Person Now\n",
    "            - Concept 05: Bayes Theorem\n",
    "            - Concept 06: Quiz: False Positives\n",
    "            - Concept 07: Solution: False Positives\n",
    "            - Concept 08: Bayesian Learning 1\n",
    "            - Concept 09: Bayesian Learning 2\n",
    "            - Concept 10: Bayesian Learning 3\n",
    "            - Concept 11: Naive Bayes Algorithm 1\n",
    "            - Concept 12: Naive Bayes Algorithm 2\n",
    "            - Concept 13: Building a Spam Classifier\n",
    "            - Concept 14: Project\n",
    "            - Concept 15: Spam Classifier - Workspace\n",
    "            - Concept 16: Outro\n",
    "        - Lesson 6 : Part of Speech Tagging with HMMs\n",
    "            - Concept 01: Intro\n",
    "            - Concept 02: Part of Speech Tagging\n",
    "            - Concept 03: Lookup Table\n",
    "            - Concept 04: Bigrams\n",
    "            - Concept 05: When bigrams won't work\n",
    "            - Concept 06: Hidden Markov Models\n",
    "            - Concept 07: Quiz: How many paths?\n",
    "            - Concept 08: Solution: How many paths\n",
    "            - Concept 09: Quiz: How many paths now?\n",
    "            - Concept 10: Quiz: Which path is more likely?\n",
    "            - Concept 11: Solution: Which path is more likely?\n",
    "            - Concept 12: Viterbi Algorithm Idea\n",
    "            - Concept 13: Viterbi Algorithm\n",
    "            - Concept 14: Further Reading\n",
    "            - Concept 15: Outro\n",
    "        - Lesson 7 : Project: Part of Speech Tagging\n",
    "            - Concept 01: Lesson Plan - Week 3\n",
    "            - Concept 02: Introduction\n",
    "            - Concept 03: Workspace: Part of Speech Tagging\n",
    "        - Lesson 8 (Optional) IBM Watson Bookworm Lab\n",
    "            - Concept 01: Overview\n",
    "            - Concept 02: Getting Started\n",
    "            - Concept 03: Tasks\n",
    "            - Concept 04: Workspace: Bookworm\n",
    "    - Module 2 : Career Services\n",
    "        - Lesson 1 : Jobs in NLP\n",
    "            - Concept 01: Jobs in NLP\n",
    "            - Concept 02: Meet the Careers Team\n",
    "            - Concept 03: Access Your Career Portal\n",
    "            - Concept 04: Your Udacity Professional Profile\n",
    "        - Lesson 2 : Optimize Your GitHub Profile \n",
    "            - Concept 01: Prove Your Skills With GitHub\n",
    "            - Concept 02: Introduction\n",
    "            - Concept 03: GitHub profile important items\n",
    "            - Concept 04: Good GitHub repository\n",
    "            - Concept 05: Interview with Art - Part 1\n",
    "            - Concept 06: Identify fixes for example “bad” profile\n",
    "            - Concept 07: Quick Fixes #1\n",
    "            - Concept 08: Quick Fixes #2\n",
    "            - Concept 09: Writing READMEs with Walter\n",
    "            - Concept 10: Interview with Art - Part 2\n",
    "            - Concept 11: Commit messages best practices\n",
    "            - Concept 12: Reflect on your commit messages\n",
    "            - Concept 13: Participating in open source projects\n",
    "            - Concept 14: Interview with Art - Part 3\n",
    "            - Concept 15: Participating in open source projects 2\n",
    "            - Concept 16: Starring interesting repositories\n",
    "            - Concept 17: Next Steps\n",
    "        \n",
    "- [**Part 6**](#Concept-6) **Computing with Natural Language**<br>\n",
    "    - Module 1\n",
    "        - Lesson 1 : Feature extraction and embeddings\n",
    "            - Concept 01: Feature Extraction\n",
    "            - Concept 02: Bag of Words\n",
    "            - Concept 03: TF-IDF\n",
    "            - Concept 04: One-Hot Encoding\n",
    "            - Concept 05: Word Embeddings\n",
    "            - Concept 06: Word2Vec\n",
    "            - Concept 07: GloVe\n",
    "            - Concept 08: Embeddings for Deep Learning\n",
    "            - Concept 09: t-SNE\n",
    "            - Concept 10: Summary\n",
    "        - Lesson 2 : Topic Modeling\n",
    "            - Concept 01: Intro\n",
    "            - Concept 02: References\n",
    "            - Concept 03: Bag of Words\n",
    "            - Concept 04: Latent Variables\n",
    "            - Concept 05: Matrix Multiplication\n",
    "            - Concept 06: Matrices\n",
    "            - Concept 07: Quiz: Picking Topics\n",
    "            - Concept 08: Solution: Picking Topics\n",
    "            - Concept 09: Beta Distributions\n",
    "            - Concept 10: Dirichlet Distributions\n",
    "            - Concept 11: Latent Dirichlet Allocation\n",
    "            - Concept 12: Sample a Topic\n",
    "            - Concept 13: Sample a Word\n",
    "            - Concept 14: Combining the Models\n",
    "            - Concept 15: Outro\n",
    "            - Concept 16: Notebook: Topic Modeling\n",
    "            - Concept 17: [SOLUTION] Topic Modeling\n",
    "            - Concept 18: Next Steps\n",
    "        - Lesson 3 : Sentiment Analysis\n",
    "            - Concept 01: Intro\n",
    "            - Concept 02: Sentiment Analysis with a Regular Classifier\n",
    "            - Concept 03: Notebook: Sentiment Analysis with a regular classifier\n",
    "            - Concept 04: [SOLUTION]: Sentiment Analysis with a regular clas\n",
    "            - Concept 05: Sentiment Analysis with RNN\n",
    "            - Concept 06: Notebook: Sentiment Analysis with an RNN\n",
    "            - Concept 07: [SOLUTION]: Sentiment Analysis with an RNN\n",
    "            - Concept 08: Optional Material\n",
    "            - Concept 09: Outro\n",
    "        - Lesson 4 : Sequence to Sequence\n",
    "            - Concept 01: Introducing Jay Alammar\n",
    "            - Concept 02: Previous Material\n",
    "            - Concept 03: Jay Introduction\n",
    "            - Concept 04: Applications\n",
    "            - Concept 05: Architectures\n",
    "            - Concept 06: Architectures in More Depth\n",
    "            - Concept 07: Outro\n",
    "        - Lesson 5 : Deep Learning Attention\n",
    "            - Concept 01: Introduction to Attention\n",
    "            - Concept 02: Sequence to Sequence Recap\n",
    "            - Concept 03: Encoding -- Attention Overview\n",
    "            - Concept 04: Decoding -- Attention Overview\n",
    "            - Concept 05: Attention Overview\n",
    "            - Concept 06: Attention Encoder\n",
    "            - Concept 07: Attention Decoder\n",
    "            - Concept 08: Attention Encoder & Decoder\n",
    "            - Concept 09: Bahdanau and Luong Attention\n",
    "            - Concept 10: Multiplicative Attention\n",
    "            - Concept 11: Additive Attention\n",
    "            - Concept 12: Additive and Multiplicative Attention\n",
    "            - Concept 13: Computer Vision Applications\n",
    "            - Concept 14: NLP Application: Google Neural Machine Translation\n",
    "            - Concept 15: Other Attention Methods\n",
    "            - Concept 16: The Transformer and Self-Attention\n",
    "            - Concept 17: Notebook: Attention Basics\n",
    "            - Concept 18: [SOLUTION]: Attention Basics\n",
    "            - Concept 19: Outro\n",
    "        - Lesson 6 : RNN Keras Lab\n",
    "            - Concept 01: Intro\n",
    "            - Concept 02: Machine Translation\n",
    "            - Concept 03: Deciphering Code with character-level RNNs\n",
    "            - Concept 04: [SOLUTION] Deciphering code with character-level RNNs\n",
    "            - Concept 05: Congratulations!\n",
    "        - Lesson 7 : Cloud Computing Setup Instructions\n",
    "            - Concept 01: Overview\n",
    "            - Concept 02: Create an AWS Account\n",
    "            - Concept 03: Get Access to GPU Instances\n",
    "            - Concept 04: Launch Your Instance\n",
    "            - Concept 05: Remotely Connecting to Your Instance\n",
    "        - Lesson 8 : Project: Machine Translation\n",
    "            - Concept 01: Introduction to GPU Workspaces\n",
    "            - Concept 02: Workspaces: Best Practices\n",
    "            - Concept 03: NLP Machine Translation Workspace\n",
    "            - Concept 04: Project: Machine Translation\n",
    "    - Module 2 : Career Services\n",
    "        - Lesson 1 : Strengthen Your Online Presence Using LinkedIn\n",
    "            - Concept 01: Get Opportunities with LinkedIn\n",
    "            - Concept 02: Use Your Story to Stand Out\n",
    "            - Concept 03: Why Use an Elevator Pitch\n",
    "            - Concept 04: Create Your Elevator Pitch\n",
    "            - Concept 05: Use Your Elevator Pitch on LinkedIn\n",
    "            - Concept 06: Create Your Profile With SEO In Mind\n",
    "            - Concept 07: Profile Essentials\n",
    "            - Concept 08: Work Experiences & Accomplishments\n",
    "            - Concept 09: Build and Strengthen Your Network\n",
    "            - Concept 10: Reaching Out on LinkedIn\n",
    "            - Concept 11: Boost Your Visibility\n",
    "            - Concept 12: Up Next\n",
    "        \n",
    "- [**Part 7**](#Concept-7) **Communicating with Natural Language**<br>\n",
    "    - Lesson 1 : Intro to Voice User Interfaces\n",
    "        - Concept 01: Welcome to Voice User Interfaces!\n",
    "        - Concept 02: VUI Overview\n",
    "        - Concept 03: VUI Applications\n",
    "    - Lesson 2 : (Optional) Alexa History Skill\n",
    "        - Concept 01: Overview\n",
    "        - Concept 02: Getting Started\n",
    "        - Concept 03: Tasks\n",
    "        - Concept 04: Deploying Your Skill\n",
    "    - Lesson 3 : Speech Recognition\n",
    "        - Concept 01: Intro\n",
    "        - Concept 02: Challenges in ASR\n",
    "        - Concept 03: Signal Analysis\n",
    "        - Concept 04: References: Signal Analysis\n",
    "        - Concept 05: Quiz: FFT\n",
    "        - Concept 06: Feature Extraction with MFCC\n",
    "        - Concept 07: References: Feature Extraction\n",
    "        - Concept 08: Quiz: MFCC\n",
    "        - Concept 09: Phonetics\n",
    "        - Concept 10: References: Phonetics\n",
    "        - Concept 11: Quiz: Phonetics\n",
    "        - Concept 12: Voice Data Lab Introduction\n",
    "        - Concept 13: Lab: Voice Data\n",
    "        - Concept 14: Acoustic Models and the Trouble with Time\n",
    "        - Concept 15: HMMs in Speech Recognition\n",
    "        - Concept 16: Language Models\n",
    "        - Concept 17: N-Grams\n",
    "        - Concept 18: Quiz: N-Grams\n",
    "        - Concept 19: References: Traditional ASR\n",
    "        - Concept 20: A New Paradigm\n",
    "        - Concept 21: Deep Neural Networks as Speech Models\n",
    "        - Concept 22: Connectionist Tempora Classification (CTC)\n",
    "        - Concept 23: References: Deep Neural Network ASR\n",
    "        - Concept 24: Outro\n",
    "    - Lesson 4 : Project: DNN Speech Recognizer\n",
    "        - Concept 01: Overview\n",
    "        - Concept 02: Introduction to GPU Workspaces\n",
    "        - Concept 03: Workspaces: Best Practices\n",
    "        - Concept 04: Tasks\n",
    "        - Concept 05: VUI Speech Recognizer Workspace\n",
    "    ---\n",
    "    ---\n",
    "    ---\n",
    "- [**Part 8**](#Concept-8) (Elective) **Recurrent Neural Networks**<br>\n",
    "    - Lesson 1 : Recurrent Neural Networks\n",
    "        - Concept 01: Introducing Ortal\n",
    "        - Concept 02: RNN Introduction\n",
    "        - Concept 03: RNN History\n",
    "        - Concept 04: RNN Applications\n",
    "        - Concept 05: Feedforward Neural Network-Reminder\n",
    "        - Concept 06: The Feedforward Process\n",
    "        - Concept 07: Feedforward Quiz\n",
    "        - Concept 08: Backpropagation- Theory\n",
    "        - Concept 09: Backpropagation - Example (part a)\n",
    "        - Concept 10: Backpropagation- Example (part b)\n",
    "        - Concept 11: Backpropagation Quiz\n",
    "        - Concept 12: RNN (part a)\n",
    "        - Concept 13: RNN (part b)\n",
    "        - Concept 14: RNN- Unfolded Model\n",
    "        - Concept 15: Unfolded Model Quiz\n",
    "        - Concept 16: RNN- Example\n",
    "        - Concept 17: Backpropagation Through Time (part a)\n",
    "        - Concept 18: Backpropagation Through Time (part b)\n",
    "        - Concept 19: Backpropagation Through Time (part c)\n",
    "        - Concept 20: BPTT Quiz 1\n",
    "        - Concept 21: BPTT Quiz 2\n",
    "        - Concept 22: BPTT Quiz 3\n",
    "        - Concept 23: Some more math\n",
    "        - Concept 24: RNN Summary\n",
    "        - Concept 25: From RNN to LSTM\n",
    "        - Concept 26: Wrap Up\n",
    "    - Lesson 2 : Long Short-Term Memory Networks (LSTM)\n",
    "        - Concept 01: Intro to LSTM\n",
    "        - Concept 02: RNN vs LSTM\n",
    "        - Concept 03: Basics of LSTM\n",
    "        - Concept 04: Architecture of LSTM\n",
    "        - Concept 05: The Learn Gate\n",
    "        - Concept 06: The Forget Gate\n",
    "        - Concept 07: The Remember Gate\n",
    "        - Concept 08: The Use Gate\n",
    "        - Concept 09: Putting it All Together\n",
    "        - Concept 10: Quiz\n",
    "        - Concept 11: Other architectures\n",
    "        - Concept 12: Outro LSTM\n",
    "    \n",
    "- [**Part 9**](#Concept-9) (Elective) **Keras** <br>\n",
    "    - Lesson 1 : Keras\n",
    "        - Concept 01: Intro\n",
    "        - Concept 02: Keras\n",
    "        - Concept 03: Pre-Lab: Student Admissions in Keras\n",
    "        - Concept 04: Lab: Student Admissions in Keras\n",
    "        - Concept 05: Optimizers in Keras\n",
    "        - Concept 06: Mini Project Intro\n",
    "        - Concept 07: Pre-Lab: IMDB Data in Keras\n",
    "        - Concept 08: Lab: IMDB Data in Keras\n",
    "    \n",
    "- [**Part 10**](#Concept-10) (Elective) **Sentiment Analysis Extras** <br>\n",
    "    - Lesson 1 : Sentiment Analysis with Andrew Trask\n",
    "        - Concept 01: Meet Andrew\n",
    "        - Concept 02: Materials\n",
    "        - Concept 03: The Notebooks\n",
    "        - Concept 04: Framing the Problem\n",
    "        - Concept 05: Mini Project 1\n",
    "        - Concept 06: Mini Project 1 Solution\n",
    "        - Concept 07: Transforming Text into Numbers\n",
    "        - Concept 08: Mini Project 2\n",
    "        - Concept 09: Mini Project 2 Solution\n",
    "        - Concept 10: Building a Neural Network\n",
    "        - Concept 11: Mini Project 3\n",
    "        - Concept 12: Mini Project 3 Solution\n",
    "        - Concept 13: Understanding Neural Noise\n",
    "        - Concept 14: Mini Project 4\n",
    "        - Concept 15: Understanding Inefficiencies in our Network\n",
    "        - Concept 16: Mini Project 5\n",
    "        - Concept 17: Mini Project 5 Solution\n",
    "        - Concept 18: Further Noise Reduction\n",
    "        - Concept 19: Mini Project 6\n",
    "        - Concept 20: Mini Project 6 Solution\n",
    "        - Concept 21: Analysis: What's Going on in the Weights?\n",
    "        - Concept 22: Conclusion\n",
    "    - Lesson 2 : Sentiment Prediction RNN\n",
    "        - Concept 01: Intro\n",
    "        - Concept 02: Sentiment RNN\n",
    "        - Concept 03: Data Preprocessing\n",
    "        - Concept 04: Creating Testing Sets\n",
    "        - Concept 05: Building the RNN\n",
    "        - Concept 06: Training the Network\n",
    "        - Concept 07: Solutions\n",
    "    \n",
    "- [**Part 11**](#Concept-11) (Elective) **TensorFlow** <br>\n",
    "    - Lesson 1 : TensorFlow\n",
    "        - Concept 01: Intro\n",
    "        - Concept 02: Installing TensorFlow\n",
    "        - Concept 03: Hello, Tensor World!\n",
    "        - Concept 04: Quiz: TensorFlow Linear Function\n",
    "        - Concept 05: Quiz: TensorFlow Softmax\n",
    "        - Concept 06: Quiz: TensorFlow Cross Entropy\n",
    "        - C\\oncept 07: Quiz: Mini-batch\n",
    "        - Concept 08: Epochs\n",
    "        - Concept 09: Pre-Lab: NotMNIST in TensorFlow\n",
    "        - Concept 10: Lab: NotMNIST in TensorFlow\n",
    "        - Concept 11: Two-layer Neural Network\n",
    "        - Concept 12: Quiz: TensorFlow ReLUs\n",
    "        - Concept 13: Deep Neural Network in TensorFlow\n",
    "        - Concept 14: Save and Restore TensorFlow Models\n",
    "        - Concept 15: Finetuning\n",
    "        - Concept 16: Quiz: TensorFlow Dropout\n",
    "        - Concept 17: Outro\n",
    "    \n",
    "- [**Part 12**](#Concept-12) (Elective) **Embeddings and Word2Vec** <br>\n",
    "    - Lesson 1 : Embeddings and Word2Vec\n",
    "        - Concept 01: Additional NLP Lessons\n",
    "        - Concept 02: Embeddings Intro\n",
    "        - Concept 03: Implementing Word2Vec\n",
    "        - Concept 04: Subsampling Solution\n",
    "        - Concept 05: Making Batches\n",
    "        - Concept 06: Batches Solution\n",
    "        - Concept 07: Building the Network\n",
    "        - Concept 08: Negative Sampling\n",
    "        - Concept 09: Building the Network Solution\n",
    "        - Concept 10: Training Results\n",
    "    \n",
    "- [**Part 13**](#Concept-13) **Project** <br>\n",
    "    - Lesson 1 : Project: Part of Speech Tagging\n",
    "        - Concept 01: Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Welcome to Natural Language Processing\n",
    "- which has 1 lesson:\n",
    "    - Lesson 1 : Welcome to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Welcome to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to a Free Preview of the Natural Language Processing Nanodegree program! Come and take a sneak peek at what our program offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (6 concepts)\n",
    "- Concept 1: Program Syllabus\n",
    "- Concept 2: School of AI Team\n",
    "- Concept 3: NLP Overview\n",
    "- Concept 4: Context Is Everything\n",
    "- Concept 5: NLP and IBM Watson\n",
    "- Concept 6: Towards Augmented Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-1.1) Program Syllabus\n",
    "\n",
    "#### Reading\n",
    "Here's an overview of the syllabus for the full program:\n",
    "\n",
    "- **Introduction to Natural Language**: This section teaches the fundamentals of NLP, including text processing, sentiment analysis, and part of speech tagging. And its project consists of building a part of speech tagging models using hidden Markov models. <br><br>\n",
    "\n",
    "- **Computing with Natural Language Processing**: This section takes a deep dive into some of the main natural language processing techniques, including modeling, feature extraction, embeddings, and deep learning attention. Its project consists of building a machine translation model, using several different deep learning architectures. <br><br>\n",
    "\n",
    "- **Communicating with Natural Language**: This section teaches you about speech recognition and voice user interfaces. It will include a lab in which you'll create an Alexa skill. The project consists of building a speech recognition model using deep learning. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-1.2) School of AI Team\n",
    "فكك"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-1.3) NLP Overview\n",
    "\n",
    "<img src=\"img/1_1_3_1.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "Welcome to Natural Language Processing. Language is an important medium for human communication. It allows us to convey information, express our ideas, and give instructions to others. Some philosophers argue that it enables us to form complex thoughts and reason about them. It may turn out to be a critical component of human intelligence. Now consider the various artificial systems we interact with every day, phones, cars, websites, coffee machines. It's natural to expect them to be able to process and understand human language, right? Yet, computers are still lagging behind. No doubt, we have made some incredible progress in the field of natural language processing, but there is still a long way to go. And that's what makes this an exciting and dynamic area of study. In this lesson you will not only get to know more about the applications and challenges in NLP, you will learn how to design an intelligent application that uses NLP techniques and deploy it on a scalable platform. Sounds fun? Let's get started. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-1.4) Context Is Everything\n",
    "So what is stopping computers from becoming as capable as humans in understanding natural language? Part of the problem lies in the variability and complexity of our sentences. Consider this excerpt from a movie review. \"I was lured to see this on the promise of a smart, witty slice of old fashioned fun and intrigue - I was conned. \" Although it starts with some potentially positive words it turns out to be a strongly negative review. Sentences like this might be somewhat entertaining for us but computers tend to make mistakes when trying to analyze them. But there is a bigger challenge that makes NLP harder than you think. Take a look at this sentence. \"The sofa didn't fit through the door because it was too narrow.\" What does \"it\" refer to? Clearly \"it\" refers to the door. Now consider a slight variation of this sentence. \"The sofa didn't fit through the door because it was too wide.\" What does \"it\" refer to in this case? Here it's the sofa. Think about it. To understand the proper meaning or semantics of the sentence you implicitly applied your knowledge about the physical world, that wide things don't fit through narrow things. You may have experienced a similar situation before. You can imagine that there are countless other scenarios in which some knowledge or context is indispensable for correctly understanding what is being said. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-1.5) NLP and IBM Watson\n",
    "\n",
    "<img src=\"img/1_1_5_1.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "Natural Language Processing, even with its current challenges, holds a lot of promise. To help us better understand how NLP is being used today, we have with us Armen Pischdotchian from IBM. Hello. My name is Armen. In my current role as an Academic Tech Mentor I lecture and conduct workshops using Watson Cognitive Services for universities, highschools, STEM programs, and the girls who code. We also run hackathons using IBM Watson Cognitive Services that reside on Bluemix, our platform as a service. Another aspect of the work that we do is often wearing a Solution Architect cap because most universities are also working with clients who use the academic environment as their development arena. I especially look forward to such extended engagements where I strive to build a proof of concept for such clients. And you're right, Arpun. We are seeing an increasing number of projects that use NLP in one way or another. And it is being used today, not in isolation, but embedded as part of intelligent end to end solutions that use a number of different technologies often augmented with understanding vision and not just a bag of words. I see. So, what would you say is the primary role of NLP in such a system? NLP is characterized as a hard problem in computer science. Human language is rarely precise or plainly spoken. To understand human language is to understand not only the words but the concepts and how they are linked together to create meaning. Despite language being one of the easiest things for humans to learn, the ambiguity of language is what makes Natural Language Processing a difficult problem for computers to master. For example, consider this phrase, this fountain is not drinking water. A cognitive system that can tokenize, parse, and annotate the phrase will infer that the fountain is currently not engaged in the act of drinking water. But we know that it means we should not drink the water from this fountain. Today's systems go beyond analyzing utterances. They understand the relationship of the words, the context surrounding the utterances. For example, why do I need to scan through dozens of airbnb reviews to find a suitable accommodation when I can better describe what I want in my own words? I am looking for an apartment with views of the river, facing the Manhattan skyline, but I don't want a noisy and smelly location. I expect the cognitive system to uncover customer reviews that depict exactly the criteria that I am seeking. And that's where NLP comes in. It can help understand the user's intentions and what they like or dislike and process large volumes of text to better infer the context of our conversations and just exactly what we mean by what we say. Here's an example for you, Arpun. A wise man is absolutely not the same as a wise guy. Consider this, a slim chance is the same as a fat chance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1-1.6) Towards Augmented Intelligence\n",
    "\n",
    "<img src=\"img/1_1_6_1.png\" alt=\"Drawing\" style=\"width: 550px;\"/> <br>\n",
    "\n",
    "Now I have to ask you this question and likely many of our students are pondering it as well. So, will AI replace all our jobs? Will it take over the world? How worried should we be? I was waiting for that priceless question. First and foremost, our leader, Ginni Rometty, has coined the term Augmented Intelligence, not artificial intelligence. Our capabilities are here not to supplant jobs but to help remove the mundane tasks so workers can focus on the core mission. Yes, they will require new training and we call that new collar jobs and that is exactly what our partnership with Udacity is providing. So yes, we are rapidly approaching an inflection point in human history where Augmented Intelligence will exceed human intelligence and debates about humans versus machines have become part of our common vernacular. How can we prepare our next generation of students to compete? One positive step being taken in our K through 12 schools is a growing emphasis on STEM: science technology, engineering and mathematics education, plus modern coding skills. For example, IBM's P-TECH program, where students graduate with a high school diploma and an associate degree in computers or engineering. But STEM education is not enough, and that is where our partnership comes in. Jeffrey Calvin in his book, \"Humans Are Underrated: What High Achievers Know That Brilliant Machines Never Will,\" argues that it is right brain skills like creativity, imagination, collaboration, holistic thinking, and emotional intelligence that are the greatest source of competitive advantage for humans versus machines. Thank you so much for giving us such a comprehensive tour of natural language processing and artificial intelligence. Absolutely, our mission at IBM Watson is to bring the cutting edge cognitive technologies within reach of everyone. There's no reason for Augmented Intelligence to stay locked up in research labs and in the hands of a few companies. With the power of IBM Watson behind you I can't even imagine what applications you'll end up building. Let's get started today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Building an NLP Pipeline\n",
    "- which has 1 lesson: \n",
    "    - Lesson 1 : Building an NLP Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 1 : Building an NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn about text processing, feature extraction, and part-of-speech tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### content (14 concepts)\n",
    "- Concept 1: NLP and Pipelines\n",
    "- Concept 2: How NLP Pipelines Work\n",
    "- Concept 3: Text Processing\n",
    "- Concept 4: Counting Words\n",
    "- Concept 5: Feature Extraction\n",
    "- Concept 6: Modeling\n",
    "- Concept 7: Quiz: Split Sentences\n",
    "- Concept 8: Part-of-Speech Tagging\n",
    "- Concept 9: Named Entity Recognition\n",
    "- Concept 10: Bag of Words\n",
    "- Concept 11: TF-IDF\n",
    "- Concept 12: One-Hot Encoding\n",
    "- Concept 13: Word Embeddings\n",
    "- Concept 14: t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.1) NLP and Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.2) How NLP Pipelines Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.3) Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.4) Counting Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.5) Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.6) Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.7) Quiz: Split Sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.8) Part-of-Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.9) Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.10) Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.11) TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.12) One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.13) Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2-1.14) t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
