earlier in this lesson you were introduced to an iterative method for determining the value function corresponding to a policy in this video we'll discuss the algorithm in more detail the algorithm is called iterative policy evaluation and it assumes that the agent already has full and perfect knowledge of the MDP that characterizes the environment remember that this algorithm was motivated by the bellmen expectation equation for the state value function and this equation is really a system of equations since we have one equation for each environment state each equation relates the value of a state to the values of its successor States theoretically since we have a system of equations with one equation for each unknown quantity we could solve the system but there's an easier way where instead of solving the system exactly we'll construct an iterative algorithm where each step gets us closer and closer to solving this system of equations specifically our iterative algorithm will adapt this bellman equation so it can be used as an update rule here the capital V denotes the current guess for the policies value function note that I've only changed two things about the bellman equation which I've underlined in yellow here so the algorithm begins with an initial guess for the value function of the policy what's typically done is to set the initial guess for each state to 0 okay and so there's no way that's actually the true value function but we just need to start somewhere then we'll loop over the states and use the update rule to improve our guests for the state value function and what's nice is that as long as a few technical conditions are satisfied this algorithm is guaranteed to converge to the value function for the policy now we can only attain true convergence in the limit of running this algorithm an infinite number of times and that's not feasible in practice so we'll have to stop short of true convergence and the question is how can we tell when we've gotten close enough well in practice when you implement the algorithm you'll notice that the first few times the update step is applied there are big changes to the value function but then eventually at a later time point you'll notice that updates are hardly noticeable and once we get to that point we're applying the update rule doesn't change the estimate of the value function much that's a strong indication that our algorithm has gotten reasonably close to converging to the true value function and so inspired by this fact we can design a stopping criterion that terminates the algorithm whenever we've done a complete pass over all the states and none of the values has changed much to do this we'll amend the pseudocode where the first step is for you to set a very small positive number theta next as before you initialize the first guess for the value function to be all zeros then you enter the iterative loop where each step you apply the development update and check to see how much the values for each of the states has changed if the maximum change over all states is less than that small number theta that you set then you stop the algorithm and say that your estimate for the value function is good enough and that's it but now if that update step feels suspicious to you I don't blame you and your instincts to one a proof are correct it's not at all intuitive that this algorithm should work the way it does but here's the main idea behind what that update step is doing remember that we loop over states and apply the update to one state at a time and from each state the agent could choose any of a number of possible actions that bring it to any of a number of potential next States and the bellman equation helps us relate the value of this parent state to the values of all of its possible successor states now what we do is look at the estimated value function for the parent state along with the values of the successor States and if we plug in those values to the bellman equation and it's not satisfied what we'll do is we'll change the value of the parent state so that the bellman equation is satisfied then we'll do the same for the second state and so on and what's important to note is that if at some point we go to apply this update rule and there's no change in any of the values of any of the states then that means we have value function that perfectly satisfies the bellman equation and the only value function that does that is the true value function of the policy