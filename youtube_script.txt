So let's talk about life. There are two errors that sometimes we make in life. One is trying to kill Godzilla with a flyswatter. That's a pretty bad error to make. It's oversimplifying the problem we're trying to solve. The other one is to try to kill a fly with a bazooka. That's also pretty bad. It's overcomplicating the problem we're trying to solve. In machine learning, these two types of errors are very easy to make. When we oversimplify the problem, we call this underfitting. And when we overcomplicate the problem, we call it overfitting. So let's look at underfitting and overfitting in a bit more detail. Let's look at this classification problem. We need to find a property that separates the set on the left from the set on the right. It seems like the solution is easy. The set on the right is made of dogs, and the set on the left is made of things that are not dogs. But what if we oversimplify this? What if we say the set on the right is made of animals, and the set on the left is made of everything but animals? Then our model is is a bit too simple. And we can see that it already makes a mistake on the training set, since it misclassified this cat on the left side. This oversimplification is called underfitting. One characteristic of it is that it doesn't do well on the training set. We call this type of error an error due to bias. The other mistake we can make is to overcomplicate the model. If instead of describing the set on the right as dogs, we describe it as dogs that are wagging their tail, then this seems to do the job well in training set, but somehow our intuition is telling us that this is not right. This can be confirmed when you bring out a new instance. For example, this dog over here. Our logic tells us this dog should belong to the set on the right. But since the dog is not wagging its tail, then this model mistakenly classifies it in the set on the left. This error is called overfitting, this means the model is too specific. One characteristic of it is that it does well in the training set but it tends to memorize it instead of learning the characteristics of it, so it will not do well on the testing set. We call this type of error an error due to variance. Let's get more technical. In our regression example, we can see underfitting as follows. Let's look at the points on the left. It seems like the correct model for this point is a quadratic equation like this one. We could try to model it as a line, but this won't work too well, since it's too simple. The model won't do well in our training set. This is an example of underfitting. Now, what if instead we try to fit a polynomial of large degree like this one? This polynomial does great in the training set, fits it perfectly, but somehow it seems like it's not the best answer. It memorizes the training set and it fails to find good properties of the training set that will generalize well to the testing set. So even though it performs well in the training set, it will perform poorly on the testing set. This is an example of overfitting. We can see underfitting in a classification model as well. The red and blue points seems to be nicely separated by a quadratic curve like this one. When we try to use a line, the model doesn't fit the points properly, and it underfits. And when we try to fit in a curve that is very complicated, we end up with a model that is too complex. And this may not do well in the testing set. Thus it overfits. So here's a small summary. On one side, we get the errors due to high bias, or underfitting. This is where we oversimplify the problem and our model is too simple to capture the complexity of our data. On the other side, we get the errors due to high variance or overfitting. This is when we overcomplicate the problem and our model is too complex and ends up memorizing our data instead of learning it. Then in the middle, we've got the good model. When it comes to the training data, the high bias model tends to not fit it well since it's just too simple a model. The high variance model tends to fit the training data really well since it's designed for it. Finally, the good model tends to fit the training data well. Now, when it comes to the testing data, the high bias model tends to perform poorly. And so does the high variance model. The good model is the one that performs well in the testing data.