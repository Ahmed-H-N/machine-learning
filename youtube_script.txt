Let's recall the problem at hand. We have an agent and environment. Time has broken into discrete time steps, and every time step, the agent receives a reward and state from the environment and chooses an action to perform in response. In this way, the interaction evolves as a sequence of states, actions, and rewards. In this lesson, we'll confine our attention to episodic tasks where the interaction stops at some time step T when the agent encounters a terminal state. And we refer to this sequence as an episode. For any episode, the agent's goal is to find the optimal policy in order to maximize expected cumulative reward. Towards this goal, we'll start with the prediction problem. Given the policy, how might the agent estimate the value function for that policy? Remember that the environment's dynamics are unknown to the agent. So it will have to estimate the value function by interacting with the environment. And in order to interact with the environment, the agent needs to have a policy in mind. Now, it's possible to have one policy that we'd like to evaluate and a different policy to interact with the environment. This is referred to as an off-policy method for the prediction problem. Off-policy methods entail some complexity that we'll save for later. Instead, we'll start with an on policy method, where the agent interacts with the environment by following a policy, whose value function it would like to calculate. Before we go into the details of the algorithm, let's look at a motivating example. Say we're working with an episodic task and the MDP has three states; X,Y, and Z, where Z has a terminal state. In case it helps to have a more visual understanding of the MDP, you can think of it as a very, very small frozen world. Say states X and Y correspond to different locations in a snowy forest with possible delights and terrors corresponding to possibly positive or negative reward. Remember state Z is the terminal state and say it corresponds to a house in the forest. If the agent lands at the state, it enters the house and the episode ends. Say there are two potential actions, up and down and similar to the frozen lake environment, the world is slippery. So if the agent chooses action down, at the next time step, there is some positive probability that it instead moves up or stays where it is. Likewise, if it decides to go up then when it tries to move in that direction, it might instead go down or again not move at all. And say we'd like to evaluate the policy where the agent chooses action up in state X and action down in State Y. Then towards this goal the agent could interact with the environment by following this policy. Say at the beginning of the interaction, the agent observes state X, then following the policy, it chooses action up, as a result. It receives a reward of negative two and the next state of Y. Again, following the policy, it chooses to go down, and as a result, gets a reward of zero and next state of Y. At this point, it follows the policy and chooses action down. As a result, it gets a reward of three and reaches the terminal stage Z, so the interaction ends. Say the agent interacts with the environment in two additional episodes. Then we can use these episodes to estimate the value function. Of course these three short episodes aren't really enough interaction for the agent to get a great understanding of the environment, but for now, let's assume that they're sufficient. We'll begin with one state, say state X. Then we look at all of the occurrences of state X and all of the episodes. Then we just calculate the discounted return that followed after that state appeared. Say the discount is one so in other words, we won't discount. Then in the case of the first episode, the return is negative two, plus zero, plus three or one. And in the third episode, the return is negative three plus three or zero. The Monte Carlo prediction algorithm takes the average of these values and plugs it in as an estimate for the value of state X. In this case, the value of state X is estimated to be one half. This algorithm makes some intuitive sense. Remember that the value of a state is defined to be the expected return after that state is observed. And so the average return that was experienced by the agent makes for a good estimate. We'll follow the same process to evaluate state Y. And you'll notice that state Y appears more than once in each episode and it's not quite clear what to do in this case. To address this, we'll need to define some additional terminology. We define every occurrence of a state in an episode as a visit to that state. And in the event where a state is visited, more than once in an episode we have two options. As a first option, we could for each episode only consider the first visit to the state and average those returns. In this case, the value of state Y would be estimated as the average of three, three and one or seven third's. If we choose this approach, then we say that we are using the first visit MC method. The other option is to average the return following all visits to state Y in all episodes. In this case, the value of State Y would be estimated as the average of all of these numbers, which can be calculated as fourteen sevenths or two. You'll soon have the chance to implement this algorithm for yourself. Feel free to choose either first visit or every visit MC prediction, or if you like, implement both.