Our Monte Carlo control algorithm will draw inspiration from generalized policy iteration. We'll begin with the policy evaluation step. We've already somewhat addressed how to accomplish this and you implemented it in part two of the mini project. In your implementation, the agent needs to play blackjack about 5,000 times to get a good estimate of the value function. In the context of policy iteration, this seems like way too long to spend evaluating a policy before trying to improve it. Maybe it would make more sense to improve the policy after every individual game of blackjack. In that case, we could initialize the value of each state action pair to zero and have some starting policy. Then, we could use that policy to generate an episode. When that episode finishes, we could update the value function. Then, the value function could be used to improve the policy. That new policy could then be used to generate the next episode, and so on. To do this well, we'll have to change our algorithm for policy evaluation. We'll begin by digging deeper into our current algorithm. Remember that we begin by running many episodes. Then, we look at some state action pair. We calculate the return that followed in each case and then take the average. As a more general case, say we visited the state action pair some number and times. We'll denote the corresponding returns by x1, x2 all the way up to xn. Then, our value function estimate is calculated as the average of those values. We'll denote it with a MU n where the n just helps us to remember that we visited the pair n times. And so Instead of calculating that average at the end of all the episodes, maybe what we could do instead is iteratively update the estimate after every visit. So the first time that we visited, whatever the return was, that's our estimate. Then the second time we visit, we can update the estimate to be the average of x1 and x2. And for an arbitrary number of visits K, we just take the average of all the values x1 through xk. And our final estimate, will just be exactly what we'd end up with if we'd waited to calculate anything until all the episodes finished. And so let's see if we can figure out how to accomplish this in a computationally efficient way. And towards that goal, we'll have to do a little bit of math. We'll begin by remembering how we calculate the estimate for the return. In particular, the Kth estimate is just the average of the returns that followed from the first K visits. Then, we'll notice we can rewrite the sum of the first K returns, as the sum of the first K minus one returns plus the Kth return. After that, we can re-express the sum of the first K minus one terms, as the K minus first mean times K minus one. It's just a simple rearrangement of the terms to get the last line. Now this equation expresses the Kth mean in terms of the K minus first mean and the Kth return. And we'll use this formula to design an algorithm that keeps a running mean of the returns. We begin by initializing the mean to zero. We'll also need to keep track of the number of returns that we've included in the average already. So at the beginning, we'll initialize that to zero. Then we enter a while loop at every point we increment K by one and then use the most recent return x of K to update the mean. And that's it. Now while it's not entirely clear yet exactly how to use this in the context of Monte Carlo control, this algorithm will come in very useful soon.